[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biology 305: Biostatistics",
    "section": "",
    "text": "Preface\nWelcome to Biology 305 at the University of Nebraska at Kearney! Material in this class was designed by Dr. Melissa Wuellner and adapted by Dr. Jacob C. Cooper for use in R.\nIn this class, you will learn:\n\nThe basics of study design, the importance of understanding your research situation before embarking on a full study, and practice creating research frameworks based on different scenarios.\nThe basics of data analysis, including understanding what kind of variables are being collected, why understanding variable types are important, and basic tests to understand univariate distributions.\nBasic multivariate statistics, including ANOVA, correlation, and regression, for comparing multiple different groups.\nThe basics of coding and working in R for performing statistical analyses.\n\nThis site will help you navigate different homework assignments to perform the necessary R tests. Furthermore, this GitHub repository contains all of the homework dataframes, so you will not have to manually enter assignments if you use R to complete your assignments.\nWelcome to class!\nDr. Jacob C. Cooper, BHS 321",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "1  Intro to R",
    "section": "",
    "text": "2 Setup\nFirst, we need to download R onto your machine. We are also going to download RStudio to assist with creating R scripts and documents.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#installing-r",
    "href": "intro_to_r.html#installing-r",
    "title": "1  Intro to R",
    "section": "2.1 Installing R",
    "text": "2.1 Installing R\nFirst, navigate to the R download and install page. Download the appropriate version for your operating system (Windows, Mac, or Linux). Note that coding will be formatted slightly different for Windows than for other operating systems. If you have a Chromebook, you will have to follow the online instructions for installing both programs on Chrome.\nFollow the installation steps for R, and verify that the installation was successful by searching for R on your machine. You should be presented with a coding window that looks like the following:\nR version 4.4.1 (2024-06-14) -- \"Race for Your Life\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: aarch64-apple-darwin20\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\nIf that screen appears, congratulations! R is properly installed. If the install was not successful, please talk to Dr. Cooper and check with your classmates as well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#installing-rstudio",
    "href": "intro_to_r.html#installing-rstudio",
    "title": "1  Intro to R",
    "section": "2.2 Installing RStudio",
    "text": "2.2 Installing RStudio\nRStudio is a GUI (graphics user interface) that helps make R easier to use. Furthermore, it allows you to create documents in R, including websites (such as this one), PDFs, and even presentations. This can greatly streamline the research pipeline and help you publish your results and associated code in a quick and efficient fashion.\nHead over the the RStudio download website and download “RStudio Desktop”, which is free. Be sure to pick the correct version for your machine.\nOpen RStudio on your machine. You should be presented with something like the following:\n\n\n\nRStudio start window. Note that the screen is split into four different quadrants. Top left: R documents; bottom left: R program; top right: environment window; bottom right: plots, help, and directories.\n\n\nIn RStudio, the top left window is always going to be our coding window. This is where we will type all of our code and create our documents. In the bottom left we will see R executing the code. This will show what the computer is “thinking” and will help us spot any potential issues. The top right window is the “environment”, which shows what variables and datasets are stored within the computers’ memory. (It can also show some other things, but we aren’t concerned with that at this point). The bottom right window is the “display” window. This is where plots and help windows will appear if they don’t appear in the document (top left) window itself.\nNow, we will create our first R document!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#setup-1",
    "href": "intro_to_r.html#setup-1",
    "title": "1  Intro to R",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nIn this class, we will be creating assignments in what is called RMarkdown. This is a rich-text version of R that allows us to create documents with the code embedded. In RStudio, click the “+” button in the far top left to open the New Document menu. Scroll down this list and click on R Markdown.\nA screen such as this will appear:\n\n\n\nA new file window for an RMarkdown file.\n\n\nAfter entering a title and your name and selecting document in the left hand menu, click OK.\n\n\n\nAn example of a markdown script.\n\n\nIn the image above, we can see what a “default” RMarkdown script looks like after creating the file. At the top of the document, between all of the dashes, we have the yaml header that tells R what kind of document will be created, who the author is, and tells it to use today’s date. In this class, we will be saving documents as html as they are the easiest documents to create and save. These documents will include all of your code, text, and even any plots you may create!\nPlain text in the document will be rendered as plain text in the document. (I.e., whatever you type normally will become “normal text” in the finished document). Lines preceded with # will become headers, with ## being a second level header and ### being a third level header, etc. Words can also be made italic by putting an asterisk on each side of the word (*italic*) and bold by putting two asterisks on each side (**bold**). URLs are also supported, with &lt;&gt; on each side of a URL making it clickable, and words being hyperlinked by typing [words to show](target URL).\nWe also have code “chunks” that are shown above. A code chunk can be manually typed out or inserted by pressing CTRL + ALT + I (Windows, Linux) or COMMAND + OPTION + I (Mac). Everything inside a “code chunk” will be read as R code and executed as such. Note that you can have additional commands in the R chunks, but we won’t cover that for now.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#using-code-chunks",
    "href": "intro_to_r.html#using-code-chunks",
    "title": "1  Intro to R",
    "section": "3.2 Using code chunks",
    "text": "3.2 Using code chunks\nIn your computer, erase all information except for the yaml header between the dashes on your computer. Save your file in a folder where you want your assignment to be located. It is important you do this step up front as the computer will sometimes save in random places if you don’t specify a file location at the beginning. Don’t forget to save your work frequently!\n\n\n\nText to type in your Rmarkdown document.\n\n\nAfter typing this into the document, hit knit near the top of the upper left window. R will now create an HTML document that should look like this:\n\n\n\nThe output from the above code knitted into a document.\n\n\nWe can see now that the HTML document has the title of the document, the author’s name, the date on which the code was run, and a greyed-out box with color coded R code followed by the output. Let’s try something a little more complex. Create a new code chunk and type the following:\n\nx &lt;- 1:10\n\nThis will create a variable in R, x, that is sequentially each whole number between 1 and 10. We can see this by highlighting or typing only the letter x and running that line of code by clicking CTRL + ENTER (Windows / Linux) or COMMAND + ENTER (Mac).\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIf you look at the top right window, you will also see the value x in the environment defined as int [1:10] 1 2 3 4 5 6 7 8 9 10. This indicates that x is integer data spanning ten positions numbered 1 to 10. Since the vector is small, it displays every number in the sequence.\n\n\n\nRStudio environment window showing saved objects. These are in the computer’s memory.\n\n\nLet’s create another vector y that is the squared values of x, such that \\(y=x^2\\). We can raise values to an exponent by using ^.\n\ny &lt;- x^2\ny\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nNow we have the value y in the environment that is the square of the values of x. This is a numeric vector of 10 values numbered 1 to 10 where each value corresponds to a square of the x value. We can raise things to any value however, including \\(x^x\\)!\n\nx^x\n\n [1]           1           4          27         256        3125       46656\n [7]      823543    16777216   387420489 10000000000\n\n\nAs we can see, since I didn’t “store” this value as a variable in R using &lt;-, the value is not in the environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#plotting",
    "href": "intro_to_r.html#plotting",
    "title": "1  Intro to R",
    "section": "3.3 Plotting",
    "text": "3.3 Plotting\nNow, let’s try creating a plot. This is easy in R, as we just use the command plot.\n\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nBy specifying the y and x components in plot, we can quickly generate a point plot. We can alter the visual parameters of this plot using a few different commands. I will outline these below with inline notes. Inline notes in the code can be made by using a # symbol before them, which basically tells R to ignore everything after the #. For example:\n\nprint(\"Test\")\n\n[1] \"Test\"\n\n# print(\"Test 2\")\n\nThis prints the word Test, but doesn’t print Test 2.\nNow let’s make the plot with some new visual parameters.\n\nplot(x = x, # specify x values\n     y = y, # specify y values\n     ylab = \"Y Values\", # specify Y label\n     xlab = \"X Values\", # specify X label\n     main = \"Plot Title\", # specify main title\n     pch = 19, # adjust point style\n     col = \"red\") # make points red",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#tab-complete",
    "href": "intro_to_r.html#tab-complete",
    "title": "1  Intro to R",
    "section": "3.4 Tab complete",
    "text": "3.4 Tab complete\nRStudio allows for “tab-completing” while typing code. Tab-completing is a way of typing the first part of a command, variable name, or file name and hitting “tab” to show all options with that spelling. You should use tab completing because it:\n\nreduces spelling mistakes\nreduces filepath mistakes\nincreases the speed at which you code\nprovides help with specific functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#help",
    "href": "intro_to_r.html#help",
    "title": "1  Intro to R",
    "section": "3.5 Help",
    "text": "3.5 Help\nAt any point in R, you can look up “help” for a specific function by typing ?functionname. Try this on your computer with the following:\n\n?mean",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#downloading-data",
    "href": "intro_to_r.html#downloading-data",
    "title": "1  Intro to R",
    "section": "4.1 Downloading data",
    "text": "4.1 Downloading data\nNow, we need to download our first data set. These datasets are stored on GitHub. We are going to be looking at data from Dr. Cooper’s dissertation concerning Afrotropical bird distributions (Cooper 2021). This website is in the data folder on this websites’ GitHub page, accessible here.\n\n# read comma separated file (csv) into R memory\n# reads directly from URL\nranges &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/datasets/lacustrine_range_size.csv\")\n\nRows: 12 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (9): combined_current_km2, consensus_km2, bioclim_current_km2, 2050_comb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAlternatively, we can use the operator %&gt;% to simplify this process. %&gt;% means “take whatever you got from the previous step and pipe it into the next step”. So, the following does the exact same thing:\n\nranges &lt;- \"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/datasets/lacustrine_range_size.csv\" %&gt;%\n  read_csv()\n\nRows: 12 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (9): combined_current_km2, consensus_km2, bioclim_current_km2, 2050_comb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUsing the %&gt;% is preferred as you can better set up a workflow and because it more closely mimics other coding languages, such as bash.\nLet’s view the data to see if it worked. We can use the command head to view the first few rows:\n\nhead(ranges)\n\n# A tibble: 6 × 10\n  species                combined_current_km2 consensus_km2 bioclim_current_km2\n  &lt;chr&gt;                                 &lt;dbl&gt;         &lt;dbl&gt;               &lt;dbl&gt;\n1 Batis_diops                          25209.         6694.              19241.\n2 Chamaetylas_poliophrys               68171.         1106.              68158.\n3 Cinnyris_regius                      60939.        13305.              53627.\n4 Cossypha_archeri                     27021.         6409.              11798.\n5 Cyanomitra_alinae                    78680.        34320.              63381.\n6 Graueria_vittata                      8770.          861.               8301.\n# ℹ 6 more variables: `2050_combined_km2` &lt;dbl&gt;, `2050_consensus_km2` &lt;dbl&gt;,\n#   `2070_combined_km2` &lt;dbl&gt;, `2070_consensus_km2` &lt;dbl&gt;,\n#   alltime_consensus_km2 &lt;dbl&gt;, past_stable_km2 &lt;dbl&gt;\n\n\nWe can perform a lot of summary statistics in R. Some of these we can view for multiple columns at once using summary.\n\nsummary(ranges)\n\n   species          combined_current_km2 consensus_km2     bioclim_current_km2\n Length:12          Min.   :  8770       Min.   :  861.3   Min.   :  3749     \n Class :character   1st Qu.: 24800       1st Qu.: 4186.2   1st Qu.: 10924     \n Mode  :character   Median : 43654       Median : 7778.1   Median : 31455     \n                    Mean   : 68052       Mean   :18161.8   Mean   : 42457     \n                    3rd Qu.: 70798       3rd Qu.:18558.7   3rd Qu.: 62835     \n                    Max.   :232377       Max.   :79306.6   Max.   :148753     \n 2050_combined_km2 2050_consensus_km2 2070_combined_km2  2070_consensus_km2\n Min.   :  1832    Min.   :    0.0    Min.   :   550.3   Min.   :    0.0   \n 1st Qu.:  6562    1st Qu.:  589.5    1st Qu.:  6583.8   1st Qu.:  311.4   \n Median : 26057    Median : 6821.9    Median : 24281.7   Median : 2714.6   \n Mean   : 33247    Mean   :14418.4    Mean   : 31811.0   Mean   : 8250.5   \n 3rd Qu.: 40460    3rd Qu.:18577.1    3rd Qu.: 38468.9   3rd Qu.:10034.4   \n Max.   :132487    Max.   :79236.2    Max.   :129591.0   Max.   :53291.8   \n alltime_consensus_km2 past_stable_km2 \n Min.   :    0.0       Min.   :   0.0  \n 1st Qu.:  790.9       1st Qu.:   0.0  \n Median : 8216.8       Median :   0.0  \n Mean   :15723.3       Mean   : 127.3  \n 3rd Qu.:19675.0       3rd Qu.:   0.0  \n Max.   :82310.5       Max.   :1434.8  \n\n\nAs seen above, we now have information for the following statistics for each variable:\n\nMin = minimum\n1st Qu. = 1st quartile\nMedian = middle of the dataset\nMean = average of the dataset\n3rd Qu. = 3rd quartile\nMax. = maximum\n\nWe can also calculate some of these statistics manually to see if we are doing everything correctly. It is easiest to do this by using predefined functions in R (code others have written to perform a particular task) or to create our own functions in R. We will do both to determine the average of combined_current_km2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#subsetting-data",
    "href": "intro_to_r.html#subsetting-data",
    "title": "1  Intro to R",
    "section": "4.2 Subsetting data",
    "text": "4.2 Subsetting data\nFirst, we need to select only the column of interest. In R, we have two ways of subsetting data to get a particular column.\n\nvar[rows,cols] is a way to look at a particular object (var in this case) and choose a specific combination of row number and column number (col). This is great if you know a specific index, but it is better to use a specific name.\nvar[rows,\"cols\"] is a way to do the above but by using a specific column name, like combined_current_km2.\nvar$colname is a way to call the specific column name directly from the dataset.\n\n\n# using R functions\n\nranges$combined_current_km2\n\n [1]  25209.4  68171.2  60939.2  27021.3  78679.9   8769.9 232377.2  17401.4\n [9]  51853.5  35455.1  23570.3 187179.1\n\n\nAs shown above, calling the specific column name with $ allows us to see only the data of interest. We can also save these data as an object.\n\ncurrent_combined &lt;- ranges$combined_current_km2\n\ncurrent_combined\n\n [1]  25209.4  68171.2  60939.2  27021.3  78679.9   8769.9 232377.2  17401.4\n [9]  51853.5  35455.1  23570.3 187179.1\n\n\nNow that we have it as an object, specifically a numeric vector, we can perform whatever math operations we need to on the dataset.\n\nmean(current_combined)\n\n[1] 68052.29\n\n\nHere, we can see the mean for the entire dataset. However, we should always round values to the same number of decimal points as the original data. We can do this with round.\n\nround(mean(current_combined),1) # round mean to one decimal\n\n[1] 68052.3\n\n\nNote that the above has a nested set of commands. We can write this exact same thing as follows:\n\n# pipe mean through round\nmean(current_combined) %&gt;%\n  round(1)\n\n[1] 68052.3\n\n\nUse the method that is easiest for you to follow!\nWe can also calculate the mean manually. The mean is \\(\\frac{\\sum_{i=1}^nx}{n}\\), or the sum of all the values within a vector divided by the number of values in that vector.\n\n# create function\n# use curly brackets to denote function\n# our data goes in place of \"x\" when finally run\nour_mean &lt;- function(x){\n  sum_x &lt;- sum(x) # sum all values in vector\n  n &lt;- length(x) # get length of vector\n  xbar &lt;- sum_x/n # calcualte mean\n  return(xbar) # return the value outside the function\n}\n\nLet’s try it.\n\nour_mean(ranges$combined_current_km2)\n\n[1] 68052.29\n\n\nAs we can see, it works just the same as mean! We can round this as well.\n\nour_mean(ranges$combined_current_km2) %&gt;%\n  round(1)\n\n[1] 68052.3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "2.1 Purposes of descriptive statistics\nDescriptive statistics enable researchers to quickly and easily examine the “behavior” of their datasets, identifying potential errors and allowing them to observe particular trends that may be worth further analysis. Here, we will cover how to calculate descriptive statistics for multiple different datasets, culminating in an assignment covering these topics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#preparing-r",
    "href": "descriptive_stats.html#preparing-r",
    "title": "2  Descriptive Statistics",
    "section": "2.2 Preparing R",
    "text": "2.2 Preparing R\nAs with every week, we will need to load our relevant packages first. This week, we are using the following:\n\n# enables data management tools\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#downloading-the-data",
    "href": "descriptive_stats.html#downloading-the-data",
    "title": "2  Descriptive Statistics",
    "section": "2.3 Downloading the data",
    "text": "2.3 Downloading the data\nFor the example this week, we will be using the starbucks dataset, describing the number of drinks purchased during particular time periods during the day.\n\nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol105_unk/main/datasets/starbucks.csv\")\n\nRows: 9 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Hour\ndbl (1): Frap_Num\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#descriptive-statistics",
    "href": "descriptive_stats.html#descriptive-statistics",
    "title": "2  Descriptive Statistics",
    "section": "2.4 Descriptive statistics",
    "text": "2.4 Descriptive statistics\nDescriptive statistics are statistics that help us understand the shape and nature of the data on hand. These include really common metrics such as mean, median, and mode, as well as more nuanced metrics like quartiles that help us understand if there is any skew in the dataset. (Skew refers to a bias in the data, where more data points lie on one side of the distribution and there is a long tail of data in the other direction).\n\n\n\nExamples of skew compared to a symmetrical, non-skewed distribution. Source: machinelearningparatodos.com\n\n\nNote above that the relative position of the mean, median, and mode can be indicative of skew. Please also note that these values will rarely be exactly equal “in the real world”, and thus you need to weigh differences against the entire dataset when assessing skew. There is a lot of nuance like this in statistics; it is not always an “exact” science, but sometimes involves judgment calls and assessments based on what you observe in the data.\nUsing the starbucks dataset, we can look at some of these descriptive statistics to understand what is going on.\n\n2.4.1 Notation\nAs a quick reminder, we use Greek lettering for populations and Roman lettering for samples. For example:\n\n\\(\\sigma\\) is a population, but \\(s\\) is a sample (both these variables refer to standard deviation).\n\\(\\mu\\) is a population, but \\(\\bar{x}\\) is a sample (both of these variables refer to the mean).\n\n\n\n2.4.2 Mean\nThe mean is the “average” value within a set of data, specifically, the sum of all values divided by the length of those values: \\(\\frac{\\sum_{i=1}^nx}{n}\\).\n\nhead(starbucks)\n\n# A tibble: 6 × 2\n  Hour      Frap_Num\n  &lt;chr&gt;        &lt;dbl&gt;\n1 0600-0659        2\n2 0700-0759        3\n3 0800-0859        2\n4 0900-0959        4\n5 1000-1059        8\n6 1100-1159        7\n\n\nHere, we are specifically interested in the number of frappuccinos.\n\n# get vector of frappuccino number\nfraps &lt;- starbucks$Frap_Num\n\n# get mean of vector\nmean(fraps)\n\n[1] 6.222222\n\n\nNote that the above should be rounded to a whole number, since we were given the data in whole numbers!\n\nmean(fraps) %&gt;%\n  round(0)\n\n[1] 6\n\n# OR\n\nround(mean(fraps),0)\n\n[1] 6\n\n\nWe already covered calculating the average manually in our previous tutorial, but we can do that here as well:\n\n# sum values\n# divide by n, length of vector\n# round to 0 places\nround(sum(fraps)/length(fraps),0)\n\n[1] 6\n\n\n\n\n2.4.3 Range\nThe range is the difference between the largest and smallest units in a dataset. We can use the commands min and max to calculate this.\n\nmax(fraps) - min(fraps)\n\n[1] 13\n\n\nThe range of our dataset is 13.\n\n\n2.4.4 Median\nThe median is also known as the 50th percentile, and is the midpoint of the data when ordered from least to greatest. If there are an even number of data points, then it is the average point between the two center points. For odd data, this is the \\(\\frac{n+1}{2}\\)th observation. For even data, since we need to take an average, this is the \\(\\frac{\\frac{n}{2}+(\\frac{n}{2}+1)}{2}\\). You should be able to do these by hand and by using a program.\n\nmedian(fraps)\n\n[1] 4\n\n\nNow, to calculate by hand:\n\nlength(fraps)\n\n[1] 9\n\n\nWe have an odd length.\n\n# order gets the order\norder(fraps)\n\n[1] 1 3 7 2 4 6 5 9 8\n\n\n\n# [] tells R which elements to put where\nfrap_order &lt;- fraps[order(fraps)]\n\nfrap_order\n\n[1]  2  2  2  3  4  7  8 13 15\n\n\n\n# always use parentheses\n# make sure the math maths right!\n(length(frap_order)+1)/2\n\n[1] 5\n\n\nWhich is the fifth element in the vector?\n\nfrap_order[5]\n\n[1] 4\n\n\nNow let’s try it for an even numbers.\n\n# remove first element\neven_fraps &lt;- fraps[-1]\n\neven_fraps_order &lt;- even_fraps[order(even_fraps)]\n\neven_fraps_order\n\n[1]  2  2  3  4  7  8 13 15\n\n\n\nmedian(even_fraps)\n\n[1] 5.5\n\n\nNow, by hand: \\(\\frac{\\frac{n}{2}+(\\frac{n}{2}+1)}{2}\\).\n\nn &lt;- length(even_fraps_order)\n\n# get n/2 position from vector\nm1 &lt;- even_fraps_order[n/2]\n# get n/2+1 position\nm2 &lt;- even_fraps_order[(n/2)+1]\n\n# add these values, divide by two for \"midpoint\"\nmed &lt;- (m1+m2)/2\n\nmed\n\n[1] 5.5\n\n\nAs we can see, these values are equal!\n\n\n2.4.5 Other quartiles and quantiles\nWe also use the 25th percentile and the 75th percentile to understand data distributions. These are calculated similar to the above, but the bottom quartile is only \\(\\frac{1}{4}\\) of the way between values and the 75th quartile is \\(\\frac{3}{4}\\) of the way between values. We can use the R function quantile to calculate these.\n\nquantile(frap_order)\n\n  0%  25%  50%  75% 100% \n   2    2    4    8   15 \n\n\nWe can specify a quantile as well:\n\nquantile(frap_order, 0.75)\n\n75% \n  8 \n\n\nWe can also calculate these metrics by hand. Let’s do it for the even dataset, since this is more difficult.\n\nquantile(even_fraps_order)\n\n   0%   25%   50%   75%  100% \n 2.00  2.75  5.50  9.25 15.00 \n\n\nNote that the 25th and 75th percentiles are also between two different values. These can be calculated as a quarter and three-quarters of the way between their respective values.\n\n# 75th percentile\n\nn &lt;- length(even_fraps_order)\n\n# get position\np &lt;- 0.75*(n+1)\n\n# get lower value\n# round down\nm1 &lt;- even_fraps_order[trunc(p)]\n\n# get upper value\n# round up\nm2 &lt;- even_fraps_order[ceiling(p)]\n\n# position between\n# fractional portion of rank\nfrac &lt;- p-trunc(p)\n\n# calculate the offset from lowest value\nval &lt;- (m2 - m1)*frac\n\n# get value\nm1 + val\n\n[1] 11.75\n\n\nWait… why does our value differ?\nR, by default, calculates quantiles using what is called Type 7, in which the quantiles are calculated by \\(p_k = \\frac{k-1}{n-1}\\), where \\(n\\) is the length of the vector and \\(k\\) refers to the quantile being used. However, in our book and in this class, we use Type 6 interpretation - \\(p_k = \\frac{k}{n + 1}\\). Let’s try using Type 6:\n\nquantile(even_fraps_order, type = 6)\n\n   0%   25%   50%   75%  100% \n 2.00  2.25  5.50 11.75 15.00 \n\n\nNow we have the same answer as we calculated by hand!\nThis is a classic example of how things in R (and in statistics in general!) can depend on interpretation and are not always “hard and fast” rules.\nIn this class, we will be using Type 6 interpretation for the quantiles - you will have to specify this in the quantile function EVERY TIME! If you do not specify Type 6, you will get the questions incorrect and you will get answers that do not agree with the book, with Excel, or what you calculate by hand.\n\n\n2.4.6 Mode\nThere is no default method for finding the mode in R. However, websites like Statology provide wraparound functions.\n\n# Based on Statology function\n# define function to calculate mode\nfind_mode &lt;- function(x) {\n  # get unique values from vector\n  u &lt;- unique(x)\n  # count number of occurrences for each value\n  tab &lt;- tabulate(match(x, u))\n  \n  # if no mode, say so\n  if(length(x)==length(u[tab == max(tab)])){\n    print(\"No mode.\")\n  }else{\n    # return the value with the highest count\n    u[tab == max(tab)]\n  }\n}\n\nfind_mode(fraps)\n\n[1] 2\n\n\nWe can also do this by hand, by counting the number of occurrences of each value. This can be done in a stepwise fashion using commands in the above function.\n\n# unique counts\nu &lt;- unique(fraps)\nu\n\n[1]  2  3  4  8  7 15 13\n\n\n\n# which elements match\nmatch(fraps,u)\n\n[1] 1 2 1 3 4 5 1 6 7\n\n\n\n# count them\ntab &lt;- match(fraps,u) %&gt;%\n  tabulate()\n\ntab\n\n[1] 3 1 1 1 1 1 1\n\n\nGet the highest value.\n\nu[tab==max(tab)]\n\n[1] 2\n\n\nNotice this uses ==. This is a logical argument that means “is equal to” or “is the same as”. For example:\n\n2 == 2\n\n[1] TRUE\n\n\nThese values are the same, so TRUE is returned.\n\n2 == 3\n\n[1] FALSE\n\n\nThese values are unequal, so FALSE is returned. R will read TRUE as 1 and FALSE as ZERO, such that:\n\nsum(2==2)\n\n[1] 1\n\n\nand\n\nsum(2==3)\n\n[1] 0\n\n\nThis allows you to find how many arguments match your condition quickly, and even allows you to subset based on these indices as well. Keep in mind you can use greater than &lt;, less than &gt;, greater than or equal to &lt;=, less than or equal to &gt;=, is equal to ==, and is not equal to != to identify numerical relationships. Other logical arguments include:\n\n&: both conditions must be TRUE to match (e.g., c(10,20) & c(20,10)). Try the following as well: fraps &lt; 10 & fraps &gt; 3.\n&&: and, but works with single elements and allows for better parsing. Often used with if. E.g., fraps &lt; 10 && fraps &gt; 3. This will not work on our multi-element frap vector.\n|: or, saying at least one condition must be true. Try: fraps &gt; 10 | fraps &lt; 3.\n||: or, but for a single element, like && above.\n!: not, so “not equal to” would be !=.\n\n\n\n2.4.7 Variance\nWhen we are dealing with datasets, the variance is a measure of the total spread of the data. The variance is calculated using the following:\\[\\sigma^2=\\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\]\nEssentially, this means that for every value of \\(x\\), we are finding the difference between that value and the mean and squaring it, summing all of these quared differences, and dividing them by the number of samples in the dataset minus one. Let’s do this for the frappuccino dataset.\n\nfrap_order\n\n[1]  2  2  2  3  4  7  8 13 15\n\n\nNow to find the differences.\n\ndiffs &lt;- frap_order - mean(frap_order)\n\ndiffs\n\n[1] -4.2222222 -4.2222222 -4.2222222 -3.2222222 -2.2222222  0.7777778  1.7777778\n[8]  6.7777778  8.7777778\n\n\nNote that R is calculating the same thing for the entire vector! Since these are differences from the mean, they should sum to zero.\n\nsum(diffs)\n\n[1] 3.552714e-15\n\n\nThis is not quite zero due to rounding error, but is essentially zero as it is 0.0000000000000036.\n\n# square differences\ndiffs_sq &lt;- diffs^2\n\ndiffs_sq\n\n[1] 17.8271605 17.8271605 17.8271605 10.3827160  4.9382716  0.6049383  3.1604938\n[8] 45.9382716 77.0493827\n\n\nNow we have the squared differences. We need to sum these and divide by \\(n-1\\).\n\nn &lt;- length(frap_order)\n\nvar_frap &lt;- sum(diffs_sq)/(n-1)\n\nvar_frap\n\n[1] 24.44444\n\n\nLet’s check this against the built-in variance function in R.\n\nvar(frap_order)\n\n[1] 24.44444\n\n\nThey are identical! We can check this using a logical argument.\n\nvar_frap == var(frap_order)\n\n[1] TRUE\n\n\nSeeing as this is TRUE, we calculated it correctly.\n\n\n2.4.8 Standard deviation\nAnother common measurement of spread is the standard deviation (\\(\\sigma\\)). As you remember from class (or may have guessed from the notation on this site), the standard deviation is just the square root of the variance.\n\nsqrt(var_frap)\n\n[1] 4.944132\n\n\nWe can test this against the built in sd function in R:\n\nsqrt(var_frap) == sd(frap_order)\n\n[1] TRUE\n\n\nAs you can see, we calculated this correctly!\n\n\n2.4.9 Standard error\nThe standard error is used to help understand the spread of data and to help estimate the accuracy of our measurements for things like the mean. The standard error is calculated thusly:\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nThere is not built in function for the standard error in excel, but we can write our own:\n\nse &lt;- function(x){\n  n &lt;- length(x) # calculate n\n  s &lt;- sd(x) # calculate standard deviation\n  se_val &lt;- s/sqrt(n)\n  return(se_val)\n}\n\nLet’s test this code.\n\nse(frap_order)\n\n[1] 1.648044\n\n\nOur code works! And we can see exactly how the standard error is calculate. We can also adjust this code as needed for different situations, like samples.\nRemember, the standard error is used to help reflect our confidence in a specific measurement (e.g., how certain we are of the mean, and what values we believe the mean falls between). We want our estimates to be as precise as possible with as little uncertainty as possible. Given this, does having more samples make our estimates more or less confident? Mathematically, what happens as our sample size increases?\n\n\n2.4.10 Coefficient of variation\nThe coefficient of variation, another measure of data spread and location, is calculated by the following:\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\]\nWe can write a function to calculate this in R as well.\n\ncv &lt;- function(x){\n  sigma &lt;- sd(x)\n  mu &lt;- mean(x)\n  val &lt;- sigma/mu\n  return(val)\n}\n\ncv(frap_order)\n\n[1] 0.7945927\n\n\nRemember that we will need to round values.\n\n\n2.4.11 Outliers\nOutliers are any values that are outside of the 1.5 times the interquartile range. We can calculate this for our example dataset as follows:\n\nlowquant &lt;- quantile(frap_order,0.25,type = 6) %&gt;% as.numeric()\n\nhiquant &lt;- quantile(frap_order,0.75,type = 6) %&gt;% as.numeric()\n\niqr &lt;- hiquant - lowquant\n\nWe can also calculate the interquartile range using IQR. Remember, you must use type = 6!\n\niqr &lt;- IQR(frap_order, type = 6)\n\nNow, to calculate the “whiskers”.\n\nlowbound &lt;- lowquant - (1.5*iqr)\nhibound &lt;- hiquant + (1.5*iqr)\n\n# low outliers?\n# select elements that match\n# identify using logical \"which\"\nfrap_order[which(frap_order &lt; lowbound)]\n\nnumeric(0)\n\n\n\n# high outliers?\n# select elements that match\n# identify using logical \"which\"\nfrap_order[which(frap_order &gt; hibound)]\n\nnumeric(0)\n\n\nWe have no outliers for this particular dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#homework-descriptive-statistics",
    "href": "descriptive_stats.html#homework-descriptive-statistics",
    "title": "2  Descriptive Statistics",
    "section": "2.5 Homework: Descriptive Statistics",
    "text": "2.5 Homework: Descriptive Statistics\nNow that we’ve covered these basic statistics, it’s your turn! For this week, you will be completing homework that involves methods from Chapter 4 in your book.\n\n2.5.1 Homework instructions\nPlease create an RMarkdown document that will render as an .html file. You will submit this file to show your coding and your work. Please refer to the Introduction to R for refreshers on how to create an .html document in RMarkdown. You will need to do the following for each of these datasets:\n\nmean\nmedian\nrange\ninterquartile range\nvariance\nstandard deviation\ncoefficient of variation\nstandard error\nwhether there are any “outliers”\n\nPlease show all of your work for full credit.\n\n\n2.5.2 Data for homework problems\nFor each question, calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers”.\nPlease also write your own short response to the Synthesis question posed, which will involve thinking about the data and metrics you just analyzed.\n\n2.5.2.1 1: UNK Nebraskans\nEver year, the university keeps track of where students are from. The following are data on the umber of students admitted to UNK from the state of Nebraska:\n\n# create dataset in R\nnebraskans &lt;- c(5056,5061,5276,5244,5209,\n                5262,5466,5606,5508,5540,5614)\n\nyears &lt;- 2023:2013\n\nnebraskans_years &lt;- cbind(years,nebraskans) %&gt;% \n  as.data.frame()\n\nnebraskans_years\n\n   years nebraskans\n1   2023       5056\n2   2022       5061\n3   2021       5276\n4   2020       5244\n5   2019       5209\n6   2018       5262\n7   2017       5466\n8   2016       5606\n9   2015       5508\n10  2014       5540\n11  2013       5614\n\n\nUsing these data, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers” for the number of UNK students from Nebraska.\nIn order to do this, we will need to first select the column that denotes the number of Nebraskans from the dataset. Remember, we need to save this as an object in R to do the analyses. Here is a demonstration of getting the column to look at the mean, so that you can repeat this for the other questions. This relies heavily on the use of $, used to get the data from a specific column.\n\n# $ method\nnebraskans &lt;- nebraskans_years$nebraskans\n\nnebraskans\n\n [1] 5056 5061 5276 5244 5209 5262 5466 5606 5508 5540 5614\n\n\nNow we can get the mean of this vector.\n\nmean(nebraskans) %&gt;%\n  round(0) # don't forget to round!\n\n[1] 5349\n\n\nSynthesis question: Do you think that there are any really anomalous years? Do you feel data are similar between years? Note we are not looking at trends through time but whether any years are outliers.\n\n\n2.5.2.2 2: Piracy in the Gulf of Guinea\nThe following is a dataset looking at oceanic conditions and other variables associated with pirate attacks within the region between 2010 and 2021 (Moura et al. 2023). Using these data, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers” for distance from shore for each pirate attack (column Distance_from_Coast).\n\npirates &lt;- read_csv(\"https://figshare.com/ndownloader/files/42314604\")\n\nNew names:\nRows: 595 Columns: 40\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(18): Period, Season, African_Season, Coastal_State, Coastal_Zone, Navi... dbl\n(20): ...1, Unnamed: 0, Year, Month, Lat_D, Lon_D, Distance_from_Coast,... dttm\n(2): Date_Time, Date\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nSynthesis question: Do you notice any patterns in distance from shore? What may be responsible for these patterns? Hint: Think about what piracy entails and also what other columns are available as other variables in the above dataset.\n\n\n2.5.2.3 3: Patient ages at presentation\nThe following is a dataset on skin sores in different communities in Australia and Oceania, specifically looking at the amount of time that passes between skin infections (Lydeamore et al. 2020a). This file includes multiple different datasets, and focuses on data from children in the first five years of their life, on househould visits, and on data collected during targeted studies (Lydeamore et al. 2020b).\n\nages &lt;- read_csv(\"https://doi.org/10.1371/journal.pcbi.1007838.s006\")\n\nRows: 17150 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): dataset\ndbl (1): time_difference\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s see what this file is like real fast. We can use the command dim to see the rows and columns.\n\ndim(ages)\n\n[1] 17150     2\n\n\nAs you can see, this file has only two columns but 17,150 rows! For the column time_difference, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers”.\nSynthesis question: Folks will often think about probabilities of events being “low but never zero”. What does that mean in the context of these data? What about these data make you feel like probabilities may decrease through time but never become zero?\n\n\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W. Cuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon, S. Y. C. Tong, and J. M. McCaw (2020b). Patient ages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W. Cuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon, S. Y. C. Tong, and J. M. McCaw (2020a). Estimation of the force of infection and infectious period of skin sores in remote Australian communities using interval-censored data. PLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset. https://doi.org/10.6084/m9.figshare.24119643.v1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "visual.html",
    "href": "visual.html",
    "title": "3  Diagnosing data visually",
    "section": "",
    "text": "3.1 The importance of visual inspection\nInspecting data visually can give us a lot of information about whether data are normally distributed and about whether there are any major errors or issues with our dataset. It can also help us determine if data meet model assumptions, or if we need to use different tests more appropriate for our datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#sample-data-and-preparation",
    "href": "visual.html#sample-data-and-preparation",
    "title": "3  Diagnosing data visually",
    "section": "3.2 Sample data and preparation",
    "text": "3.2 Sample data and preparation\nFirst, we need to load our R libraries.\n\nlibrary(curl)\n\nUsing libcurl 8.7.1 with LibreSSL/3.3.6\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::parse_date() masks curl::parse_date()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNext, we can download our data sample.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#histograms",
    "href": "visual.html#histograms",
    "title": "3  Diagnosing data visually",
    "section": "3.3 Histograms",
    "text": "3.3 Histograms\nA histogram is a frequency diagram that we can use to visually diagnose data and their distributions. We are going to examine a histogram using a random string of data. R can generate random (though, actually pseudorandom) strings of data on command, pulling them from different distributions. These distributions are pseudorandom because we can’t actually program R to be random, so it starts from a wide variety of pseudorandom points.\n\n3.3.1 Histograms on numeric vectors\nThe following is how to create default histograms on data. If you need to create custom bin sizes, please see the notes under Cumulative frequency plots for data that are not already in frequency format.\n\n# create random string from normal distribution\n# this step is not necessary for data analysis in homework\nx &lt;- rnorm(n = 1000, # 1000 values\n           mean = 0,\n           sd = 1)\n\n# make histogram\nhist(x)\n\n\n\n\n\n\n\n\nNOTE that a histogram can only be made on a vector of values. If you try to make a histogram on a data frame, you will get an error and it will not work. You have to specify which column you wish to use with the $ operator. (For example, for dataframe xy with columns x and y, you would use hist(xy$y)).\nWe can up the number of bins to see this better.\n\nhist(x,breaks = 100)\n\n\n\n\n\n\n\n\nThe number of bins can be somewhat arbitrary, but a value should be chosen based off of what illustrates the data well. R will auto-select a number of bins in some cases, but you can also select a number of bins. Some assignments will ask you to choose a specific number of bins as well.\n\n\n3.3.2 Histograms on frequency counts\nSay, for example, that we have a dataset where everything is already shown as frequencies. We can create a frequency histogram using barplot.\n\ncount_table &lt;- matrix(nrow = 4, ncol = 2, byrow = T,\n                      data = c(\"Cat 1\", 4,\n                               \"Cat 2\", 8,\n                               \"Cat 3\", 7,\n                               \"Cat 4\", 3)) %&gt;%\n  as.data.frame()\n\ncolnames(count_table) &lt;- c(\"Category\",\"Count\")\n\n# ensure counts are numeric data\ncount_table$Count &lt;- as.numeric(count_table$Count)\n\n# manually create histogram\nbarplot(count_table$Count, # response variable, counts for histogram\n        axisnames = T, # make names on plot\n        names.arg = count_table$Category) # make these the names\n\n\n\n\n\n\n\n\n\n\n3.3.3 ggplot histograms\nThe following is an optional workthrough on how to make really fancy plots.\nWe can also use the program ggplot, part of the tidyverse, to create histograms.\n\n# ggplot requires data frames\nx2 &lt;- x %&gt;% as.data.frame()\ncolnames(x2) &lt;- \"x\"\n\nggplot(data = x2, aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot is nice because we can also clean up this graph a little.\n\nggplot(x2,aes(x=x)) + geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can also do a histogram of multiple values at once in R.\n\nx2$cat &lt;- \"x\"\n\ny &lt;- rnorm(n = 1000,\n           mean = 1,\n           sd = 1) %&gt;%\n  as.data.frame()\n\ncolnames(y) &lt;- \"x\"\ny$cat &lt;- \"y\"\n\nxy &lt;- rbind(x2,y)\n\nhead(xy)\n\n            x cat\n1  0.72409742   x\n2  1.35841773   x\n3  1.13821938   x\n4 -0.34692767   x\n5 -0.06186526   x\n6  0.40325559   x\n\n\n\nggplot(xy,aes(x = x, fill = cat)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can also make this look a little nicer.\n\nggplot(xy, aes(x = x, colour = cat)) +\n  geom_histogram(fill = \"white\", alpha = 0.5, # transparency\n                 position = \"identity\") +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can show these a little differently as well.\n\nggplot(xy, aes(x = x, fill = cat))+\n  geom_histogram(position = \"identity\", alpha = 0.5) +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThere are lots of other commands you can incorporate as well if you so choose; I recommend checking sites like this one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#boxplots",
    "href": "visual.html#boxplots",
    "title": "3  Diagnosing data visually",
    "section": "3.4 Boxplots",
    "text": "3.4 Boxplots\nWe can also create boxplots to visualize the spread of the data. Boxplots include a bar for the median, a box representing the interquartile range between the 25th and 75th percentiles, and whiskers that extend \\(1.5 \\cdot IQR\\) beyond the 25th and 75th percentiles. We can create a boxplot using the command boxplot.\n\n# using pre-declared variable x\n\nboxplot(x)\n\n\n\n\n\n\n\n\nWe can set the axis limits manually as well.\n\nboxplot(x, # what to plot\n        ylim = c(-4, 4), # set y limits\n        pch = 19) # make dots solid\n\n\n\n\n\n\n\n\nOn the above plot, outliers for the dataset are shown as dots beyond the ends of the “whiskers”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#skewness",
    "href": "visual.html#skewness",
    "title": "3  Diagnosing data visually",
    "section": "3.5 Skewness",
    "text": "3.5 Skewness\nSkew is a measure of how much a dataset “leans” to the positive or negative directions (i.e., to the “left” or to the “right”). To calculate skew, we are going to use the moments library.\n\n# don't forget to install if needed!\nlibrary(moments)\n\nskewness(x)\n\n[1] 0.002085201\n\n\nGenerally, a value between \\(-1\\) and \\(+1\\) for skewness is “acceptable” and not considered overly skewed. Positive values indicate “right” skew and negative values indicate a “left” skew. If something is too skewed, it may violate assumptions of normality and thus need non-parametric tests rather than our standard parametric tests - something we will cover later!\nLet’s look at a skewed dataset. We are going to artificially create a skewed dataset from our x vector.\n\n# create more positive values\nx3 &lt;- c(x,\n        x[which(x &gt; 0)]*2,\n        x[which(x &gt; 0)]*4,\n        x[which(x &gt; 0)]*8)\n\nhist(x3)\n\n\n\n\n\n\n\n\n\nskewness(x3)\n\n[1] 2.125796\n\n\nAs we can see, the above is a heavily skewed dataset with a positive (“right”) skew.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#kurtosis",
    "href": "visual.html#kurtosis",
    "title": "3  Diagnosing data visually",
    "section": "3.6 Kurtosis",
    "text": "3.6 Kurtosis\nKurtosis refers to how sharp or shallow the peak of the distribution is (platykurtic vs. leptokurtic). Remember - platykyrtic are plateaukurtic, wide and broad like a plateau, and leptokurtic distributions are sharp. Intermediate distributions that are roughly normal are mesokurtic.\nMuch like skewness, kurtosis values of \\(&gt; 2\\) and \\(&lt; -2\\) are generally considered extreme, and thus not mesokurtic. This threshold can vary a bit based on source, but for this class, we will use a threshold of \\(\\pm 2\\) for both skewness and kurtosis.\nLet’s see the kurtosis of x. Note that when doing the equation, a normal distribution actually has a kurtosis of \\(3\\); thus, we are doing kurtosis \\(-3\\) to “zero” the distribution and make it comparable to skewness.\n\nhist(x)\n\n\n\n\n\n\n\n\n\n# non-zeroed\nkurtosis(x)\n\n[1] 2.898536\n\n\n\n# zeroed\nkurtosis(x)-3\n\n[1] -0.1014637\n\n\nAs expected, out values drawn from a normal distribution are not overly skewed. Let’s compare these to a more kurtic distribution:\n\nxk &lt;- x^3\n\nkurtosis(xk)-3\n\n[1] 35.66092\n\n\nWhat does this dataset look like?\n\nhist(xk,breaks = 100)\n\n\n\n\n\n\n\n\nAs we can see, this is a very leptokurtic distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#cumulative-frequency-plot",
    "href": "visual.html#cumulative-frequency-plot",
    "title": "3  Diagnosing data visually",
    "section": "3.7 Cumulative frequency plot",
    "text": "3.7 Cumulative frequency plot\nA cumulative frequency plot shows the overall spread of the data as a cumulative line over the entire dataset. This is another way to see the spread of the data and is often complementary to a histogram.\nThe following cumulative distribution plot is based on the method outlined in Geeks for Geeks.\n\n3.7.1 If data are not in histogram/frequency format\nYou will need to create a frequency table to make them be in histogram format.\n\n# declaring the break points\n# make break points based on data\n# always round UP with ceiling\nrange.x &lt;- ceiling(max(x)-min(x))\n\nrange.x\n\n[1] 7\n\nrange(x)\n\n[1] -3.313068  3.045090\n\n\nBased on this range, we need to create our bin sizes. We want our lowest bin to be below our lowest value, and our highest bin above our highest value. Here, I’m setting a step size of 1. You will have to examine your data and determine the best break size for your datasets.\n\n# make bins based on range\n# create sequential series\nbreak_points &lt;- seq(-3, # starting value, low\n                    4, # end value, high\n                    1) # increment size\n\n\n# transforming the data\ndata_transform = cut(x, # your data!\n                     break_points, # the breaks you defined\n                    right=FALSE) # closed on the left for intervals\n# creating the frequency table\nfreq_table = table(data_transform) # create a table\n\n# printing the frequency table\nprint(\"Frequency Table\")\n\n[1] \"Frequency Table\"\n\nprint(freq_table)\n\ndata_transform\n[-3,-2) [-2,-1)  [-1,0)   [0,1)   [1,2)   [2,3)   [3,4) \n     14     146     350     344     129      14       1 \n\n\n\n# calculating cumulative frequency\ncumulative_freq = c(0, # start at 0, no points\n                    cumsum(freq_table)) # get the cumulative frequency!\nprint(\"Cumulative Frequency\")\n\n[1] \"Cumulative Frequency\"\n\nprint(cumulative_freq)\n\n        [-3,-2) [-2,-1)  [-1,0)   [0,1)   [1,2)   [2,3)   [3,4) \n      0      14     160     510     854     983     997     998 \n\n\n\n# plotting the data\nplot(break_points, # x axis is break_points\n     cumulative_freq, # y axis is cumulative frequency\n    xlab=\"Data Distribution\", # x axis label\n    ylab=\"Cumulative Frequency\") # y axis label\n# creating line graph\nlines(break_points, # add lines to the graph, this is x\n      cumulative_freq) # this is y for lines\n\n\n\n\n\n\n\n\n\n\n3.7.2 If data are in histogram/frequency format\nIf you have a list of frequencies (say, for river discharge over several years), you only need to do the cumsum function. For example:\n\ny &lt;- c(1 ,2 ,4, 8, 16, 8, 4, 2, 1)\n\nsum_y &lt;- cumsum(y)\n\nprint(y)\n\n[1]  1  2  4  8 16  8  4  2  1\n\nprint(sum_y)\n\n[1]  1  3  7 15 31 39 43 45 46\n\n\nNow we can see we have out cumulative sums. Let’s plot these. NOTE that this method will not have the x variables match the dataset you started with, it will only plot the curve based on the number of values given.\n\nplot(x = 1:length(sum_y), # get length of sum_y, make x index\n     y = sum_y, # plot cumulative sums\n     type = \"l\") # make a line plot",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#homework-chapter-3",
    "href": "visual.html#homework-chapter-3",
    "title": "3  Diagnosing data visually",
    "section": "3.8 Homework: Chapter 3",
    "text": "3.8 Homework: Chapter 3\nFrom your book, complete problems 3.1, 3.4 & 3.5. Data for these problems are available on Canvas and in your book.\n\n3.8.1 Helpful hint\nHINT: For 3.5, consider just making a vector of the values of interest for a histogram.\nFor example, see the following. For reference:\n\nc means “concatenate”, or place things together in an object.\n\n\n# numeric vector data for counts\ny &lt;- c(17,24,16)\n\n# manually create a histogram using barplot\nbarplot(y, \n        # axis names must be true\n        axisnames = T, \n        # input names here\n        # each category as a separate quoted character string\n        names.arg = c(\"Cat 1\", \"Cat 2\", \"Cat 3\"))\n\n\n\n\n\n\n\n\n\n\n3.8.2 Directions\nPlease complete all computer portions in an rmarkdown document knitted as an html. Upload any “by hand” calculations as images in the HTML or separately on Canvas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#addendum",
    "href": "visual.html#addendum",
    "title": "3  Diagnosing data visually",
    "section": "3.9 Addendum",
    "text": "3.9 Addendum\nWith thanks to Hernan Vargas & Riley Grieser for help in formatting this page. Additional comments provided by BIOL 305 classes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "normality.html",
    "href": "normality.html",
    "title": "4  Normality & hypothesis testing",
    "section": "",
    "text": "4.1 Normal distributions\nA standard normal distribution is a mathematical model that describes a commonly observed phenomenon in nature. When measuring many different kinds of datasets, the data being measured often becomes something that resembles a standard normal distribution. This distribution is described by the following equation:\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^\\frac{(x-\\mu)^2}{2\\sigma^2}\\]\nThis equation is fairly well defined by the variance (\\(\\sigma^2\\)), the overall spread of the data, and by the standard deviation (\\(\\sigma\\)), which is defined by the square root of the variance.\nStandard normal distributions have a mean, median, and mode that are equal. The standard normal distribution is a density function, and we are interested in the “area under the curve” (AUC) to understand the relative probability of an event occurring. At the mean/median/mode, the probability on either side of the distribution is \\(50\\)%. When looking at a normal distribution distribution, it is impossible to say the probability of a specific event occurring, but it is possible to state the probability of an event as extreme or more extreme than the event observed occurring. This is known as the \\(p\\) value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#normal-distributions",
    "href": "normality.html#normal-distributions",
    "title": "4  Normality & hypothesis testing",
    "section": "",
    "text": "A standard normal distribution, illustrating the percentage of area found within each standard deviation away from the mean. By Ainali on Wikipedia; CC-BY-SA 3.0.\n\n\n\n\n4.1.1 Example in nature\nIn order to see an example of the normal distribution in nature, we are going to examine the BeeWalk survey database from the island of Great Britain (Comont 2020). We are not interested in the bee data at present, however, but in the climatic data from when the surveys were performed.\n\nbeewalk &lt;- curl(\"https://figshare.com/ndownloader/files/44726902\") %&gt;%\n  read_csv()\n\nRows: 306550 Columns: 49\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): Website.ID, Website.RecordKey, SiteName, Site.section, ViceCounty,...\ndbl (19): RecordKey, established, Precision, Transect.lat, Transect.long, tr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that this is another massive dataset - \\(306,550\\) rows of data!\nThe dataset has the following columns:\n\ncolnames(beewalk)\n\n [1] \"RecordKey\"            \"Website.ID\"           \"Website.RecordKey\"   \n [4] \"SiteName\"             \"Site.section\"         \"ViceCounty\"          \n [7] \"established\"          \"GridReference\"        \"Projection\"          \n[10] \"Precision\"            \"Transect.lat\"         \"Transect.long\"       \n[13] \"transect.OS1936.lat\"  \"Transect.OS1936.long\" \"transect_length\"     \n[16] \"section_length\"       \"section_grid_ref\"     \"H1\"                  \n[19] \"H2\"                   \"H3\"                   \"H4\"                  \n[22] \"habitat_description\"  \"L1\"                   \"L2\"                  \n[25] \"land_use_description\" \"start_time\"           \"end_time\"            \n[28] \"sunshine\"             \"wind_speed\"           \"temperature\"         \n[31] \"TaxonVersionKey\"      \"species\"              \"latin\"               \n[34] \"queens\"               \"workers\"              \"males\"               \n[37] \"unknown\"              \"Comment\"              \"transect_comment\"    \n[40] \"flower_visited\"       \"StartDate\"            \"EndDate\"             \n[43] \"DateType\"             \"Year\"                 \"Month\"               \n[46] \"Day\"                  \"Sensitive\"            \"Week\"                \n[49] \"TotalCount\"          \n\n\nWe are specifically interested in temperature to determine weather conditions. Let’s see what the mean of this variable is.\n\nmean(beewalk$temperature)\n\n[1] NA\n\n\nHmmm… we are getting an NA value, indicating that not every cell has data recorded. Let’s view summary.\n\nsummary(beewalk$temperature)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   16.00   19.00   18.65   21.00   35.00   16151 \n\n\nAs we can see, \\(16,151\\) rows do not have temperature recorded! We want to remove these NA rows, which we can do by using using na.omit.\n\nbeewalk$temperature %&gt;%\n  na.omit() %&gt;%\n  mean() %&gt;%\n  round(2) # don't forget to round!\n\n[1] 18.65\n\n\nNow we can record the mean.\nLet’s visualize these data using a histogram. Note I do not use na.omit as the hist function automatically performs this data-cleaning step!\n\nhist(beewalk$temperature,breaks = 5)\n\n\n\n\n\n\n\n\nEven with only five breaks, we can see an interesting, normal-esque distribution in the data. Let’s refine the bin number.\n\nhist(beewalk$temperature,breaks = 40)\n\n\n\n\n\n\n\n\nWith forty breaks, the pattern becomes even more clear. Let’s see what a standard normal distribution around these data would look like.\n\n# save temperature vector without NA values\ntemps &lt;- beewalk$temperature %&gt;% na.omit()\n\nmu &lt;- mean(temps)\nt.sd &lt;- sd(temps)\n\n# sample random values\nnormal.temps &lt;- rnorm(length(temps), # sample same size vector\n                      mean = mu,\n                      sd = t.sd)\n\nhist(normal.temps, breaks = 40)\n\n\n\n\n\n\n\n\nAs we can see, our normal approximation of temperatures is not too dissimilar from the distribution of temperatures we actually see!\nLet’s see what kind of data we have for temperatures:\n\n# load moments library\nlibrary(moments)\n\nskewness(temps)\n\n[1] 0.02393257\n\n\nData do not have any significant skew.\n\nkurtosis(temps)-3\n\n[1] 0.3179243\n\n\nData do not show any significant kurtosis.\n\n\n4.1.2 Effect of sampling\nOftentimes, we will see things approach the normal distribution as we collect more samples. We can model this by subsampling our temperature vector.\n\n# make reproducible\nset.seed(1839)\n\nsub.temps &lt;- sample(temps,\n                    size = 10,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"10 samples\")\n\n\n\n\n\n\n\n\nWith only ten values sampled, we do not have much of a normal distribution. Let’s up this to \\(100\\) samples.\n\nsub.temps &lt;- sample(temps,\n                    size = 100,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"100 samples\",breaks = 10)\n\n\n\n\n\n\n\n\nNow we are starting to see more of a normal distribution! Let’s increase this to \\(1000\\) temperatures.\n\nsub.temps &lt;- sample(temps,\n                    size = 1000,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"1000 samples\", breaks = 40)\n\n\n\n\n\n\n\n\nNow the normal distribution is even more clear. As we can also see, the more we sample, the more we approach the true means and distribution of the actual dataset. Because of this, we can perform experiments and observations of small groups and subsamples and make inferences about the whole, given that most systems naturally approach statistical distributions like the normal!\n\n\n4.1.3 Testing if data are normal\nThere are two major methods we can use to see if data are normally distributed.\n\n4.1.3.1 QQ Plots\nAnother way to see if data are normal is to use a QQ plot. These plots data quantiles to theoretical quantiles to see how well they align, with a perfectly normal distribution having a completely linear QQ plot. Let’s look at these with our beewalk data.\n\nqqnorm(temps)\n\n\n\n\n\n\n\n\nAs we can see above, the data are roughly linear, which means are data are very normal. The “stairsteps” are from the accuracy in measuring temperature, which was likely rounded and thus created a distribution that is not completely continuous.\n\n\n4.1.3.2 Shapiro-Wilk test\nAnother way to test for normality is to use a Shapiro-Wilk test of normality. We will not get into the specific of this distribution, but this tests the null hypothesis that data originated in a normal distribution, with the alternative hypothesis that the data originated in a non-normal distribution. You will read next about the specifics of hypothesis testing, but this test uses an \\(\\alpha = 0.05\\), and we reject the null hypothesis if our \\(p &lt; \\alpha\\), with \\(p\\) representing the probability of observing something as extreme or more extreme than the result we observe. Our temps dataset is too large, as shapiro.test requires vectors of \\(&lt; 5000\\). Let’s take a random sample and try again.\n\nsample(temps, 200) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98922, p-value = 0.1371\n\n\nFor our subsets of temperature, we find that the temperature is normally distributed. Note that having all \\(5000\\) temperatures included makes this test find a “non-normal” result, likely from the same stairstepping phenomenon from rounding that we discussed before, which may result in model overfitting to the rounded data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#hypothesis-testing",
    "href": "normality.html#hypothesis-testing",
    "title": "4  Normality & hypothesis testing",
    "section": "4.2 Hypothesis testing",
    "text": "4.2 Hypothesis testing\nSince we can define specific areas under the curve within these distributions, we can look at the percentage of area within a certain bound to determine how likely a specific outcome would be. Thus, we can begin to test what the probability of observing an event is within a theoretical, probabilistic space. A couple of important conceptual ideas:\n\nWe may not be able to know the probability of a specific event, but we can figure out the probability of events more extreme or less extreme as that event.\nIf the most likely result is the mean, then the further we move away from the mean, the less likely an event becomes.\nIf we look away from the mean at a certain point, then the area represents the chances of getting a result as extreme or more extreme than what we observe. This probability is known as the \\(p\\) value.\n\nOnce we have a \\(p\\) value, we can make statements about the event that we’ve seen relative to the overall nature of the dataset, but we do not have sufficient information to declare if this result is statistically significant.\n\n4.2.1 Critical Values - \\(\\alpha\\)\nIn order to determine if something is significant, we compare things to a critical value, known as \\(\\alpha\\). This value is traditionally defined as \\(0.05\\), essentially stating that we deem an event as significant if \\(5\\)% or fewer of observed or predicted events are as extreme or more extreme than what we observe.\nYour value should always set your \\(\\alpha\\) critical value before you do your experiments and analyses.\nOur critical value of \\(\\alpha\\) represents our criterion for rejecting the null hypothesis. We set our \\(\\alpha\\) to try to minimize the chances of error.\nType I Error is also known as a false-positive, and is when we reject the null hypothesis when the null is true.\nType II Error is also known as a false-negative, and is when we support the null hypothesis when the null is false.\nBy setting an \\(\\alpha\\), we are creating a threshold of probability at which point we can say, with confidence, that results are different.\n\n\n4.2.2 Introduction to \\(p\\) values\nLet’s say that we are looking at a dataset defined by a standard normal distribution with \\(\\mu=0\\) and \\(\\sigma=1\\). We draw a random value, \\(x\\), with \\(x=1.6\\). What is the probability of drawing a number this extreme or more extreme from the dataset?\nFirst, let’s visualize this distribution:\n\n###THIS WILL TAKE A WHILE TO RUN###\n\n# create gigantic normal distribution dataset\n# will be essentially normal for plotting\n# rnorm gets random values\nx &lt;- rnorm(100000000)\n\n# convert to data frame\nx &lt;- as.data.frame(x)\n# rename column\ncolnames(x) &lt;- c(\"values\")\n\n# thank you stack overflow for the following\n# Creating density plot\np = ggplot(x, \n           aes(x = values)\n          ) + \n  # generic density plot, no fill\n  geom_density(fill=\"lightblue\")\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # rename axes\n  geom_vline(xintercept = 1.6) +\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nAbove, the solid black line represents \\(x\\), with the illustrated standard normal distribution being filled in blue.\nLet’s see how much of the area represents values as extreme or more extreme as our value \\(x\\).\n\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  &lt;-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x &lt; 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nNow we can see that it is only a portion of the distribution as extreme or more extreme than the value we placed on the graph. The area of this region is our \\(p\\) value. This represents the probability of an event as extreme or more extreme occurring given the random variation observed in the dataset or in the distribution approximating the dataset. This is the value we compare to \\(\\alpha\\) - our threshold for rejecting the null hypothesis - to determine whether or not we are going to reject the null hypothesis.\nLet’s look at the above graph again, but let’s visualize a two-tailed \\(\\alpha\\) around the mean with a \\(95\\)% confidence interval. First, we need to get the \\(Z\\) scores for our \\(\\alpha=0.05\\), which we calculate by taking \\(\\frac{\\alpha}{2}\\) to account for the two tails. (Two tails essentially meaning we reject the null mean if we see things greater than or less than our expected value to a significant extent). We can calculate \\(Z\\) scores using qnorm.\n\nlow_alpha &lt;- qnorm(0.025) # looks left\nhi_alpha &lt;- qnorm(0.975) # looks left\n\nprint(paste0(\"Our Z scores are: \", \n             round(low_alpha,2), \n             \" & \", \n             round(hi_alpha,2)))\n\n[1] \"Our Z scores are: -1.96 & 1.96\"\n\n\nThe above values make sense, given the distribution is symmetrical. Our above dashed line is as \\(Z = 1.6\\), which means we should have a \\(p = 0.05\\), so the dashed line should be closer to the mean than our cutoffs.\n\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  &lt;-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x &lt; 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  geom_vline(xintercept = low_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  geom_vline(xintercept = hi_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nExactly as we calculated, we see that our \\(p &lt; \\alpha\\) and thus we do not see an area (in blue) less than the area that would be further than the mean as defined by the red lines.\n\n\n4.2.3 Calculating a \\(Z\\) score\nWhen we are trying to compare our data to a normal distribution, we need to calculate a \\(Z\\) score for us to perform the comparison. A \\(Z\\) score is essentially a measurement of the number of standard deviations we are away from the mean on a standard normal distribution. The equation for a \\(Z\\) score is:\n\\[\nZ = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nWhere \\(\\bar{x}\\) is either a sample mean or a sample value, \\(\\mu\\) is the population mean, \\(\\sigma\\) is the population standard deviation and \\(n\\) is the number of individuals in your sample (\\(1\\) if comparing to a single value).\nWe can calculate this in R using the following function:\n\nzscore &lt;- function(xbar, mu, sd.x, n = 1){\n  z &lt;- (xbar - mu)/(sd.x/sqrt(n))\n  return(z)\n}\n\nNOTE that is the above isn’t working for you, you have a mistake somewhere in your code. Try comparing - character by character - what is listed above to what you have.\nLet’s work through an example, where we have a sample mean of \\(62\\) with \\(5\\) samples compared to a sample mean of \\(65\\) with a standard deviation of \\(3.5\\).\n\nZ &lt;- zscore(xbar = 62, # sample mean\n            mu = 65, # population mean\n            sd.x = 3.5, # population standard deviation\n            n = 5) # number in sample\n\nprint(Z)\n\n[1] -1.91663\n\n\nNow, we can calculate the \\(p\\) value for this \\(Z\\) score.\n\npnorm(Z)\n\n[1] 0.0276425\n\n\nAfter rounding, we get \\(p = 0.03\\), a \\(p\\) that is significant if for a one-tailed \\(\\alpha = 0.05\\) but insignificant for a two-tailed \\(\\alpha = 0.05\\).\n\n\n4.2.4 Calculated the \\(p\\) value\nWe have two different methods for calculating a \\(p\\) value:\n\n4.2.4.1 Comparing to a \\(z\\) table\nWe can compare the \\(z\\) value we calculate to a \\(z\\) table, such as the one at ztable.net. On this webpage, you can scroll and find tables for positive and negative \\(z\\) scores. Note that normal distributions are symmetrical, so you can also transform from negative to positive to get an idea of the area as well. Given that a normal distribution is centered at \\(0\\), a \\(z\\) score of \\(0\\) will have a \\(p\\) value of \\(0.50\\).\nOn the \\(z\\) tables, you will find the tenths place for your decimal in the rows, and then go across to the columns for the hundredths place. For example, go to the website and find the \\(p\\) value for a \\(z\\) score of \\(-1.33\\). You should find the cell marked \\(0.09176\\). Note the website uses naked decimals, which we do not use in this class.\nFor values that aren’t on the \\(z\\) table, we can approximate its position between different points on the \\(z\\) table or, if it is extremely unlikely, denote that \\(p &lt; 0.0001\\).\n\n\n4.2.4.2 Using R\nIn R, we can calculate a \\(p\\) value using the function pnorm. This function uses the arguments of p for our \\(p\\) value, mean for the mean of our distribution, sd for the standard deviation of our distribution, and also information on whether we want to log-transform \\(p\\) or if we are testing a specific hypothesis (lower tail, upper tail, or two-tailed). The function pnorm defaults to a standard normal distribution, which would be a \\(z\\) score, but it can also perform the \\(z\\) transformations for us if we define the mean and standard deviation.\nFor example, if we have a \\(z\\) score of \\(-1.33\\):\n\npnorm(-1.33)\n\n[1] 0.09175914\n\n\nAs we can see, we get the same result as our \\(z\\) table, just with more precision!\nThere are other functions in this family as well in R, including dnorm for quantiles, qnorm for determining the \\(z\\) score for a specific \\(p\\) value, and rnorm for getting random values from a normal distribution with specific dimensions. For now, we will focus on pnorm.\n\n\n\n4.2.5 Workthrough Example\nWhen this class was being designed, Hurricane Milton was about to make contact with Florida. Hurricane Milton is considered one of the strongest hurricanes of all time, so we can look at historical hurricane data to determine just how powerful this storm really was. We can get information on maximum wind speeds of all recorded Atlantic hurricanes as of 2024 from Wikipedia.\n\nhurricanes &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/assignments/hurricane_speeds.csv\")\n\nRows: 889 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): Hurricane_Windspeed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAbove, we have loaded a .csv of one column that has all the hurricane speeds up to 2024. Hurricane Milton is the last row - the most recent hurricane. Let’s separate this one out. We will use [ , ], which defines [rows,columns] to subset data.\n\nmilton &lt;- hurricanes$Hurricane_Windspeed[nrow(hurricanes)]\n\nother_hurricanes &lt;- hurricanes[-nrow(hurricanes),]\n\nWe want to compare the windspeed of Milton (180 mph) to the overall distribution of hurricane speeds. We can visualize this at first.\n\n# all hurricanes\nhist(hurricanes$Hurricane_Windspeed)\n\n\n\n\n\n\n\n\nWindspeeds are more towards the lower end of the distribution, with strong storms being rarer.\nFor the sake of this class, we will assume we can use a normal distribution for these data, but if we were doing an official study we would likely need to use a non-parametric test (we will cover these later, but they cover non-normal data).\n\nmu &lt;- mean(other_hurricanes$Hurricane_Windspeed)\n\nmu\n\n[1] NA\n\n\nHmmm… we need to use na.omit to be sure we do this properly.\n\nother_hurricanes_windspeed &lt;- na.omit(other_hurricanes$Hurricane_Windspeed)\n\nmu &lt;- mean(other_hurricanes_windspeed)\nmean(other_hurricanes_windspeed)\n\n[1] 102.5254\n\n\nNext, we need the standard deviation.\n\nsd.hurricane &lt;- sd(other_hurricanes_windspeed)\n\nsd.hurricane\n\n[1] 23.08814\n\n\nNow, we can calculate our \\(Z\\) value.\n\nZ &lt;- (milton - mu)/sd.hurricane\n\nZ\n\n[1] 3.355603\n\n\nHow significant is this?\n\npnorm(Z)\n\n[1] 0.999604\n\n\nThis is greater than \\(0.5\\), so we need to do \\(1-p\\) to figure things out.\n\n1 - pnorm(Z)\n\n[1] 0.000395961\n\n\nThis rounds to \\(0.0004\\), which means that this is an extremely strong hurricane.\n\n4.2.5.1 Non-normality, for those curious\nWe can do a Shapiro-Wilk test of normality to see if this dataset is normal.\n\nshapiro.test(other_hurricanes_windspeed)\n\n\n    Shapiro-Wilk normality test\n\ndata:  other_hurricanes_windspeed\nW = 0.88947, p-value &lt; 2.2e-16\n\n\nA \\(p &lt; 0.05\\) indicates that these data are non-normal.\nWe can do a Wilcoxon-Test since these data are extremely non-normal.\n\nwilcox.test(other_hurricanes_windspeed,\n            milton)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  other_hurricanes_windspeed and milton\nW = 6.5, p-value = 0.08678\nalternative hypothesis: true location shift is not equal to 0\n\n\nUsing non-normal corrections, we find that this is not an extremely strong hurricane, but it is near the upper end of what we would consider “normal” under historical conditions. Still an extremely bad hurricane!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#confidence-intervals",
    "href": "normality.html#confidence-intervals",
    "title": "4  Normality & hypothesis testing",
    "section": "4.3 Confidence Intervals",
    "text": "4.3 Confidence Intervals\nBecause we can figure out the probability of an event occurring, we can also calculate confidence intervals. A confidence interval provides a range of numbers around a value of interest that indicates where we believe the mean of a population lies and our confidence that it lies within that range. Note that nothing is ever 100% certain, but this helps us determine where a mean is and demonstrates our confidence in our results.\nSpecifically, if the tails of the distribution, our \\(\\alpha\\), are \\(0.05\\), then we have an area of \\(0.95\\) around the mean where we do not reject results. Another perspective on this area is that we can say with \\(95\\)% certainty that a mean is within a certain area, and that if values fall within that confidence area then we do not reject the null hypothesis that the means are equal.\nWe will cover several different ways to calculate confidence intervals, but for normal distributions, we use the following equation, with the \\(0.95\\) confidence interval shown as an example:\n\\[\nCI=\\bar{x} \\pm Z_{1-\\frac{\\alpha}{2}}\\cdot \\frac{\\sigma}{\\sqrt{n}}\n\\]\nThis interval gives us an idea of where the mean should lie. For example, if we are looking at the aforemention beewalk temperature data, we can calulate a \\(0.95\\) confidence interval around the mean.\n\ntemps &lt;- na.omit(beewalk$temperature)\n\nxbar &lt;- mean(temps)\nn &lt;- length(temps)\nsdtemp &lt;- sd(temps)\n# Z for 0.95 as P, so for 0.975, 0.025\n# get value from P\nZ &lt;- qnorm(0.975)\n\nCI &lt;- Z*(sdtemp/sqrt(n))\n\nCI\n\n[1] 0.01458932\n\n\nWe have a very narrow confidence zone, because we have so many measurements. Let’s round everything and present it in a good way.\nIf I want numbers to show up in text in RMarkdown, I can add code to a line of plain text using the following syntax:\n\n# DO NOT RUN\n# Format in plaintext\n`r xbar`\n\n\n\n\nThis is the “coded” version of the text below. Compare the above window to the text below this image.\n\n\nTyping that into the plaintext should render as the following: 18.645963. Then I can also type my answer as follows:\nThe \\(95\\)% Confidence Interval for the mean for this temperature dataset is 18.65 \\(\\pm\\) 0.01.\nNote that the above is rounded to two decimal places to illustrative purposes ONLY, and should be rounded to one decimal place if it was a homework assignment because the original data has only one decimal place.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#homework-chapter-8",
    "href": "normality.html#homework-chapter-8",
    "title": "4  Normality & hypothesis testing",
    "section": "4.4 Homework: Chapter 8",
    "text": "4.4 Homework: Chapter 8\nPlease complete problems 8.1, 8.2, 8.3 & 8.6. Follow the directions as written in the book. Submit one html file, as derived from RStudio. For maximum clarity, create headings to separate your problems. (Remember, a header can be invoked by placing ‘#’ in front of a line of text. For example: the header here is written as # Homework: Chapter 8).\n\n\n\n\nComont, R. (2020). BeeWalk dataset 2008-23. https://doi.org/10.6084/m9.figshare.12280547.v4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "exam2.html",
    "href": "exam2.html",
    "title": "5  Exam 2 practice",
    "section": "",
    "text": "5.1 Exam 2 Practice\nThe following is practice for the exam. Please work through these problems and be ready to discuss them in class.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exam 2 practice</span>"
    ]
  },
  {
    "objectID": "exam2.html#question-1-cyclones",
    "href": "exam2.html#question-1-cyclones",
    "title": "5  Exam 2 practice",
    "section": "5.2 Question 1: Cyclones",
    "text": "5.2 Question 1: Cyclones\nConsider this short data set:\n\n\n\nLatitude band\nSeason\nNumber of cyclones\n\n\n\n\n40 - 49° S\nFall\n370\n\n\n40 - 49° S\nWinter\n452\n\n\n40 - 49° S\nSpring\n273\n\n\n40 - 49° S\nSummer\n422\n\n\n50 - 59° S\nFall\n526\n\n\n50 - 59° S\nWinter\n624\n\n\n50 - 59° S\nSpring\n513\n\n\n50 - 59° S\nSummer\n1,059\n\n\n60 - 69° S\nFall\n980\n\n\n60 - 69° S\nWinter\n1,200\n\n\n60 - 69° S\nSpring\n995\n\n\n60 - 69° S\nSummer\n1,751\n\n\n\nAll of the data was collected in the same year to determine whether the occurrences of cyclones differed by latitude and season around Antarctica. Use this data to answer the following questions.\n\nClassify each of the three variables above as either an explanatory or a response variable. Justify your answer.\nClassify each of the three variables above as either a nominal, ordinal, interval, or ratio variable. Justify your answer.\nState the null and alternative hypotheses for this scenario. Note: there are two sets of null/alternative hypotheses.\nCalculate the mean, median, and mode of your response variable. Based on your results, would you expect your data to be normally distributed or not? Justify your answer.\nLet’s say now that you’re testing the null hypothesis that the mean number of cyclones in this one year is similar to the average year. The mean number of cyclones per year across all years of data collected is 650 with a standard deviation of 425. Calculate the z-score and make a decision regarding the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exam 2 practice</span>"
    ]
  },
  {
    "objectID": "exam2.html#question-2-minnows",
    "href": "exam2.html#question-2-minnows",
    "title": "5  Exam 2 practice",
    "section": "5.3 Question 2: Minnows",
    "text": "5.3 Question 2: Minnows\nConsider this scenario: You have discovered a never-before-documented population of minnow in the Kearney Canal near campus. During your first sampling trip, you notice that the total length (i.e., measured from the tip of the snout to the very tip of the tail) of the fish you measure appear to be smaller than the average total length of the species as recorded among all known individuals across their range. The mean total length noted in one publication is 85 mm with a standard deviation of 4.50. Below is your data the data from 20 minnows that you captured during your first sampling trip to the Kearney Canal:\n\n\n\nFish ID\nLength (mm)\n\n\n\n\n1\n89\n\n\n2\n75\n\n\n3\n86\n\n\n4\n74\n\n\n5\n69\n\n\n6\n100\n\n\n7\n73\n\n\n8\n69\n\n\n9\n96\n\n\n10\n79\n\n\n11\n61\n\n\n12\n62\n\n\n13\n95\n\n\n14\n98\n\n\n15\n100\n\n\n16\n57\n\n\n17\n70\n\n\n18\n78\n\n\n19\n65\n\n\n20\n65\n\n\n\nHINT: Try combining numbers using the c command. See the Glossary for more information!\n\nState the null and alternative hypothesis for your study.\nCalculate the median, first and third quartiles, and interquartile range of your response variable. Create a boxplot. Based on your results, are there any outliers in your data? Explain.\nCalculate the \\(z\\)-score for this scenario. Please show all of your work.\nWhat is the probability that, by random chance alone, you would find your observed mean or something more extreme?\nAssume that you set your \\(\\alpha\\) for your study prior to your data collection to \\(0.05\\). Based on this information and the \\(p\\)-value you obtained for part \\(c\\) above, is your null hypothesis supported or rejected?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exam 2 practice</span>"
    ]
  },
  {
    "objectID": "probdist.html",
    "href": "probdist.html",
    "title": "6  Probability distributions",
    "section": "",
    "text": "6.1 Probability distributions\nWe rely on multiple different probability distributions to help us understand what probable outcomes are for a specific scenario. All of the tests that we are performing are comparing our results to what we would expect under perfectly random scenarios. For example, if we are flipping a coin, we are interested in whether the the observation we have of the flips on our coin matches our expectation given the probability of getting heads or tails on a perfectly fair coin. While it is possible to get all heads or all tails on a coin flip, it is highly unlikely and may lead us to believe we have an unfair coin. The more trails we perform, the more confident we can be that out coin is atypical.\nWe perform similar comparisons for other distributions. If we are comparing sets of events, we can look at the probability of those events occurring if events are occurring randomly. If we are comparing counts, we can compare our counts to our expectation of counts if events or subjects are distributed randomly throughout the matrix or whether two sets of counts are likely under the same sets of assumptions.\nRemember, for our specific tests, we are setting an \\(\\alpha\\) value in advance (traditionally \\(0.05\\), or \\(5\\)%) against which we compare our \\(p\\) value, with \\(p\\) representing the probability of observing an event as extreme or more extreme than the event we observe given a specific probability distribution.\nPreviously, we talked about the normal distribution, which is used to approximate a lot of datasets in nature. However, several other probability distributions are also useful for biological systems, which are outlined here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#binomial-distribution",
    "href": "probdist.html#binomial-distribution",
    "title": "6  Probability distributions",
    "section": "6.2 Binomial distribution",
    "text": "6.2 Binomial distribution\nA binomial distribution is one in which only two outcomes are possible - often coded as \\(0\\) and \\(1\\) and usually representing failure and success, respectively. The binomial is described by the following function:\n\\[\np(x)=\\binom{n}{x}p^{x}(1-p)^{n-x}\n\\]\nwhere \\(n =\\) number of trials, \\(x =\\) the number of successes, and \\(p =\\) the probability of a success under random conditions.\nIn R, the binomial distribution is represented by the following functions:\n\ndbinom: the density of a binomial distribution\npbinom: the distribution function, or the probability of a specific observation\nqbinom: the value at which a specific probability is found (the quantile function)\nrbinom: generates random values according to a binomial.\n\n\n6.2.1 Binomial examples\nLet’s see what this looks like. Let’s consider a scenario where we flip a coin 10 times and get 9 heads. How likely is this outcome?\n\nx &lt;- pbinom(q = 9, # number successes, 9 heads\n            size = 10, # number of trials, 10 flips\n            prob = 0.5) # probability with a fair coin\n\nround(x,4)\n\n[1] 0.999\n\n\nNOTE that the trailing \\(0\\) is dropped, such that the real answer is \\(0.9990\\). However, we mentioned before that the \\(p\\) value should be the probability of a result as extreme or more extreme, meaning that it should always be less than \\(0.5\\). If we are reporting a value of greater than \\(0.5\\), then we are comparing to the upper tail of the distribution. For a one-tailed \\(\\alpha\\) of \\(0.05\\), this would mean that we are looking for a value greater than \\(0.95\\) (\\(1-\\alpha\\)).\nSo, our real \\(p\\) is:\n\n1 - round(x,4)\n\n[1] 0.001\n\n\nAgain, the trailing zero is missing. Given that \\(p &lt; \\alpha\\), we reject the null hypothesis that this is a fair coin.\nHow does this distribution look?\n\n# number of successes\n# start at 0 for no heads\nx &lt;- 0:10\n# cumulative probability to left of outcome\ny &lt;- pbinom(x,\n            size = 10, \n            prob = 0.5, \n            lower.tail = T)\n\n# cumulative probability of results to the left\nplot(x,\n     y,\n     type=\"l\") # line plot\n\n\n\n\n\n\n\n\nWhat about if we always have \\(p\\) less than \\(0.5\\) to reflect two tails?\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,\n     y,\n     type=\"l\")\n\n\n\n\n\n\n\n\nWhat if we do this with a bigger dataset, like for \\(50\\) flips?\n\n# number of successes\n# start at 0 for no heads\nx &lt;- 0:50\n# cumulative probability to left of outcome\ny &lt;- pbinom(x,\n            size = length(x), \n            prob = 0.5, \n            lower.tail = T)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,\n     y,\n     type=\"l\")\n\n\n\n\n\n\n\n\nAs we increase the number of flips, we can see that the probability of success forms a normal distribution centered on the outcome given the default probability. Thus, as we deviate from our expected outcome (initial probability multiple by the number of trials), then our results become less likely.\n\n\n6.2.2 Binomial exact tests\nWe can perform exact binomial tests by using the R function binom.test. This is a built in function within R. This test requires the following arguments:\n\nx: number of successes (success = outcome of interest)\nn: number of trials (number of events)\np: probability of success in a typical situation (i.e., for a fair coin, this is \\(50\\)%)\nalternative: the hypothesis to be tested, whether two.sided, greater, or less.\nconf.level is the confidence level to be returned; default is \\(95\\)%.\n\nLet’s say you flip a coin ten times, randomly assigning one side as a “success” and one side as a “failure”. We do ten flips, and get 3 “successes”. How likely is this outcome?\n\nbinom.test(x = 3,   # three successes\n           n = 10,  # ten flips\n           p = 0.5) # 50% chance on fair coin\n\n\n    Exact binomial test\n\ndata:  3 and 10\nnumber of successes = 3, number of trials = 10, p-value = 0.3438\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.06673951 0.65245285\nsample estimates:\nprobability of success \n                   0.3 \n\n\nNow let’s say we do 1000 flips, and we get 300 successes.\n\nbinom.test(x = 300,\n           n = 1000,\n           p = 0.5)\n\n\n    Exact binomial test\n\ndata:  300 and 1000\nnumber of successes = 300, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2717211 0.3294617\nsample estimates:\nprobability of success \n                   0.3 \n\n\nAs we can see, both of these return a confidence interval among other things. If we save the object, we can access these “slots” of data using the $ character.\n\nbinom_result &lt;- binom.test(x = 3,\n                           n = 10,\n                           p = 0.5)\n\nbinom_result$p.value %&gt;% round(2)\n\n[1] 0.34\n\n\n\nbinom_result$conf.int\n\n[1] 0.06673951 0.65245285\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThis test is easily implemented, but always double check and make sure you are setting it up correctly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#poisson-distribution",
    "href": "probdist.html#poisson-distribution",
    "title": "6  Probability distributions",
    "section": "6.3 Poisson distribution",
    "text": "6.3 Poisson distribution\nThe Poisson distribution is used to reflect random count data. Specifically, the Poisson is used to determine if success events are overdispersed (i.e., regularly spaced), random, or underdispersed (i.e., clustered). The Poisson introduces the variable lambda (\\(\\lambda\\)) which represents the mean (\\(\\mu\\)) and the variance (\\(\\sigma^2\\)), which are equal in a Poisson distribution. A Poisson distribution is described by the following function:\n\\[\np(x)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\]\n\n6.3.1 Poisson example\nThe Poisson is represented by the following functions in R which closely resemble the functions for the normal and binomial distributions:\n\ndpois: the log density function\nppois: log distribution (probability) function\nqpois: quantile function\nrpois: random values from a Poisson.\n\nLet’s look at the probability of \\(0\\) to \\(10\\) successes when we have our \\(\\lambda=1\\).\n\nx &lt;- 0:10\ny &lt;- ppois(x,lambda = 1)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")\n\n\n\n\n\n\n\n\nAs we can see, the probability of rare events is high, whereas the probability quickly decreases as the number of successes increases.\n\n\n6.3.2 Poisson test\nMuch like the Binomial Distribution and its binom.test, we can use poisson.test to analyze data via a Poisson Distribution. This command uses the arguments:\n\nx: number of events of interest\nT: time base (if for an event count)\nr: hypothesized rate or ratio\nalternative and conf.level are the same as for binom.test\n\nWe will not often use the poisson.test in this class, but it is good to be aware of.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#cumulative-probabilities",
    "href": "probdist.html#cumulative-probabilities",
    "title": "6  Probability distributions",
    "section": "6.4 Cumulative Probabilities",
    "text": "6.4 Cumulative Probabilities\nWith both the Binomial and the Poisson, we can calculate cumulative probabilities. Both of these require the density (d) versions of the arguments.\n\n6.4.1 Binomial cumulative\nLet’s say we have ten trials with three successes and a base probability of \\(0.5\\). We can calculate the probability to the left by using the following:\n\npbinom(q = 3,\n       size = 10,\n       prob = 0.5)\n\n[1] 0.171875\n\n\nAs we can see, this is ~17.19%. Now let’s try using dbinom. This command gives us the value at an individual bin, given that it is a more discrete distribution for these smaller sample sizes.\n\ndbinom(x = 0:3, \n       size = 10, \n       prob = 0.5)\n\n[1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000\n\n\nAbove, we can see the probability of each number of successes three and fewer, for 0, 1, 2, and 3. Let’s sum these probabilities.\n\ndbinom(x = 0:3, \n       size = 10, \n       prob = 0.5) %&gt;%\n  sum()\n\n[1] 0.171875\n\n\nAs we can see, we get the same value as for pbinom! We can use this method for finding very specific answers, like what is the probability of getting between 3 and 6 successes in ten trials?\n\ndbinom(x = 3:6,\n       size = 10,\n       prob = 0.5) %&gt;% \n  sum() %&gt;% \n  round(4)\n\n[1] 0.7734\n\n\nThe probability of getting one of these outcomes is 77.34%.\n\n\n6.4.2 Poisson cumulative probability\nLikewise, we can use ppois to get the \\(p\\) value and dpois to get the distribution function of specific outcomes. So, let’s say we have a scenario with a \\(\\lambda = 0.5\\) and we are looking at the probability of 2 successes or greater. In this case, we have an infinite series, which we can’t calculate. However, we can calculate the probability of what it isn’t and then subtract from 1. In this case, we are looking for the probability of not having 0 or 1 successes.\n\ndpois(x = 0:1,\n      lambda = 0.5)\n\n[1] 0.6065307 0.3032653\n\n\nNow, let’s sum this and subtract it from 1.\n\n1 - dpois(x = 0:1, lambda = 0.5) %&gt;% \n  sum()\n\n[1] 0.09020401\n\n\nThe probability is only about 9%.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#homework",
    "href": "probdist.html#homework",
    "title": "6  Probability distributions",
    "section": "6.5 Homework",
    "text": "6.5 Homework\n\n6.5.1 Chapter 5\nComplete problems 5.1, 5.2, 5.3, 5.5, 5.6, 5.11, 5.12, 5.13, 5.14, and 5.20 as written in your textbook.\nBe sure to show your work, and submit your assignment as a knitted html document.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "chi.html",
    "href": "chi.html",
    "title": "7  χ2 (Chi-squared) tests",
    "section": "",
    "text": "7.1 \\(\\chi^2\\)-squared distribution\n\\(\\chi^2\\)-squared (pronounced “kai”, and spelled “chi”) is a distribution used to understand if count data between different categories matches our expectation. For example, if we are looking at students in the class and comparing major vs. number of books read, we would expect no association, however we may find an association for a major such as English which required reading more literature. The \\(\\chi^2\\) introduces a new term degrees of freedom (\\(df\\)) which reflects the number of individuals in the study. For many tests, \\(df\\) are needed to reflect how a distribution changes with respect the number of individuals (and amount of variation possible) within a dataset. The equation for the \\(\\chi^2\\) is as follows, with the \\(\\chi^2\\) being a special case of the gamma (\\(\\gamma\\) or \\(\\Gamma\\)) distribution that is affected by the \\(df\\), which is defined as the number of rows minus one multiplied by the number of columns minus one \\(df = (rows-1)(cols-1)\\):\n\\[\nf_n(x)=\\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}x^\\frac{n}{2-1}e^\\frac{-x}{2}\n\\]\nThe \\(\\chi^2\\)-squared distribution is also represented by the following functions, which perform the same things as the previous outlined equivalents for Poisson and binomial:\nWe can view these probabilities as well:\nx &lt;- 0:10\n\ny &lt;- pchisq(x, df = 9)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#chi2-squared-distribution",
    "href": "chi.html#chi2-squared-distribution",
    "title": "7  χ2 (Chi-squared) tests",
    "section": "",
    "text": "dchisq\npchisq\nqchisq\nrchisq\n\n\n\n\n7.1.1 Calculating the test statistic\nWe evaluate \\(\\chi^2\\) tests by calculating a \\(\\chi^2\\) value based on our data and comparing it to an expected \\(\\chi^2\\) distribution. This test statistic can be evaluated by looking at a \\(\\chi^2\\) table or by using R. Note that you need to know the degrees of freedom in order to properly evaluate a \\(\\chi^2\\) test. We calculate our test statistic as follows:\n\\[\n\\chi^2=\\Sigma\\frac{(o-e)^2}{e}\n\\]\nwhere \\(e =\\) the number of expected individuals and \\(o =\\) the number of observed individuals in each category. Since we are squaring these values, we will only have positive values, and thus this will always be a one-tailed test.\nThere are multiple types of \\(\\chi^2\\) test, including the following we will cover here:\n\n\\(\\chi^2\\) Goodness-of-fit test\n\\(\\chi^2\\) test of independence\n\n\n\n7.1.2 \\(\\chi^2\\) goodness-of-fit test\nA \\(\\chi^2\\) goodness-of-fit test looks at a vector of data, or counts in different categories, and asks if the observed frequencies vary from the expected frequencies.\n\n7.1.2.1 \\(\\chi^2\\) estimate by hand\nLet’s say, for example, we have the following dataset:\n\n\n\nHour\nNo. Drinks Sold\n\n\n\n\n6-7\n3\n\n\n7-8\n8\n\n\n8-9\n15\n\n\n9-10\n7\n\n\n10-12\n5\n\n\n12-13\n20\n\n\n13-14\n18\n\n\n14-15\n8\n\n\n15-16\n10\n\n\n16-17\n12\n\n\n\nNow, we can ask if the probability of selling drinks is the same across all time periods.\n\ndrinks &lt;- c(3, 8, 15, 7, 5, 20, 18, 8, 10, 12)\n\nWe can get the expected counts by assuming an equal probability for each time period; thus, \\(Exp(x)=\\frac{N}{categories}\\).\n\n# sum all values for total number\nN &lt;- sum(drinks)\n\n# get length of vector for categories\ncats &lt;- length(drinks)\n\n# repeat calculation same number of times as length\nexp_drinks &lt;- rep(N/cats, cats)\n\nexp_drinks\n\n [1] 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6\n\n\nNow, we can do out \\(\\chi^2\\) calculation.\n\nchi_vals &lt;- ((drinks - exp_drinks)^2)/exp_drinks\n\nchi_vals\n\n [1] 5.44905660 0.63773585 1.82641509 1.22264151 2.95849057 8.33584906\n [7] 5.16603774 0.63773585 0.03396226 0.18490566\n\n\n\nsum(chi_vals)\n\n[1] 26.45283\n\n\nAnd now, to get the probability.\n\npchisq(sum(chi_vals), # chi stat\n       df = length(chi_vals) - 1, # degrees of freedom\n       lower.tail = FALSE) # looking RIGHT\n\n[1] 0.001721825\n\n\nHere, we get \\(p = 0.002\\), indicating that there is not an equal probability for selling drinks at different times of day.\n\n\n7.1.2.2 \\(\\chi^2\\) estimation by code\nWe can use the test chisq.test to perform this analysis as well.\n\nchisq.test(drinks)\n\n\n    Chi-squared test for given probabilities\n\ndata:  drinks\nX-squared = 26.453, df = 9, p-value = 0.001722\n\n\nAs we can see, these values are exactly the same as we just calculated by hand! Note that we can define the probability p if we want, otherwise it defaults to p = rep(1/length(x), length(x)).\n\n\n\n7.1.3 \\(\\chi^2\\) test of independence\nUsually when we use a \\(\\chi^2\\), we are looking at count data. Let’s consider the following hypothetical scenario, comparing experience with R between non-biology majors (who, in this theoretical scenario, do not regularly use R) and Biology majors who are required to take R for this class:\n\nThe above table of counts is also known as a contingency table. Intuitively, we can see a difference, but we want to perform a statistical test to see just how likely these counts would be if both groups were equally likely. We can calculate this both “by hand” and using built in R functions.\n\n\nMajor\nR experience\nNo R experience\n\n\n\n\nNon-biology\n3\n10\n\n\nBiology\n9\n2\n\n\n\n\n7.1.3.1 \\(\\chi^2\\) estimations by hand\nFirst, we can enter the data into R.\n\ndata &lt;- matrix(data = c(3,10,9,2), nrow = 2, ncol = 2, byrow = T)\n\ncolnames(data) &lt;- c(\"R\", \"No R\")\nrownames(data) &lt;- c(\"Non-biology\", \"Biology\")\n\ndata\n\n            R No R\nNon-biology 3   10\nBiology     9    2\n\n\nNext, we need the observed - expected values. We determine expected values either through probability (\\(0.5 \\cdot n\\) for equal probability for two categories) or via calculating the the expected values (see later section on expected counts). In this case, since we are looking at equally likely in each cell, we have an expected matrix as follows:\n\n# total datapoints\nN &lt;- sum(data)\n\nexpected &lt;- matrix(data = c(0.25*N,0.25*N,0.25*N,0.25*N), nrow = 2, ncol = 2, byrow = T)\n\ncolnames(expected) &lt;- c(\"R\", \"No R\")\nrownames(expected) &lt;- c(\"Non-biology\", \"Biology\")\n\nexpected\n\n            R No R\nNon-biology 6    6\nBiology     6    6\n\n\nNow we need to find our observed - expected.\n\no_e &lt;- data - expected\n\no_e\n\n             R No R\nNon-biology -3    4\nBiology      3   -4\n\n\nNote that in R we can add and subtract matrices, so there’s no reason to reformat these data!\nNow, we can square these data.\n\no_e2 &lt;- o_e^2\n\no_e2\n\n            R No R\nNon-biology 9   16\nBiology     9   16\n\n\nNext, we take these and divide them by the expected values and then sum those values.\n\nchi_matrix &lt;- o_e2/expected\n\nchi_matrix\n\n              R     No R\nNon-biology 1.5 2.666667\nBiology     1.5 2.666667\n\n\n\nsum(chi_matrix)\n\n[1] 8.333333\n\n\nHere, we get a \\(\\chi^2\\) value of 8.3333333. We can use our handy family functions to determine the probability of this event:\n\npchisq(sum(chi_matrix), df = 1, lower.tail = F)\n\n[1] 0.003892417\n\n\nHere, we get a \\(p\\) value of 0.004.\nAlternatively, we can calculate this using expected counts. For many situations, we don’t know what the baseline probability should be, so we calculate the expected counts based on what we do know. Expected counts are calculated as follows:\n\\[\nExp(x)=\\frac{\\Sigma(row_x)\\cdot\\Sigma(col_x)}{N}\n\\]\nwhere \\(N\\) is the sum of all individuals in the table. For the above example, this would look like this:\n\ndata_colsums &lt;- colSums(data)\ndata_rowsums &lt;- rowSums(data)\nN &lt;- sum(data)\n\nexpected &lt;- matrix(data = c(data_colsums[1]*data_rowsums[1],\n                            data_colsums[2]*data_rowsums[1],\n                            data_colsums[1]*data_rowsums[2],\n                            data_colsums[2]*data_rowsums[2]),\n                   nrow = 2, ncol = 2, byrow = T)\n\n# divide by total number\nexpected &lt;- expected/N\n\ncolnames(expected) &lt;- colnames(data)\nrownames(expected) &lt;- rownames(data)\n\nexpected\n\n              R No R\nNon-biology 6.5  6.5\nBiology     5.5  5.5\n\n\nHere, we can see our expected number are not quite 50/50! this will give us a different result than our previous iteration.\n\ne_o2 &lt;- ((data - expected)^2)/expected\n\nsum(e_o2)\n\n[1] 8.223776\n\n\nNow we have a \\(\\chi^2\\) of 8.22 - which, as we will see below, is the exact same value as we get for an uncorrected chisq.test from R’s default output.\n\n\n7.1.3.2 \\(\\chi^2\\) estimations in R\nWe can calculate this in R by entering in the entire table and using chisq.test.\n\ndata\n\n            R No R\nNon-biology 3   10\nBiology     9    2\n\n\n\nchi_data &lt;- chisq.test(data, correct = F)\n\nchi_data\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 8.2238, df = 1, p-value = 0.004135\n\n\nHere, we get $p = $0.004, which is significant with \\(\\alpha = 0.05\\).\nNote that these values are slightly different. they will be even more different if correct is set to TRUE. By default, R, uses a Yate’s correction for continuity. This accounts for error introduced by comparing the discrete values to a continuous distribution.\n\nchisq.test(data)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  data\nX-squared = 6.042, df = 1, p-value = 0.01397\n\n\nApplying this correction lowers the degrees of freedom, and increases the \\(p\\) value, thus making it harder to get \\(p &lt; \\alpha\\).\nNote that the Yate’s correction is only applied for 2 x 2 contingency tables.\nGiven the slight differences in calculation between by hand and what the functions of R are performing, it’s important to always show your work.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#fishers-exact-test",
    "href": "chi.html#fishers-exact-test",
    "title": "7  χ2 (Chi-squared) tests",
    "section": "7.2 Fisher’s exact test",
    "text": "7.2 Fisher’s exact test\n\\(\\chi^2\\) tests don’t work in scenarios where we have very small count sizes, such as a count size of 1. For these situations with small sample sizes and very small count sizes, we use Fisher’s exact test. This test gives us the \\(p\\) value directly - no need to use a table of any kind! Let’s say we have a \\(2x2\\) contingency table, as follows:\n\n\n\n\\(a\\)\n\\(b\\)\n\n\n\\(c\\)\n\\(d\\)\n\n\n\nWhere row totals are \\(a+b\\) and \\(c + d\\) and column totals are \\(a + c\\) and \\(b + d\\), and \\(n = a + b + c + d\\). We can calculate \\(p\\) as follows:\n\\[\np = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!n!}\n\\]\nIn R, we can use the command fisher.test to perform these calculations. For example, we have the following contingency table, looking at the number of undergrads and graduate students in introductory and graduate level statistics courses:\n\n\n\n\nUndergrad\nGraduate\n\n\n\n\nIntro Stats\n8\n1\n\n\nGrad Stats\n3\n5\n\n\n\n\nstats_students = matrix(data = c(8,1,3,5),\n                        byrow = T, ncol = 2, nrow = 2)\n\nstats_students\n\n     [,1] [,2]\n[1,]    8    1\n[2,]    3    5\n\n\n\nfisher.test(stats_students)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  stats_students\np-value = 0.04977\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.7934527 703.0167380\nsample estimates:\nodds ratio \n  11.10917 \n\n\nIn this situation, \\(p = 0.05\\) when rounded, so we would fail to reject but note that this is borderline.\n\n7.2.1 Chapter 7\nComplete problems 7.1, 7.2, 7.3, 7.5, and 7.7 as written in your text books. For these problems, please also state your null and alternative hypotheses, as well as a conclusion as to whether you support or reject the null.\nNext, do problems 7.9 and 7.11. For these two problems, pick which test is best and then perform said test. State your hypothesis and make a conclusion with respect to your \\(p\\) value.\nBe sure to submit your homework as a knitted html document.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "",
    "text": "8.1 Introduction\nPreviously, we talked about normal distributions as a method for comparing samples to overall populations or comparing individuals to overall populations. However, sample sizes can introduce some error, and oftentimes we may not have access to an entire population. In these situations, we need a better test that can account for this changing error and the effect of different sample sizes. This is especially important when comparing two samples to each other. We may find a small sample from one population and a small sample for another, and we want to determine if these came from the same overall population as effectively as possible.\nThe distribution that we commonly refer to as a \\(t\\)-distribution is also sometimes known as a “Student’s \\(t\\)-distribution” as it was first published by a man with the pseudonym of “Student”. Student was in fact William Sealy Gossett, an employee of the Guinness corporation who was barred from publishing things by his employer to ensure that trade secrets were not made known to their competitors. Knowing that his work regarding statistics was important, Gossett opted to publish his research anyway under his pseudonym.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#dataset",
    "href": "ttest.html#dataset",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.2 Dataset",
    "text": "8.2 Dataset\nFor all of the examples on this page, we will be using a dataset on the morphology of canine teeth for identification of predators killing livestock (Courtenay 2019).\n\ncanines &lt;- read_csv(\"https://figshare.com/ndownloader/files/15070175\")\n\nWe want to set up some of these columns as “factors” to make it easier to process and parse in R. We will look at the column OA for these examples. Unfortunately, it is unclear what exactly OA stands for since this paper is not published at the present time.\n\ncanines$Sample &lt;- as.factor(canines$Sample)\n\n# we will be examining the column \"OA\"\n\ncanines$OA &lt;- as.numeric(canines$OA)\n\nsummary(canines)\n\n  Sample        WIS              WIM              WIB         \n Dog :34   Min.   :0.1323   Min.   :0.1020   Min.   :0.03402  \n Fox :41   1st Qu.:0.5274   1st Qu.:0.3184   1st Qu.:0.11271  \n Wolf:28   Median :1.1759   Median :0.6678   Median :0.25861  \n           Mean   :1.6292   Mean   :1.0233   Mean   :0.44871  \n           3rd Qu.:2.4822   3rd Qu.:1.5194   3rd Qu.:0.74075  \n           Max.   :4.8575   Max.   :3.2423   Max.   :1.51721  \n       D                 RDC               LDC                OA       \n Min.   :0.005485   Min.   :0.05739   Min.   :0.02905   Min.   :100.7  \n 1st Qu.:0.034092   1st Qu.:0.28896   1st Qu.:0.22290   1st Qu.:139.2  \n Median :0.182371   Median :0.61777   Median :0.55985   Median :149.9  \n Mean   :0.250188   Mean   :0.88071   Mean   :0.84615   Mean   :148.4  \n 3rd Qu.:0.361658   3rd Qu.:1.26417   3rd Qu.:1.26754   3rd Qu.:158.0  \n Max.   :1.697461   Max.   :3.02282   Max.   :3.20533   Max.   :171.5",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#t-distribution",
    "href": "ttest.html#t-distribution",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.3 \\(t\\)-distribution",
    "text": "8.3 \\(t\\)-distribution\nFor these scenarios where we are testing a single sample mean from one or more samples we use a \\(t\\)-distributions. A \\(t\\)-distribution is a specially altered normal distribution that has been adjusted to account for the number of individuals being sampled. Specifically, a \\(t\\)-distributions with infinite degrees of freedom is the same as a normal distribution, and our degrees of freedom help create a more platykurtic distribution to account for error and uncertainty. The distribution can be calculated as follows:\n\\[\nt = \\frac{\\Gamma(\\frac{v+1}{2})}{\\sqrt{\\pi \\nu}\\Gamma(\\frac{\\nu}{2})}(1+\\frac{t^2}{\\nu})^{-\\frac{(v+1)}{2}}\n\\]\nThese \\(t\\)-distributions can be visualized as follows:\n\n\n\nIkamusumeFan - Wikipedia\n\n\nFor all \\(t\\)-tests, we calculate the degrees of freedom based on the number of samples. If comparing values to a single sample, we use \\(df = n -1\\). If we are comparing two sample means, then we have \\(df = n_1 + n_2 -2\\).\nImportantly, we are testing to see if the means of the two distributions are equal in a \\(t\\)-test. Thus, our hypotheses are as follows:\n\\(H_0: \\mu_1 = \\mu_2\\) or \\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\\(H_A: \\mu_1 \\ne \\mu_2\\) or \\(H_A: \\mu_1 - \\mu_2 \\ne 0\\)\nWhen asked about hypotheses, remember the above as the statistical hypotheses that are being directly tested.\nIn R, we have the following functions to help with \\(t\\) distributions:\n\ndt: density function of a \\(t\\)-distribution\npt: finding our \\(p\\) value from a specific \\(t\\) in a \\(t\\)-distribution\nqt: finding a particular \\(t\\) from a specific \\(p\\) in a \\(t\\)-distribution\nrt: random values from a \\(t\\)-distribution\n\nAll of the above arguments required the degrees of freedom to be declared. Unlike the normal distribution functions, these can not be adjusted for your data; tests must be performed using t.test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#t-tests",
    "href": "ttest.html#t-tests",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.4 \\(t\\)-tests",
    "text": "8.4 \\(t\\)-tests\nWe have three major types of \\(t\\)-tests:\n\nOne-sample \\(t\\)-tests: a single sample is being compared to a value, or vice versa\nTwo-sample \\(t\\)-tests: two samples are being compared to one another to see if they come from the same population\nPaired \\(t\\)-tests: before-and-after measurements of the same individuals are being compared. This is necessary to account for a repeat in the individuals being measured, and different potential baselines at initiation. In this case, we are looking to see if the difference between before and after is equal to zero.\n\nWe also have what we call a “true” \\(t\\)-test and “Welch’s” \\(t\\)-test. The formula for a “true” \\(t\\) is as follows:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n\\]\nWhere \\(s_p\\) is based on the “pooled variance” between the samples. This can be calculated as follows:\n\\[\ns_p = \\sqrt{\\frac{(n_1-1)(s_1^2)+(n_2-1)(s_2^2)}{n_1+n_2 -2}}\n\\]\nWhereas the equation for a “Welch’s” \\(t\\) is:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\n\\]\nWelch’s \\(t\\) also varies with respect to the degrees of freedom, calculated by:\n\\[\ndf = \\frac{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1-1}+\\frac{(\\frac{s_2^2}{n_2})^2}{n_2-1}}\n\\]\nOK, so why the difference?\nA \\(t\\)-test works well under a certain set of assumptions, include equal variance between samples and roughly equal sample sizes. A Welch’s \\(t\\)-test is better for scenarios with unequal variance and small sample sizes. If sample sizes and variances are equal, the two \\(t\\)-tests should perform the same.\nBecause of this, some argue that “Welch’s” should be the default \\(t\\)-test, and in R, Welch’s is the default \\(t\\)-test. If you want to specify a “regular” \\(t\\)-value, you will have to set the option var.equal = TRUE. (The default is var.equal = FALSE).\n\n8.4.1 One-sample \\(t\\)-tests\nLet’s look at the values of all of the dog samples in our canines dataset.\n\ndogs &lt;- canines %&gt;%\n  filter(Sample == \"Dog\") %&gt;%\n  select(Sample, OA)\n\nxbar &lt;- mean(dogs$OA)\nsd_dog &lt;- sd(dogs$OA)\nn &lt;- nrow(dogs)\n\nNow we have stored all of our information on our dog dataset. Let’s say that the overall populations of dogs a mean OA score of \\(143\\) with a \\(\\sigma = 1.5\\). Is our sample different than the overall population?\n\nt.test(x = dogs$OA,\n       alternative = \"two.sided\",\n       mu = 143)\n\n\n    One Sample t-test\n\ndata:  dogs$OA\nt = -0.74339, df = 33, p-value = 0.4625\nalternative hypothesis: true mean is not equal to 143\n95 percent confidence interval:\n 138.4667 145.1070\nsample estimates:\nmean of x \n 141.7869 \n\n\nAs we can see above, we fail to reject the null hypothesis that our sample is different than the overall mean for dogs.\n\n\n8.4.2 Two-sample \\(t\\)-tests\nNow let’s say we want to compare foxes and dogs to each other. Since we have all of our data in the same data frame, we will have to subset our data to ensure we are doing this properly.\n\n# already got dogs\ndog_oa &lt;- dogs$OA\n\nfoxes &lt;- canines %&gt;%\n  filter(Sample == \"Fox\") %&gt;%\n  select(Sample, OA)\n\nfox_oa &lt;- foxes$OA\n\nNow, we are ready for the test!\n\nt.test(dog_oa, fox_oa)\n\n\n    Welch Two Sample t-test\n\ndata:  dog_oa and fox_oa\nt = -6.3399, df = 72.766, p-value = 1.717e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -19.62289 -10.23599\nsample estimates:\nmean of x mean of y \n 141.7869  156.7163 \n\n\nAs we can see, the dogs and the foxes significantly differ in their OA measurement, so we reject the null hypothesis that \\(\\mu_{dog} = \\mu_{fox}\\).\n\n\n8.4.3 Paired \\(t\\)-tests\nI will do a highly simplified version of a paired \\(t\\)-test here just for demonstrations sake. Remember that you want to used paired tests when we are looking at the same individuals at different points in time.\n\n# create two random distributions\n# DEMONSTRATION ONLY\n\n# make repeatable\nset.seed(867)\n\nt1 &lt;- rnorm(20,0,1)\nt2 &lt;- rnorm(20,2,1)\n\nNow we can compare these using paired = TRUE.\n\nt.test(t1, t2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  t1 and t2\nt = -7.5663, df = 19, p-value = 3.796e-07\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.107787 -1.760973\nsample estimates:\nmean difference \n       -2.43438 \n\n\nAs we can see, we reject the null hypothesis that these distributions are equal in this case. Let’s see how this changes though if we set paired = FALSE.\n\nt.test(t1, t2)\n\n\n    Welch Two Sample t-test\n\ndata:  t1 and t2\nt = -8.1501, df = 37.48, p-value = 8.03e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.039333 -1.829428\nsample estimates:\n  mean of x   mean of y \n-0.07258938  2.36179080 \n\n\nThis value differs because, in a paired test, we are looking to see if the difference between the distributions is \\(0\\), while in the independent (standard) test we are comparing the overall distributions of the samples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#wilcoxon-tests",
    "href": "ttest.html#wilcoxon-tests",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.5 Wilcoxon tests",
    "text": "8.5 Wilcoxon tests\nWhen data (and the differences among data) are non-normal, they violate the assumptions of a \\(t\\)-test. In these cases, we have to do a Wilcoxon test (also called a Wilcoxon signed rank test). In R, the command wilcox.test also includes the Mann-Whitney \\(U\\) test for unpaired data and the standard Wilcoxon test \\(W\\) for paired data.\n\n8.5.1 Mann-Whitney \\(U\\)\nFor this test, we would perform the following procedures to figure out our statistics:\n\nRank the pooled dataset from smallest to largest, and number all numbers by their ranks\nSum the ranks for the first column and the second column\nCompute \\(U_1\\) and \\(U_2\\), comparing the smallest value to a Mann-Whitney \\(U\\) table.\n\nThe equations for these statistics are as follows, where \\(R\\) represents the sum of the ranks for that sample:\n\\[\nU_1 = n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\n\\]\n\\[\nU_2 = n_1n_2 + \\frac{n_2(n_2+1)}{2} - R_2\n\\]\nIn R, this looks like so:\n\nwilcox.test(t1, t2, paired = FALSE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  t1 and t2\nW = 11, p-value = 2.829e-09\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n8.5.2 Wilcoxon signed rank test\nFor paired samples, we want to do the Wilcoxon signed rank test. This is performed by:\n\nFinding the difference between sampling events for each sampling unit.\nOrder the differences based on their absolute value\nFind the sum of the positive ranks and the negative ranks\nThe smaller of the values is your \\(W\\) statistic.\n\nIn R, this test looks as follows:\n\nwilcox.test(t1, t2, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  t1 and t2\nV = 0, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#confidence-intervals",
    "href": "ttest.html#confidence-intervals",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.6 Confidence intervals",
    "text": "8.6 Confidence intervals\nIn \\(t\\) tests, we are looking at the difference between the means. Oftentimes, we are looking at a confidence interval for the difference between these means. This can be determined by:\n\\[\n(\\bar{x}_1-\\bar{x}_2) \\pm t_{crit}\\sqrt{\\frac{s_p^2}{n_1}+\\frac{s_p^2}{n_2}}\n\\]\nThis is very similar to the CI we calculated with the \\(Z\\) statistic. Remember that we can use the following function to find our desired \\(t\\), which requires degrees of freedom to work:\n\nqt(0.975, df = 10)\n\n[1] 2.228139",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#homework-chapter-9",
    "href": "ttest.html#homework-chapter-9",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.7 Homework: Chapter 9",
    "text": "8.7 Homework: Chapter 9\nFor Chapter 9, complete problems 9.1, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9, and 9.10.  For problems 9.3 - 9.8, be sure to state the null and alternative hypotheses and whether the test is one- or two-tailed.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "anova1.html",
    "href": "anova1.html",
    "title": "9  ANOVA: Part 1",
    "section": "",
    "text": "9.1 Introduction\nWhen we are comparing multiple (2+) populations, we perform what is called an analysis of variance - or an ANOVA. We opt for this different method because we are trying to minimize error. As you’ll recall, we use \\(\\alpha\\) to minimize our chances of making an error and coming to an incorrect conclusion regarding our data. In our previous tests (\\(t\\)-tests) we are comparing the means between two different populations, such that \\(H_0: \\mu_1 = \\mu_2\\). When comparing multiple populations, comparing the means in this direct fashion can increase the probability of introducing error into a system. Consider the following:\nlibrary(tidyverse)\n\n# needed for summarizing data\nlibrary(plyr)\n\n# needed for better Tukey tests\nlibrary(agricolae)\n# This creates a reproducible example\n# rnorm creates random datasets\n\nset.seed(8675309)\n\nfor(i in 1:100){\n  x &lt;- rnorm(10)\n  if(i == 1){\n    data &lt;- x %&gt;% as.data.frame()\n    colnames(data) &lt;- \"Data\"\n    data$Group &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x %&gt;% as.data.frame()\n    colnames(newdat) &lt;- \"Data\"\n    newdat$Group &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# summarize by group\ngg_data &lt;- ddply(data, \"Group\", summarise,\n                 N = length(Data),\n                 mean = mean(Data),\n                 sd = sd(Data),\n                 se = sd / sqrt(N))\n\nggplot(gg_data, aes(x = Group, y = mean, group = Group)) +\n  geom_point() +\n  geom_errorbar(data = gg_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Group), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-1.5,1.5)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\nAs we can see above, with just ten random samples and 100 sampling events, we get some datasets that do not have the mean included within the interquartile range, and thus have means that would be statistically different than what we draw. As we increase the number of draws, we get closer to the mean:\nset.seed(8675309)\n\nfor(i in 1:100){\n  x &lt;- rnorm(100)\n  if(i == 1){\n    data &lt;- x %&gt;% as.data.frame()\n    colnames(data) &lt;- \"Data\"\n    data$Group &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x %&gt;% as.data.frame()\n    colnames(newdat) &lt;- \"Data\"\n    newdat$Group &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# summarize by group\ngg_data &lt;- ddply(data, \"Group\", summarise,\n                 N = length(Data),\n                 mean = mean(Data),\n                 sd = sd(Data),\n                 se = sd / sqrt(N))\n\nggplot(gg_data, aes(x = Group, y = mean, group = Group)) +\n  geom_point() +\n  geom_errorbar(data = gg_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Group), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-1.5,1.5)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\nAs we can see, even with 100 sample, we still have some chances of having groups that are different! When we do pairwise comparisons, we are compounding the error and the possibility of coming to an incorrect conclusion. Thus, when comparing multiple groups, we use the variances to see if groups come from the same distribution rather than the mean.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#anova-by-hand",
    "href": "anova1.html#anova-by-hand",
    "title": "9  ANOVA: Part 1",
    "section": "9.2 ANOVA: By hand",
    "text": "9.2 ANOVA: By hand\nFor this workthrough, we will use the following example dataset:\n\nset.seed(8675309)\n\nfor(i in 1:4){\n  x &lt;- rnorm(10)\n  if(i == 1){\n    x &lt;- rnorm(10, mean = 2)\n    data &lt;- x %&gt;% as.data.frame()\n    colnames(data) &lt;- \"Data\"\n    data$Group &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x %&gt;% as.data.frame()\n    colnames(newdat) &lt;- \"Data\"\n    newdat$Group &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# split into \"typical\" table\nexpanded_data &lt;- NULL\nexpanded_data$x1 &lt;- data$Data[which(data$Group==\"x1\")]\nexpanded_data$x2 &lt;- data$Data[which(data$Group==\"x2\")]\nexpanded_data$x3 &lt;- data$Data[which(data$Group==\"x3\")]\nexpanded_data$x4 &lt;- data$Data[which(data$Group==\"x4\")]\n\nexpanded_data &lt;- expanded_data %&gt;%\n  as.data.frame()\n\n# summarize by group\ngg_data &lt;- ddply(data, \"Group\", summarise,\n                 N = length(Data),\n                 mean = mean(Data),\n                 sd = sd(Data),\n                 se = sd / sqrt(N))\n\nggplot(gg_data, aes(x = Group, y = mean, group = Group)) +\n  geom_point() +\n  geom_errorbar(data = gg_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Group), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-3,3)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\n\n\n\n\n\n\n\n\n\nexpanded_data %&gt;% round(2)\n\n     x1    x2    x3    x4\n1  0.45  1.99  0.38 -0.64\n2  3.02  0.04  1.15  0.00\n3  2.15 -0.40  1.57  0.05\n4  1.34 -0.47  0.59  0.68\n5  1.01 -0.41 -0.62 -0.25\n6  3.97  0.68 -0.23 -0.15\n7  1.56  0.69  0.06 -0.87\n8  1.10  0.53 -0.31 -1.98\n9  1.85 -0.19 -0.25  0.24\n10 1.17  0.38 -0.15  0.04\n\n\nAbove, we can see the made-up dataset where it appears as though one population differs from the other populations in our measurements. Let’s calculate an ANOVA and find out if this is the case!\nNOTE throughout this process that I am trying to name variables in a straightforward fashion so as not to lose my way.\n\n9.2.1 Calculate group means and Grand Mean\nLet us assume we have a dataset, \\(x\\), that is \\(k\\) columns and \\(n\\) rows, with \\(N\\) data points in the entire data frame. We first want to take the column means for each group \\(k\\), such that we have \\(\\bar{x}_k\\). We also need to find the mean of the entire dataset, \\(\\bar{x}_n\\). We can calculate this by taking \\(\\frac{\\Sigma x}{n}\\), which we have to calculate by column as follows.\n\n# calculate the mean of each group\n# each group is in a single column\ngroup_means &lt;- colMeans(expanded_data)\n\n# rounding to two decimal places\ngroup_means %&gt;% round(2)\n\n   x1    x2    x3    x4 \n 1.76  0.28  0.22 -0.29 \n\n\nNext, we need to calculate the number of total entries in the dataset. We have written a function to accomplish this incase some rows have different numbers of entries from others.\n\nn &lt;- 0\n\nfor(i in 1:ncol(expanded_data)){\n  # account for unequal row length, if exists\n  sample &lt;- expanded_data[,i] %&gt;% \n    as.numeric() %&gt;%\n    na.omit()\n  n &lt;- n + length(sample)\n}\n\nn\n\n[1] 40\n\n\nNext, we can calculate the grand_mean of all of the data.\n\n# sum up all the data\ndataset_sum &lt;- colSums(expanded_data) %&gt;%\n  sum()\n\n# divide by the number of data points\ngrand_mean &lt;- dataset_sum/n\n\n# display mean\ngrand_mean %&gt;% round(2)\n\n[1] 0.49\n\n\n\n\n9.2.2 Total sum of squares\nTo calculate the total sum of squares (TSS), we need to take the deviations (differences) of each point from the grand mean \\(\\bar{x}_n\\), square them, and them take the sum of them.\n\n# calculate deviates\n# can calculate across all table at once\ngrand_deviates_squared &lt;- (expanded_data - grand_mean)^2\n\n# round output for here\ngrand_deviates_squared %&gt;% round(2)\n\n      x1   x2   x3   x4\n1   0.00 2.22 0.01 1.28\n2   6.39 0.20 0.43 0.25\n3   2.74 0.81 1.17 0.20\n4   0.72 0.94 0.01 0.04\n5   0.26 0.83 1.23 0.56\n6  12.10 0.04 0.52 0.42\n7   1.13 0.04 0.19 1.87\n8   0.37 0.00 0.65 6.11\n9   1.84 0.46 0.55 0.07\n10  0.46 0.01 0.42 0.21\n\n\n\n# calculate the sum of all the deviates\nss_total &lt;- rowSums(grand_deviates_squared) %&gt;%\n  sum()\n\nss_total %&gt;% round(2)\n\n[1] 47.73\n\n\n\n\n9.2.3 Within-group sum of squares\nFor each data point, we need to calculate its deviation from its own group mean, squaring these deviations and then summing them together. We can’t calcualte this quite as elegantly as the aforementioned data, but we can write a function that will operate across the table and create a new dataset on our behalf.\n\n# replicate dataset\n# replace columns with deviate data\ngroup_deviates &lt;- expanded_data\n\n# loop through each column\nfor(i in 1:ncol(group_deviates)){\n  # get the data in each group\n  dat &lt;- group_deviates[,i]\n  # calculate the group mean\n  mu &lt;- mean(dat)\n  # calculate the group deviates\n  dev.dat &lt;- (dat - mu)^2\n  # save into table\n  group_deviates[,i] &lt;- dev.dat\n}\n\ngroup_deviates %&gt;% round(2)\n\n     x1   x2   x3   x4\n1  1.72 2.90 0.02 0.12\n2  1.59 0.06 0.87 0.08\n3  0.15 0.47 1.84 0.11\n4  0.18 0.57 0.14 0.95\n5  0.57 0.49 0.70 0.00\n6  4.89 0.16 0.20 0.02\n7  0.04 0.16 0.02 0.34\n8  0.44 0.06 0.28 2.85\n9  0.01 0.22 0.22 0.28\n10 0.35 0.01 0.14 0.11\n\n\n\n# calculate sum of data table\nss_within &lt;- colSums(group_deviates) %&gt;% \n  sum()\n\nss_within %&gt;% round(2)\n\n[1] 24.33\n\n\n\n\n9.2.4 Among-group sum of squares\nThe total sum of squares is equal to the among groups sum of squares and the within groups sum of squares added together; thus, we can solve this part with some easy arithmetic.\n\nss_among &lt;- ss_total - ss_within\n\nss_among %&gt;% round(2)\n\n[1] 23.4\n\n\n\n\n9.2.5 Calculate degrees of freedom\nOur degrees of freedom for the “between” group is the number of categories minus one (\\(K-1\\)).\n\nss_among_df &lt;- ncol(expanded_data) - 1\n\nss_among_df\n\n[1] 3\n\n\nOur degrees of freedom for the within group are the number of total samples minus the number of categories (\\(N - K\\)).\n\nss_within_df &lt;- n - ncol(expanded_data)\n\nss_within_df\n\n[1] 36\n\n\nOur degrees of freedom for the total sum of squares is the number of samples minus one (\\(N-1\\)).\n\nss_total_df &lt;- n - 1\n\nss_total_df\n\n[1] 39\n\n\n\n\n9.2.6 Calculate mean squares\nFor each category (among and within), the mean square is equal to the sum of squares divided by the degrees of freedom.\n\nms_among &lt;- ss_among/ss_among_df\n\nms_among %&gt;% round(2)\n\n[1] 7.8\n\n\n\nms_within &lt;- ss_within/ss_within_df\n\nms_within %&gt;% round(2)\n\n[1] 0.68\n\n\n\n\n9.2.7 Get \\(F\\) statistic\nWe divide the sum of squares among data point by the sum of squares within data points to obtain our \\(F\\) statistic.\n\nf_stat &lt;- ms_among/ms_within\n\nf_stat %&gt;% round(2)\n\n[1] 11.54\n\n\n\n\n9.2.8 Get \\(p\\) value\nWe can use the function pf to calculate the \\(p\\) value for any given \\(F\\). Note that this function requires two different degrees of freedom to work correctly, and we are always looking right since this is a unidirectional distribution.\n\npf(f_stat, \n   df1 = ss_among_df, \n   df2 = ss_within_df, \n   lower.tail = F)\n\n[1] 1.894073e-05\n\n\nGiven how small our \\(p\\) value is, we want to round this to \\(p&lt;0.0001\\). As we can see, it is very unlikely that these are the same population.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#anova-by-r",
    "href": "anova1.html#anova-by-r",
    "title": "9  ANOVA: Part 1",
    "section": "9.3 ANOVA: By R",
    "text": "9.3 ANOVA: By R\nFor this, we need to use the dataframe where we have all data in a single column and all ID’s in the other columns. We then show how we want the ANOVA to operate across the data using the ~ symbol.\n\ndata_aov &lt;- aov(Data ~ Group, data = data)\n\nsummary(data_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup        3  23.40   7.801   11.54 1.89e-05 ***\nResiduals   36  24.33   0.676                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs we can see, all these values match what we did by hand above!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#kruskal-wallis-tests",
    "href": "anova1.html#kruskal-wallis-tests",
    "title": "9  ANOVA: Part 1",
    "section": "9.6 Kruskal-Wallis tests",
    "text": "9.6 Kruskal-Wallis tests\nThe Kruskal-Wallis test is the non-parametric version of an ANOVA. To demonstrate this, we will be creating a non-normal distribution by pulling random values from a uniform distribution, using the random uniform function runif. Note we are rounding the data here to make it more similar to non-normal datasets you may encounter, and to increase the probability of ties.\n\nset.seed(8675309)\n\nfor(i in 1:4){\n  x &lt;- runif(10, min = -1, max = 1) %&gt;%\n    round(2)\n  if(i == 1){\n    x &lt;- runif(10, min = 1, max = 2) %&gt;%\n      round(2)\n    data &lt;- x %&gt;% as.data.frame()\n    colnames(data) &lt;- \"Data\"\n    data$Group &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x %&gt;% as.data.frame()\n    colnames(newdat) &lt;- \"Data\"\n    newdat$Group &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# split into \"typical\" table\nexpanded_data &lt;- NULL\nexpanded_data$x1 &lt;- data$Data[which(data$Group==\"x1\")]\nexpanded_data$x2 &lt;- data$Data[which(data$Group==\"x2\")]\nexpanded_data$x3 &lt;- data$Data[which(data$Group==\"x3\")]\nexpanded_data$x4 &lt;- data$Data[which(data$Group==\"x4\")]\n\nexpanded_data &lt;- expanded_data %&gt;%\n  as.data.frame()\n\n# summarize by group\ngg_data &lt;- ddply(data, \"Group\", summarise,\n                 N = length(Data),\n                 mean = mean(Data),\n                 sd = sd(Data),\n                 se = sd / sqrt(N))\n\nggplot(gg_data, aes(x = Group, y = mean, group = Group)) +\n  geom_point() +\n  geom_errorbar(data = gg_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Group), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-3,3)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\n\n\n\n\n\n\n\n\nWe can confirm these data are non-normal with Shapiro-Wilk tests and histograms. Demonstrating with the first column only:\n\nhist(data[,1])\n\n\n\n\n\n\n\n\n\nshapiro.test(data[,1])\n\n\n    Shapiro-Wilk normality test\n\ndata:  data[, 1]\nW = 0.93404, p-value = 0.02187\n\n\nThe above histogram appears non-normal, and the Shapiro-Wilk also indicates this is non-normal.\n\n9.6.1 By hand\nFirst, we need to order all the data in the entire dataset. This is easiest to do if we use the dataset with all data in a single column.\n\ndata$ranks &lt;- rank(data$Data, ties.method = \"average\")\n\n# view first couple of rows\nhead(data)\n\n  Data Group ranks\n1 1.84    x1    39\n2 1.58    x1    34\n3 1.51    x1    33\n4 1.26    x1    32\n5 1.75    x1    37\n6 1.92    x1    40\n\n\nNow, we need to calculate the sum of the ranks for each category. The below function will take the sum of the ranks for the rows which match the condition of having the group be x1.\n\nx1_sum &lt;- sum(data$ranks[which(data$Group==\"x1\")])\nx2_sum &lt;- sum(data$ranks[which(data$Group==\"x2\")])\nx3_sum &lt;- sum(data$ranks[which(data$Group==\"x3\")])\nx4_sum &lt;- sum(data$ranks[which(data$Group==\"x4\")])\n\nNow, we need to calculate our test statistic. The test statistic is \\(H\\), with: \\[H = \\frac{12}{N(N+1)} \\cdot \\Sigma \\frac{R_j^2}{n_j}-3(N+1)\\]In this equation, \\(R\\) is the sum of the ranks for a given category. This follows a \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom, with \\(k\\) referring to categories.\nFor these, we need to know what \\(n\\) is for each category.\n\nn1 &lt;- length(data$ranks[which(data$Group==\"x1\")])\nn2 &lt;- length(data$ranks[which(data$Group==\"x2\")])\nn3 &lt;- length(data$ranks[which(data$Group==\"x3\")])\nn4 &lt;- length(data$ranks[which(data$Group==\"x4\")])\n\nNext, we can calculate the sums of the \\(\\frac{R^2}{n}\\) term.\n\nr2_sum &lt;- sum(x1_sum^2/n,\n              x2_sum^2,n,\n              x3_sum^2/n,\n              x4_sum^2/n)\n\nNow, we can calculate \\(H\\).\n\nN &lt;- sum(n1, n2, n3, n4)\n\nH &lt;- ((12/(N*(N+1)))*r2_sum)-(3*(N+1))\n\nH %&gt;% round(2)\n\n[1] 57.43\n\n\nNow, we can evaluate this with a \\(\\chi^2\\) \\(p\\) value.\n\npchisq(q = H, \n       df = ncol(data)-1, \n       # remember, looking right!\n       lower.tail = FALSE)\n\n[1] 3.382831e-13\n\n\nAs we can see, the probability is extremely low with \\(p &lt; 0.0001\\). One distribution is different, and we can proceed with Tukey tests.\n\n\n9.6.2 Using R\nWe can also use R for this test.\n\nkruskal_data &lt;- kruskal.test(Data ~ Group, data)\n\nprint(kruskal_data)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Data by Group\nKruskal-Wallis chi-squared = 22.149, df = 3, p-value = 6.073e-05\n\n\nThis test is similar, but not quite the same, as what we calculated above. The package agricolae provides a more in-depth Kruskal-Wallis test that gives us more metadata. We have to use the wrapper with to get this to work properly, per the help page.\n\n# use \"with\", defining your dataset first\ndata_kruskal &lt;- with(data,\n                     # define parameters for Kruskal-Wallis test\n                     # First, variable of interest\n                     kruskal(Data,\n                             # second, grouping variable\n                             Group,\n                             # do you want group designations returned?\n                             group = TRUE,\n                             # what is this being done on?\n                             # must match dataset name in \"with\"!!!\n                             main = \"data\"))\n\n# display results\n# summary WILL NOT show the right thing\nprint(data_kruskal)\n\n$statistics\n     Chisq Df     p.chisq  t.value      MSD\n  22.14917  3 6.07312e-05 2.028094 7.252228\n\n$parameters\n            test p.ajusted name.t ntr alpha\n  Kruskal-Wallis      none  Group   4  0.05\n\n$means\n     Data  rank       std  r   Min  Max     Q25    Q50    Q75\nx1  1.633 35.50 0.2415252 10  1.21 1.92  1.5275  1.720 1.8025\nx2 -0.075 14.20 0.6424130 10 -0.88 0.69 -0.6325 -0.105 0.5575\nx3  0.020 16.15 0.5327080 10 -0.63 0.95 -0.3250 -0.110 0.3775\nx4  0.045 16.15 0.6774011 10 -0.92 0.95 -0.3500 -0.135 0.6475\n\n$comparison\nNULL\n\n$groups\n    Data groups\nx1 35.50      a\nx3 16.15      b\nx4 16.15      b\nx2 14.20      b\n\nattr(,\"class\")\n[1] \"group\"\n\n\nAs we can see, the above gives us our group designations under the $group section. This is what we need to be able to plot things, as with ANOVA above. I do not repeat those steps here.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#homework-chapter-11",
    "href": "anova1.html#homework-chapter-11",
    "title": "9  ANOVA: Part 1",
    "section": "9.7 Homework: Chapter 11",
    "text": "9.7 Homework: Chapter 11\nYour homework is to complete problems 11.1, 11.2, 11.3, and 11.4. The first two will require ANOVA, and the last two will require Kruskal-Wallis tests. For each pair of tests, you must also complete the problem by hand in addition to using the default R methods. For all problems:\n\nState your hypothesis in words\nState your hypothesis mathematically (hint: \\(\\mu\\))\nAnswer in complete sentences and don’t forget to round your answers.\nIf you reject the null hypothesis, you must plot the results and label them to show which means are different.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "10  ANOVA: Part 2",
    "section": "",
    "text": "10.1 Two-way ANOVA",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#designs",
    "href": "anova2.html#designs",
    "title": "10  ANOVA: Part 2",
    "section": "10.2 Designs",
    "text": "10.2 Designs\n\n10.2.1 Randomized block design\n\n\n10.2.2 Repeated measures\n\n\n10.2.3 Factorial ANOVA",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#friedmans-test",
    "href": "anova2.html#friedmans-test",
    "title": "10  ANOVA: Part 2",
    "section": "10.3 Friedman’s test",
    "text": "10.3 Friedman’s test",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#homework-chapter-12",
    "href": "anova2.html#homework-chapter-12",
    "title": "10  ANOVA: Part 2",
    "section": "10.4 Homework: Chapter 12",
    "text": "10.4 Homework: Chapter 12",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "cor_reg.html",
    "href": "cor_reg.html",
    "title": "11  Correlation & regression",
    "section": "",
    "text": "11.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#correlation",
    "href": "cor_reg.html#correlation",
    "title": "11  Correlation & regression",
    "section": "11.2 Correlation",
    "text": "11.2 Correlation\n\n11.2.1 Pearson’s\n\n\n11.2.2 Spearman’s\n\n\n11.2.3 Other non-parametric methods",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#correlation-1",
    "href": "cor_reg.html#correlation-1",
    "title": "11  Correlation & regression",
    "section": "11.3 Correlation",
    "text": "11.3 Correlation\n\n11.3.1 Parametric\n\n\n11.3.2 Non-parametric",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#homework",
    "href": "cor_reg.html#homework",
    "title": "11  Correlation & regression",
    "section": "11.4 Homework",
    "text": "11.4 Homework\n\n11.4.1 Chapter 13\n\n\n11.4.2 Chapter 14",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "final_exam.html",
    "href": "final_exam.html",
    "title": "12  Final exam & review",
    "section": "",
    "text": "12.1 Pick the test",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "final_exam.html#final-review",
    "href": "final_exam.html#final-review",
    "title": "12  Final exam & review",
    "section": "12.2 Final review",
    "text": "12.2 Final review",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "13  Conclusions",
    "section": "",
    "text": "13.1 Parting thoughts\nIn this class, we have covered two major things: (1) the basics of statistics for biological research and (2) the basics of using R to solve different computational problems. It is my hope that this class helps you both become a better researcher and also a more efficient researcher and student by using code to help you with your future projects.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusion.html#ꮩꮣꮣꭺꮂꭲ",
    "href": "conclusion.html#ꮩꮣꮣꭺꮂꭲ",
    "title": "13  Conclusions",
    "section": "13.2 ᏙᏓᏓᎪᎲᎢ",
    "text": "13.2 ᏙᏓᏓᎪᎲᎢ\nᏙᏓᏓᎪᎲᎢ (pronounced doh-dah-dah-go-huh-ee) is a traditional Cherokee farewell. It does not mean goodbye, but rather reflects a parting of ways until a group of folks meet again.\nI enjoyed getting to know all of you in class, and please feel free to reach out or stop by and say hi if you are ever passing through Kearney in the future or if you need help with something biology related.\nWishing you the best,\nDr. Cooper",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "14  Glossary",
    "section": "",
    "text": "14.1 Common Commands\nThe following are common useful commands used in R, with examples of their use.\nx &lt;- 10\nx\n\n[1] 10\n# make repeatable\nset.seed(930)\n\n# random string\nx &lt;- rnorm(20)\n\nx %&gt;% \n  # pass to summary\n  summary() %&gt;% \n  # pass summary through round\n  round(2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -1.94   -0.48   -0.06   -0.05    0.36    1.80\nx &lt;- c(10,11)\nx\n\n[1] 10 11",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#common-commands",
    "href": "glossary.html#common-commands",
    "title": "14  Glossary",
    "section": "",
    "text": "&lt;- / = - save a value as an object\n\n\n\n%&gt;% - “pipe” a command or output into another command. Required tidyverse to run, and you can use the shortcut CTRL SHIFT M on Windows or Mac.\n\n\n\nc - concatenate, place two values together",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#basic-statistics",
    "href": "glossary.html#basic-statistics",
    "title": "14  Glossary",
    "section": "14.2 Basic statistics",
    "text": "14.2 Basic statistics\nFor these examples, we will create a random vector of number to demonstrate how they work.\n\nx &lt;- rnorm(1000)\n\n\nmean - get the mean / average of a set of data\n\n\nmean(x)\n\n[1] 0.04962364",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Comont, R. (2020). BeeWalk dataset 2008-23.\nhttps://doi.org/10.6084/m9.figshare.12280547.v4\n\n\nCooper, J. C. (2021). Biogeographic and Ecologic\nDrivers of Avian Diversity.\n[Online.] Available at https://doi.org/10.6082/uchicago.3379.\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth\nScores. https://doi.org/10.6084/m9.figshare.8081108.v1\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020a). Patient\nages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020b). Estimation of the\nforce of infection and infectious period of skin sores in remote\nAustralian communities using interval-censored data.\nPLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset.\nhttps://doi.org/10.6084/m9.figshare.24119643.v1",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ttest.html#homework-chapter-10",
    "href": "ttest.html#homework-chapter-10",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.8 Homework: Chapter 10",
    "text": "8.8 Homework: Chapter 10\nTwo-sample means are practiced in Chapter 10. Please see Canvas for more information.\n\n\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth Scores. https://doi.org/10.6084/m9.figshare.8081108.v1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "anova1.html#post-hoc-tukey-test",
    "href": "anova1.html#post-hoc-tukey-test",
    "title": "9  ANOVA: Part 1",
    "section": "9.4 Post-hoc Tukey test",
    "text": "9.4 Post-hoc Tukey test\nANOVA tells us if a test is different, but it doesn’t tell us which test is different. To do this, we have to perform a Tukey test.\n\n9.4.1 Tukey test by hand\nTo do this by hand, we will need a lot of data from our aforementioned ANOVA test.\nWe need to calculate pairwise differences between each set of means.\n\n# calculate all pairwise differences\npairwise_mean_diffs &lt;- dist(group_means)\n\npairwise_mean_diffs %&gt;% round(2)\n\n     x1   x2   x3\nx2 1.48          \nx3 1.54 0.06     \nx4 2.05 0.57 0.51\n\n\nNext, we need a critical \\(Q\\) value against which we can compare. For Tukey, our degrees of freedom are the same as the degrees of freedom for \\(SS_{within}\\): \\(N - K\\).\n\n# set p value\ntukey_q &lt;- qtukey(p = 0.95,\n                  # get length of mean comparisons\n                  nmeans = ncol(expanded_data),\n                  df = ss_within_df)\n\ntukey_q %&gt;% round(2)\n\n[1] 3.81\n\n\nWe need to multiply \\(Q\\) by the pooled variance.This is the same as the average of the variances for each group.\n\nvar_data &lt;- 0\n\n# calculate pooled variance\nfor(i in 1:ncol(expanded_data)){\n  var_data &lt;- var_data + var(expanded_data[,i])\n}\n\npooled_var_dat &lt;- sqrt(var_data/n)\n\npooled_var_dat %&gt;% round(2)\n\n[1] 0.26\n\n\nWe can calculate the Tukey critical value by multiplying the pooled variance by \\(Q\\).\n\ntukey_critical &lt;- tukey_q*pooled_var_dat\n\ntukey_critical %&gt;% round(2)\n\n[1] 0.99\n\n\nRemember, we are comparing to the actual value, not the rounded value.\nWhich mean differences are difference compared to our critical value?\n\npairwise_mean_diffs[pairwise_mean_diffs &lt; tukey_critical] &lt;- 0\n\npairwise_mean_diffs\n\n         x1       x2       x3\nx2 1.477866                  \nx3 1.542282 0.000000         \nx4 2.051068 0.000000 0.000000\n\n\nAs we can see above, three differences cross our threshold - all associated with x1.\nWhen we graph things, we want to label this group as different. We will cover this a little later in the tutorial.\n\n\n9.4.2 Tukey test in R\nTukey tests in the R program are a bit more straightforward.\n\nTukeyHSD(data_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Data ~ Group, data = data)\n\n$Group\n             diff       lwr        upr     p adj\nx2-x1 -1.47786579 -2.468021 -0.4877106 0.0015554\nx3-x1 -1.54228164 -2.532437 -0.5521265 0.0009388\nx4-x1 -2.05106768 -3.041223 -1.0609125 0.0000147\nx3-x2 -0.06441585 -1.054571  0.9257393 0.9980525\nx4-x2 -0.57320189 -1.563357  0.4169533 0.4141599\nx4-x3 -0.50878604 -1.498941  0.4813691 0.5173399\n\n\nAs we can see above, only three comparisons have a p value of \\(&lt; 0.05\\), and thus only those three are significantly different. All involve x1.\nWe can also use HSD.test to get more specific results:\n\ntukey_data_aov &lt;- HSD.test(data_aov,\n                           # what to group by?\n                           \"Group\",\n                           # significance level?\n                           alpha = 0.05, \n                           # are data unbalanced\n                           unbalanced = FALSE,\n                           # show answer?\n                           console = TRUE)\n\n\nStudy: data_aov ~ \"Group\"\n\nHSD Test for Data \n\nMean Square Error:  0.6758192 \n\nGroup,  means\n\n         Data       std  r        se        Min      Max        Q25         Q50\nx1  1.7620153 1.0505466 10 0.2599652  0.4504476 3.972459  1.1175485  1.44911720\nx2  0.2841495 0.7532422 10 0.2599652 -0.4729986 1.985826 -0.3497379  0.21347543\nx3  0.2197337 0.7019368 10 0.2599652 -0.6150452 1.574903 -0.2436023 -0.04493909\nx4 -0.2890524 0.7345336 10 0.2599652 -1.9769014 0.684072 -0.5394534 -0.07741642\n          Q75\nx1 2.07491533\nx2 0.64579865\nx3 0.53544151\nx4 0.04323417\n\nAlpha: 0.05 ; DF Error: 36 \nCritical Value of Studentized Range: 3.808798 \n\nMinimun Significant Difference: 0.9901551 \n\nTreatments with the same letter are not significantly different.\n\n         Data groups\nx1  1.7620153      a\nx2  0.2841495      b\nx3  0.2197337      b\nx4 -0.2890524      b\n\nprint(tukey_data_aov)\n\n$statistics\n    MSerror Df      Mean       CV       MSD\n  0.6758192 36 0.4942115 166.3422 0.9901551\n\n$parameters\n   test name.t ntr StudentizedRange alpha\n  Tukey  Group   4         3.808798  0.05\n\n$means\n         Data       std  r        se        Min      Max        Q25         Q50\nx1  1.7620153 1.0505466 10 0.2599652  0.4504476 3.972459  1.1175485  1.44911720\nx2  0.2841495 0.7532422 10 0.2599652 -0.4729986 1.985826 -0.3497379  0.21347543\nx3  0.2197337 0.7019368 10 0.2599652 -0.6150452 1.574903 -0.2436023 -0.04493909\nx4 -0.2890524 0.7345336 10 0.2599652 -1.9769014 0.684072 -0.5394534 -0.07741642\n          Q75\nx1 2.07491533\nx2 0.64579865\nx3 0.53544151\nx4 0.04323417\n\n$comparison\nNULL\n\n$groups\n         Data groups\nx1  1.7620153      a\nx2  0.2841495      b\nx3  0.2197337      b\nx4 -0.2890524      b\n\nattr(,\"class\")\n[1] \"group\"\n\n\nThis output is nice because it labels groups based on which groups belong together! This will be important for plotting, and is cumbersome if you have a lot of means.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#plotting-our-anova-results",
    "href": "anova1.html#plotting-our-anova-results",
    "title": "9  ANOVA: Part 1",
    "section": "9.5 Plotting our ANOVA results",
    "text": "9.5 Plotting our ANOVA results\nWhen we plot our ANOVAs, we want to show which mean is different from all of the others. Below, I will show the full pipeline for performing an ANOVA on a dataset and plotting the data at the end using out data object.\n\ndata_aov &lt;- aov(Data ~ Group,data)\n\nsummary(data_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup        3  23.40   7.801   11.54 1.89e-05 ***\nResiduals   36  24.33   0.676                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we need to summarize things based on our plyr function.\n\n# summarize by group\ngg_data &lt;- ddply(data, \"Group\", summarise,\n                 N = length(Data),\n                 mean = mean(Data),\n                 sd = sd(Data),\n                 se = sd / sqrt(N))\n\nNow, we can use the aforementioned Tukey results to assign to groups. Because agricolae functions define the groups in a $groups slot, we can pull out these data and perform some minimal transformations to make them ready to plot. This will save us a lot of time and hassle.\n\n# note first group must be EXACT MATCH to your gg_data object\n# groups are saved in the Tukey object\n# this is true for Tukey later as well\n\n# pull out the groups slot from tukey\n# same for Kruskal later on!\nsig.labels &lt;- tukey_data_aov$groups %&gt;%\n  # convert to a data.frame\n  as.data.frame() %&gt;%\n  # create a new column - place rownames into the column\n  # converts to a format better for ggplot\n  mutate(Group = rownames(tukey_data_aov$groups)) %&gt;%\n  # rename column to prevent confusion\n  # specify dplyr; default function may be from plyr and not work\n  dplyr::rename(significance = groups)\n\nsig.labels\n\n         Data significance Group\nx1  1.7620153            a    x1\nx2  0.2841495            b    x2\nx3  0.2197337            b    x3\nx4 -0.2890524            b    x4\n\n\nNote that in the above, the row labels and the column Group are identical. We moved these data into a column to make it easier to use ggplot. If you want letters to be different - capitalized or something else - you will have to do this yourself. Now we can plot our data and add the labels!\n\nggplot(gg_data, # plot summary data\n       # Define plotting - x by group, y is mean, grouping by group\n       aes(x = Group, y = mean, group = Group)) +\n  # add points to plot for y values\n  geom_point() +\n  # add error bars around points\n  geom_errorbar(data = gg_data, \n                # define error bars\n                aes(ymin = mean - 2*se, ymax = mean+2*se,\n                    # define color, size\n                                    color = Group), width = 0.1) +\n  # add line at average for the main group\n  # this is not always known - nor requires!\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  # set vertical limits for plot\n  ylim(c(-3,3)) +\n  # make it a classic theme - more legible\n  theme_classic() +\n  # add text to plot\n  geom_text(data = sig.labels,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = Group, \n                # define height of label\n                y = -2, \n                # what are the labels?\n                label = paste0(significance))) +\n  xlab(\"Group Name\") +\n  ylab(\"Mean\") +\n  # remove legend - not needed here\n  theme(legend.position = \"none\",\n        # make label text vertical, easier to read\n        axis.text.x = element_text(angle = 90, \n                                   # vertical offset of text\n                                   vjust = 0.5, \n                                   # text size\n                                   size = 12))\n\n\n\n\n\n\n\n\nNote that I place the labels below here, but they could also be placed above. You have to position them based on your own judgment.\nAddendum: if you see a mean labeled a and b, it would be statistically indistinguishable from means labeled a and means labeled b, despite means with only an a or a b being different from each other.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  }
]