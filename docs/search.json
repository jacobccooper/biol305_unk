[
  {
    "objectID": "chi.html",
    "href": "chi.html",
    "title": "7  χ2 (Chi-squared) tests",
    "section": "",
    "text": "7.1 \\(\\chi^2\\)-squared distribution\n\\(\\chi^2\\)-squared (pronounced “kai”, and spelled “chi”) is a distribution used to understand if count data between different categories matches our expectation. For example, if we are looking at students in the class and comparing major vs. number of books read, we would expect no association, however we may find an association for a major such as English which required reading more literature. The \\(\\chi^2\\) introduces a new term degrees of freedom (\\(df\\)) which reflects the number of individuals in the study. For many tests, \\(df\\) are needed to reflect how a distribution changes with respect the number of individuals (and amount of variation possible) within a dataset. The equation for the \\(\\chi^2\\) is as follows, with the \\(\\chi^2\\) being a special case of the gamma (\\(\\gamma\\) or \\(\\Gamma\\)) distribution that is affected by the \\(df\\), which is defined as the number of rows minus one multiplied by the number of columns minus one \\(df = (rows-1)(cols-1)\\):\n\\[\nf_n(x)=\\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}x^\\frac{n}{2-1}e^\\frac{-x}{2}\n\\]\nThe \\(\\chi^2\\)-squared distribution is also represented by the following functions, which perform the same things as the previous outlined equivalents for Poisson and binomial:\nWe can view these probabilities as well:\nx &lt;- 0:10\n\ny &lt;- pchisq(x, df = 9)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#chi2-squared-distribution",
    "href": "chi.html#chi2-squared-distribution",
    "title": "7  χ2 (Chi-squared) tests",
    "section": "",
    "text": "dchisq\npchisq\nqchisq\nrchisq\n\n\n\n\n7.1.1 Calculating the test statistic\nWe evaluate \\(\\chi^2\\) tests by calculating a \\(\\chi^2\\) value based on our data and comparing it to an expected \\(\\chi^2\\) distribution. This test statistic can be evaluated by looking at a \\(\\chi^2\\) table or by using R. Note that you need to know the degrees of freedom in order to properly evaluate a \\(\\chi^2\\) test. We calculate our test statistic as follows:\n\\[\n\\chi^2=\\Sigma\\frac{(o-e)^2}{e}\n\\]\nwhere \\(e =\\) the number of expected individuals and \\(o =\\) the number of observed individuals in each category. Since we are squaring these values, we will only have positive values, and thus this will always be a one-tailed test.\nThere are multiple types of \\(\\chi^2\\) test, including the following we will cover here:\n\n\\(\\chi^2\\) Goodness-of-fit test\n\\(\\chi^2\\) test of independence\n\n\n\n7.1.2 \\(\\chi^2\\) goodness-of-fit test\nA \\(\\chi^2\\) goodness-of-fit test looks at a vector of data, or counts in different categories, and asks if the observed frequencies vary from the expected frequencies.\n\n7.1.2.1 \\(\\chi^2\\) estimate by hand\nLet’s say, for example, we have the following dataset:\n\n\n\nHour\nNo. Drinks Sold\n\n\n\n\n6-7\n3\n\n\n7-8\n8\n\n\n8-9\n15\n\n\n9-10\n7\n\n\n10-12\n5\n\n\n12-13\n20\n\n\n13-14\n18\n\n\n14-15\n8\n\n\n15-16\n10\n\n\n16-17\n12\n\n\n\nNow, we can ask if the probability of selling drinks is the same across all time periods.\n\ndrinks &lt;- c(3, 8, 15, 7, 5, 20, 18, 8, 10, 12)\n\nWe can get the expected counts by assuming an equal probability for each time period; thus, \\(Exp(x)=\\frac{N}{categories}\\).\n\n# sum all values for total number\nN &lt;- sum(drinks)\n\n# get length of vector for categories\ncats &lt;- length(drinks)\n\n# repeat calculation same number of times as length\nexp_drinks &lt;- rep(N/cats, cats)\n\nexp_drinks\n\n [1] 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6\n\n\nNow, we can do out \\(\\chi^2\\) calculation.\n\nchi_vals &lt;- ((drinks - exp_drinks)^2)/exp_drinks\n\nchi_vals\n\n [1] 5.44905660 0.63773585 1.82641509 1.22264151 2.95849057 8.33584906\n [7] 5.16603774 0.63773585 0.03396226 0.18490566\n\n\n\nsum(chi_vals)\n\n[1] 26.45283\n\n\nAnd now, to get the probability.\n\npchisq(sum(chi_vals), # chi stat\n       df = length(chi_vals) - 1, # degrees of freedom\n       lower.tail = FALSE) # looking RIGHT\n\n[1] 0.001721825\n\n\nHere, we get \\(p = 0.002\\), indicating that there is not an equal probability for selling drinks at different times of day.\n\n\n7.1.2.2 \\(\\chi^2\\) estimation by code\nWe can use the test chisq.test to perform this analysis as well.\n\nchisq.test(drinks)\n\n\n    Chi-squared test for given probabilities\n\ndata:  drinks\nX-squared = 26.453, df = 9, p-value = 0.001722\n\n\nAs we can see, these values are exactly the same as we just calculated by hand! Note that we can define the probability p if we want, otherwise it defaults to p = rep(1/length(x), length(x)).\n\n\n\n7.1.3 \\(\\chi^2\\) test of independence\nUsually when we use a \\(\\chi^2\\), we are looking at count data. Let’s consider the following hypothetical scenario, comparing experience with R between non-biology majors (who, in this theoretical scenario, do not regularly use R) and Biology majors who are required to take R for this class:\n\nThe above table of counts is also known as a contingency table. Intuitively, we can see a difference, but we want to perform a statistical test to see just how likely these counts would be if both groups were equally likely. We can calculate this both “by hand” and using built in R functions.\n\n\nMajor\nR experience\nNo R experience\n\n\n\n\nNon-biology\n3\n10\n\n\nBiology\n9\n2\n\n\n\n\n7.1.3.1 \\(\\chi^2\\) estimations by hand\nFirst, we can enter the data into R.\n\ndata &lt;- matrix(data = c(3,10,9,2), nrow = 2, ncol = 2, byrow = T)\n\ncolnames(data) &lt;- c(\"R\", \"No R\")\nrownames(data) &lt;- c(\"Non-biology\", \"Biology\")\n\ndata\n\n            R No R\nNon-biology 3   10\nBiology     9    2\n\n\nNext, we need the observed - expected values. We determine expected values either through probability (\\(0.5 \\cdot n\\) for equal probability for two categories) or via calculating the the expected values (see later section on expected counts). In this case, since we are looking at equally likely in each cell, we have an expected matrix as follows:\n\n# total datapoints\nN &lt;- sum(data)\n\nexpected &lt;- matrix(data = c(0.25*N,0.25*N,0.25*N,0.25*N), nrow = 2, ncol = 2, byrow = T)\n\ncolnames(expected) &lt;- c(\"R\", \"No R\")\nrownames(expected) &lt;- c(\"Non-biology\", \"Biology\")\n\nexpected\n\n            R No R\nNon-biology 6    6\nBiology     6    6\n\n\nNow we need to find our observed - expected.\n\no_e &lt;- data - expected\n\no_e\n\n             R No R\nNon-biology -3    4\nBiology      3   -4\n\n\nNote that in R we can add and subtract matrices, so there’s no reason to reformat these data!\nNow, we can square these data.\n\no_e2 &lt;- o_e^2\n\no_e2\n\n            R No R\nNon-biology 9   16\nBiology     9   16\n\n\nNext, we take these and divide them by the expected values and then sum those values.\n\nchi_matrix &lt;- o_e2/expected\n\nchi_matrix\n\n              R     No R\nNon-biology 1.5 2.666667\nBiology     1.5 2.666667\n\n\n\nsum(chi_matrix)\n\n[1] 8.333333\n\n\nHere, we get a \\(\\chi^2\\) value of 8.3333333. We can use our handy family functions to determine the probability of this event:\n\npchisq(sum(chi_matrix), df = 1, lower.tail = F)\n\n[1] 0.003892417\n\n\nHere, we get a \\(p\\) value of 0.004.\nAlternatively, we can calculate this using expected counts. For many situations, we don’t know what the baseline probability should be, so we calculate the expected counts based on what we do know. Expected counts are calculated as follows:\n\\[\nExp(x)=\\frac{\\Sigma(row_x)\\cdot\\Sigma(col_x)}{N}\n\\]\nwhere \\(N\\) is the sum of all individuals in the table. For the above example, this would look like this:\n\ndata_colsums &lt;- colSums(data)\ndata_rowsums &lt;- rowSums(data)\nN &lt;- sum(data)\n\nexpected &lt;- matrix(data = c(data_colsums[1]*data_rowsums[1],\n                            data_colsums[2]*data_rowsums[1],\n                            data_colsums[1]*data_rowsums[2],\n                            data_colsums[2]*data_rowsums[2]),\n                   nrow = 2, ncol = 2, byrow = T)\n\n# divide by total number\nexpected &lt;- expected/N\n\ncolnames(expected) &lt;- colnames(data)\nrownames(expected) &lt;- rownames(data)\n\nexpected\n\n              R No R\nNon-biology 6.5  6.5\nBiology     5.5  5.5\n\n\nHere, we can see our expected number are not quite 50/50! this will give us a different result than our previous iteration.\n\ne_o2 &lt;- ((data - expected)^2)/expected\n\nsum(e_o2)\n\n[1] 8.223776\n\n\nNow we have a \\(\\chi^2\\) of 8.22 - which, as we will see below, is the exact same value as we get for an uncorrected chisq.test from R’s default output.\n\n\n7.1.3.2 \\(\\chi^2\\) estimations in R\nWe can calculate this in R by entering in the entire table and using chisq.test.\n\ndata\n\n            R No R\nNon-biology 3   10\nBiology     9    2\n\n\n\nchi_data &lt;- chisq.test(data, correct = F)\n\nchi_data\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 8.2238, df = 1, p-value = 0.004135\n\n\nHere, we get $p = $0.004, which is significant with \\(\\alpha = 0.05\\).\nNote that these values are slightly different. they will be even more different if correct is set to TRUE. By default, R, uses a Yate’s correction for continuity. This accounts for error introduced by comparing the discrete values to a continuous distribution.\n\nchisq.test(data)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  data\nX-squared = 6.042, df = 1, p-value = 0.01397\n\n\nApplying this correction lowers the degrees of freedom, and increases the \\(p\\) value, thus making it harder to get \\(p &lt; \\alpha\\).\nNote that the Yate’s correction is only applied for 2 x 2 contingency tables.\nGiven the slight differences in calculation between by hand and what the functions of R are performing, it’s important to always show your work.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#fishers-exact-test",
    "href": "chi.html#fishers-exact-test",
    "title": "7  χ2 (Chi-squared) tests",
    "section": "7.2 Fisher’s exact test",
    "text": "7.2 Fisher’s exact test\n\\(\\chi^2\\) tests don’t work in scenarios where we have very small count sizes, such as a count size of 1. For these situations with small sample sizes and very small count sizes, we use Fisher’s exact test. This test gives us the \\(p\\) value directly - no need to use a table of any kind! Let’s say we have a \\(2x2\\) contingency table, as follows:\n\n\n\n\\(a\\)\n\\(b\\)\n\n\n\\(c\\)\n\\(d\\)\n\n\n\nWhere row totals are \\(a+b\\) and \\(c + d\\) and column totals are \\(a + c\\) and \\(b + d\\), and \\(n = a + b + c + d\\). We can calculate \\(p\\) as follows:\n\\[\np = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!n!}\n\\]\nIn R, we can use the command fisher.test to perform these calculations. For example, we have the following contingency table, looking at the number of undergrads and graduate students in introductory and graduate level statistics courses:\n\n\n\n\nUndergrad\nGraduate\n\n\n\n\nIntro Stats\n8\n1\n\n\nGrad Stats\n3\n5\n\n\n\n\nstats_students = matrix(data = c(8,1,3,5),\n                        byrow = T, ncol = 2, nrow = 2)\n\nstats_students\n\n     [,1] [,2]\n[1,]    8    1\n[2,]    3    5\n\n\n\nfisher.test(stats_students)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  stats_students\np-value = 0.04977\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.7934527 703.0167380\nsample estimates:\nodds ratio \n  11.10917 \n\n\nIn this situation, \\(p = 0.05\\) when rounded, so we would fail to reject but note that this is borderline.\n\n7.2.1 Chapter 7\nComplete problems 7.1, 7.2, 7.3, 7.5, and 7.7 as written in your text books. For these problems, please also state your null and alternative hypotheses, as well as a conclusion as to whether you support or reject the null.\nNext, do problems 7.9 and 7.11. For these two problems, pick which test is best and then perform said test. State your hypothesis and make a conclusion with respect to your \\(p\\) value.\nBe sure to submit your homework as a knitted html document.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "",
    "text": "8.1 Introduction\nPreviously, we talked about normal distributions as a method for comparing samples to overall populations or comparing individuals to overall populations. However, sample sizes can introduce some error, and oftentimes we may not have access to an entire population. In these situations, we need a better test that can account for this changing error and the effect of different sample sizes. This is especially important when comparing two samples to each other. We may find a small sample from one population and a small sample for another, and we want to determine if these came from the same overall population as effectively as possible.\nThe distribution that we commonly refer to as a \\(t\\)-distribution is also sometimes known as a “Student’s \\(t\\)-distribution” as it was first published by a man with the pseudonym of “Student”. Student was in fact William Sealy Gossett, an employee of the Guinness corporation who was barred from publishing things by his employer to ensure that trade secrets were not made known to their competitors. Knowing that his work regarding statistics was important, Gossett opted to publish his research anyway under his pseudonym.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#dataset",
    "href": "ttest.html#dataset",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.2 Dataset",
    "text": "8.2 Dataset\nFor all of the examples on this page, we will be using a dataset on the morphology of canine teeth for identification of predators killing livestock (Courtenay 2019).\n\ncanines &lt;- read_csv(\"https://figshare.com/ndownloader/files/15070175\")\n\nWe want to set up some of these columns as “factors” to make it easier to process and parse in R. We will look at the column OA for these examples. Unfortunately, it is unclear what exactly OA stands for since this paper is not published at the present time.\n\ncanines$Sample &lt;- as.factor(canines$Sample)\n\n# we will be examining the column \"OA\"\n\ncanines$OA &lt;- as.numeric(canines$OA)\n\nsummary(canines)\n\n  Sample        WIS              WIM              WIB         \n Dog :34   Min.   :0.1323   Min.   :0.1020   Min.   :0.03402  \n Fox :41   1st Qu.:0.5274   1st Qu.:0.3184   1st Qu.:0.11271  \n Wolf:28   Median :1.1759   Median :0.6678   Median :0.25861  \n           Mean   :1.6292   Mean   :1.0233   Mean   :0.44871  \n           3rd Qu.:2.4822   3rd Qu.:1.5194   3rd Qu.:0.74075  \n           Max.   :4.8575   Max.   :3.2423   Max.   :1.51721  \n       D                 RDC               LDC                OA       \n Min.   :0.005485   Min.   :0.05739   Min.   :0.02905   Min.   :100.7  \n 1st Qu.:0.034092   1st Qu.:0.28896   1st Qu.:0.22290   1st Qu.:139.2  \n Median :0.182371   Median :0.61777   Median :0.55985   Median :149.9  \n Mean   :0.250188   Mean   :0.88071   Mean   :0.84615   Mean   :148.4  \n 3rd Qu.:0.361658   3rd Qu.:1.26417   3rd Qu.:1.26754   3rd Qu.:158.0  \n Max.   :1.697461   Max.   :3.02282   Max.   :3.20533   Max.   :171.5",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#t-distribution",
    "href": "ttest.html#t-distribution",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.3 \\(t\\)-distribution",
    "text": "8.3 \\(t\\)-distribution\nFor these scenarios where we are testing a single sample mean from one or more samples we use a \\(t\\)-distributions. A \\(t\\)-distribution is a specially altered normal distribution that has been adjusted to account for the number of individuals being sampled. Specifically, a \\(t\\)-distributions with infinite degrees of freedom is the same as a normal distribution, and our degrees of freedom help create a more platykurtic distribution to account for error and uncertainty. The distribution can be calculated as follows:\n\\[\nt = \\frac{\\Gamma(\\frac{v+1}{2})}{\\sqrt{\\pi \\nu}\\Gamma(\\frac{\\nu}{2})}(1+\\frac{t^2}{\\nu})^{-\\frac{(v+1)}{2}}\n\\]\nThese \\(t\\)-distributions can be visualized as follows:\n\n\n\nIkamusumeFan - Wikipedia\n\n\nFor all \\(t\\)-tests, we calculate the degrees of freedom based on the number of samples. If comparing values to a single sample, we use \\(df = n -1\\). If we are comparing two sample means, then we have \\(df = n_1 + n_2 -2\\).\nImportantly, we are testing to see if the means of the two distributions are equal in a \\(t\\)-test. Thus, our hypotheses are as follows:\n\\(H_0: \\mu_1 = \\mu_2\\) or \\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\\(H_A: \\mu_1 \\ne \\mu_2\\) or \\(H_A: \\mu_1 - \\mu_2 \\ne 0\\)\nWhen asked about hypotheses, remember the above as the statistical hypotheses that are being directly tested.\nIn R, we have the following functions to help with \\(t\\) distributions:\n\ndt: density function of a \\(t\\)-distribution\npt: finding our \\(p\\) value from a specific \\(t\\) in a \\(t\\)-distribution\nqt: finding a particular \\(t\\) from a specific \\(p\\) in a \\(t\\)-distribution\nrt: random values from a \\(t\\)-distribution\n\nAll of the above arguments required the degrees of freedom to be declared. Unlike the normal distribution functions, these can not be adjusted for your data; tests must be performed using t.test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#t-tests",
    "href": "ttest.html#t-tests",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.4 \\(t\\)-tests",
    "text": "8.4 \\(t\\)-tests\nWe have three major types of \\(t\\)-tests:\n\nOne-sample \\(t\\)-tests: a single sample is being compared to a value, or vice versa\nTwo-sample \\(t\\)-tests: two samples are being compared to one another to see if they come from the same population\nPaired \\(t\\)-tests: before-and-after measurements of the same individuals are being compared. This is necessary to account for a repeat in the individuals being measured, and different potential baselines at initiation. In this case, we are looking to see if the difference between before and after is equal to zero.\n\nWe also have what we call a “true” \\(t\\)-test and “Welch’s” \\(t\\)-test. The formula for a “true” \\(t\\) is as follows:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n\\]\nWhere \\(s_p\\) is based on the “pooled variance” between the samples. This can be calculated as follows:\n\\[\ns_p = \\sqrt{\\frac{(n_1-1)(s_1^2)+(n_2-1)(s_2^2)}{n_1+n_2 -2}}\n\\]\nWhereas the equation for a “Welch’s” \\(t\\) is:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\n\\]\nWelch’s \\(t\\) also varies with respect to the degrees of freedom, calculated by:\n\\[\ndf = \\frac{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1-1}+\\frac{(\\frac{s_2^2}{n_2})^2}{n_2-1}}\n\\]\nOK, so why the difference?\nA \\(t\\)-test works well under a certain set of assumptions, include equal variance between samples and roughly equal sample sizes. A Welch’s \\(t\\)-test is better for scenarios with unequal variance and small sample sizes. If sample sizes and variances are equal, the two \\(t\\)-tests should perform the same.\nBecause of this, some argue that “Welch’s” should be the default \\(t\\)-test, and in R, Welch’s is the default \\(t\\)-test. If you want to specify a “regular” \\(t\\)-value, you will have to set the option var.equal = TRUE. (The default is var.equal = FALSE).\n\n8.4.1 One-sample \\(t\\)-tests\nLet’s look at the values of all of the dog samples in our canines dataset.\n\ndogs &lt;- canines %&gt;%\n  filter(Sample == \"Dog\") %&gt;%\n  select(Sample, OA)\n\nxbar &lt;- mean(dogs$OA)\nsd_dog &lt;- sd(dogs$OA)\nn &lt;- nrow(dogs)\n\nNow we have stored all of our information on our dog dataset. Let’s say that the overall populations of dogs a mean OA score of \\(143\\) with a \\(\\sigma = 1.5\\). Is our sample different than the overall population?\n\nt.test(x = dogs$OA,\n       alternative = \"two.sided\",\n       mu = 143)\n\n\n    One Sample t-test\n\ndata:  dogs$OA\nt = -0.74339, df = 33, p-value = 0.4625\nalternative hypothesis: true mean is not equal to 143\n95 percent confidence interval:\n 138.4667 145.1070\nsample estimates:\nmean of x \n 141.7869 \n\n\nAs we can see above, we fail to reject the null hypothesis that our sample is different than the overall mean for dogs.\n\n\n8.4.2 Two-sample \\(t\\)-tests\nNow let’s say we want to compare foxes and dogs to each other. Since we have all of our data in the same data frame, we will have to subset our data to ensure we are doing this properly.\n\n# already got dogs\ndog_oa &lt;- dogs$OA\n\nfoxes &lt;- canines %&gt;%\n  filter(Sample == \"Fox\") %&gt;%\n  select(Sample, OA)\n\nfox_oa &lt;- foxes$OA\n\nNow, we are ready for the test!\n\nt.test(dog_oa, fox_oa)\n\n\n    Welch Two Sample t-test\n\ndata:  dog_oa and fox_oa\nt = -6.3399, df = 72.766, p-value = 1.717e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -19.62289 -10.23599\nsample estimates:\nmean of x mean of y \n 141.7869  156.7163 \n\n\nAs we can see, the dogs and the foxes significantly differ in their OA measurement, so we reject the null hypothesis that \\(\\mu_{dog} = \\mu_{fox}\\).\n\n\n8.4.3 Paired \\(t\\)-tests\nI will do a highly simplified version of a paired \\(t\\)-test here just for demonstrations sake. Remember that you want to used paired tests when we are looking at the same individuals at different points in time.\n\n# create two random distributions\n# DEMONSTRATION ONLY\n\n# make repeatable\nset.seed(867)\n\nt1 &lt;- rnorm(20,0,1)\nt2 &lt;- rnorm(20,2,1)\n\nNow we can compare these using paired = TRUE.\n\nt.test(t1, t2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  t1 and t2\nt = -7.5663, df = 19, p-value = 3.796e-07\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.107787 -1.760973\nsample estimates:\nmean difference \n       -2.43438 \n\n\nAs we can see, we reject the null hypothesis that these distributions are equal in this case. Let’s see how this changes though if we set paired = FALSE.\n\nt.test(t1, t2)\n\n\n    Welch Two Sample t-test\n\ndata:  t1 and t2\nt = -8.1501, df = 37.48, p-value = 8.03e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.039333 -1.829428\nsample estimates:\n  mean of x   mean of y \n-0.07258938  2.36179080 \n\n\nThis value differs because, in a paired test, we are looking to see if the difference between the distributions is \\(0\\), while in the independent (standard) test we are comparing the overall distributions of the samples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#wilcoxon-tests",
    "href": "ttest.html#wilcoxon-tests",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.5 Wilcoxon tests",
    "text": "8.5 Wilcoxon tests\nWhen data (and the differences among data) are non-normal, they violate the assumptions of a \\(t\\)-test. In these cases, we have to do a Wilcoxon test (also called a Wilcoxon signed rank test). In R, the command wilcox.test also includes the Mann-Whitney \\(U\\) test for unpaired data and the standard Wilcoxon test \\(W\\) for paired data.\n\n8.5.1 Mann-Whitney \\(U\\)\nFor this test, we would perform the following procedures to figure out our statistics:\n\nRank the pooled dataset from smallest to largest, and number all numbers by their ranks\nSum the ranks for the first column and the second column\nCompute \\(U_1\\) and \\(U_2\\), comparing the smallest value to a Mann-Whitney \\(U\\) table.\n\nThe equations for these statistics are as follows, where \\(R\\) represents the sum of the ranks for that sample:\n\\[\nU_1 = n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\n\\]\n\\[\nU_2 = n_1n_2 + \\frac{n_2(n_2+1)}{2} - R_2\n\\]\nIn R, this looks like so:\n\nwilcox.test(t1, t2, paired = FALSE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  t1 and t2\nW = 11, p-value = 2.829e-09\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n8.5.2 Wilcoxon signed rank test\nFor paired samples, we want to do the Wilcoxon signed rank test. This is performed by:\n\nFinding the difference between sampling events for each sampling unit.\nOrder the differences based on their absolute value\nFind the sum of the positive ranks and the negative ranks\nThe smaller of the values is your \\(W\\) statistic.\n\nIn R, this test looks as follows:\n\nwilcox.test(t1, t2, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  t1 and t2\nV = 0, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#confidence-intervals",
    "href": "ttest.html#confidence-intervals",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.6 Confidence intervals",
    "text": "8.6 Confidence intervals\nIn \\(t\\) tests, we are looking at the difference between the means. Oftentimes, we are looking at a confidence interval for the difference between these means. This can be determined by:\n\\[\n(\\bar{x}_1-\\bar{x}_2) \\pm t_{crit}\\sqrt{\\frac{s_p^2}{n_1}+\\frac{s_p^2}{n_2}}\n\\]\nThis is very similar to the CI we calculated with the \\(Z\\) statistic. Remember that we can use the following function to find our desired \\(t\\), which requires degrees of freedom to work:\n\nqt(0.975, df = 10)\n\n[1] 2.228139",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  },
  {
    "objectID": "ttest.html#homework-chapter-9",
    "href": "ttest.html#homework-chapter-9",
    "title": "8  Testing means with \\(t\\)-tests",
    "section": "8.7 Homework: Chapter 9",
    "text": "8.7 Homework: Chapter 9\n\n\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth Scores. https://doi.org/10.6084/m9.figshare.8081108.v1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing means with $t$-tests</span>"
    ]
  }
]