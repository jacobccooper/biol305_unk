[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biology 305: Biostatistics",
    "section": "",
    "text": "Preface\nWelcome to Biology 305 at the University of Nebraska at Kearney! Material in this class was designed by Dr. Melissa Wuellner and adapted by Dr. Jacob C. Cooper for use in R.\nIn this class, you will learn:\n\nThe basics of study design, the importance of understanding your research situation before embarking on a full study, and practice creating research frameworks based on different scenarios.\nThe basics of data analysis, including understanding what kind of variables are being collected, why understanding variable types are important, and basic tests to understand univariate distributions.\nBasic multivariate statistics, including ANOVA, correlation, and regression, for comparing multiple different groups.\nThe basics of coding and working in R for performing statistical analyses.\n\nThis site will help you navigate different homework assignments to perform the necessary R tests. Furthermore, this GitHub repository contains all of the homework dataframes, so you will not have to manually enter assignments if you use R to complete your assignments.\nWelcome to class!\nDr. Jacob C. Cooper, BHS 321",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "1  Intro to R",
    "section": "",
    "text": "2 Setup\nFirst, we need to download R onto your machine. We are also going to download RStudio to assist with creating R scripts and documents.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#installing-r",
    "href": "intro_to_r.html#installing-r",
    "title": "1  Intro to R",
    "section": "2.1 Installing R",
    "text": "2.1 Installing R\nFirst, navigate to the R download and install page. Download the appropriate version for your operating system (Windows, Mac, or Linux). Note that coding will be formatted slightly different for Windows than for other operating systems. If you have a Chromebook, you will have to follow the online instructions for installing both programs on Chrome.\nFollow the installation steps for R, and verify that the installation was successful by searching for R on your machine. You should be presented with a coding window that looks like the following:\nR version 4.4.1 (2024-06-14) -- \"Race for Your Life\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: aarch64-apple-darwin20\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\nIf that screen appears, congratulations! R is properly installed. If the install was not successful, please talk to Dr. Cooper and check with your classmates as well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#installing-rstudio",
    "href": "intro_to_r.html#installing-rstudio",
    "title": "1  Intro to R",
    "section": "2.2 Installing RStudio",
    "text": "2.2 Installing RStudio\nRStudio is a GUI (graphics user interface) that helps make R easier to use. Furthermore, it allows you to create documents in R, including websites (such as this one), PDFs, and even presentations. This can greatly streamline the research pipeline and help you publish your results and associated code in a quick and efficient fashion.\nHead over the the RStudio download website and download “RStudio Desktop”, which is free. Be sure to pick the correct version for your machine.\nOpen RStudio on your machine. You should be presented with something like the following:\n\n\n\nRStudio start window. Note that the screen is split into four different quadrants. Top left: R documents; bottom left: R program; top right: environment window; bottom right: plots, help, and directories.\n\n\nIn RStudio, the top left window is always going to be our coding window. This is where we will type all of our code and create our documents. In the bottom left we will see R executing the code. This will show what the computer is “thinking” and will help us spot any potential issues. The top right window is the “environment”, which shows what variables and datasets are stored within the computers’ memory. (It can also show some other things, but we aren’t concerned with that at this point). The bottom right window is the “display” window. This is where plots and help windows will appear if they don’t appear in the document (top left) window itself.\nNow, we will create our first R document!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#setup-1",
    "href": "intro_to_r.html#setup-1",
    "title": "1  Intro to R",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nIn this class, we will be creating assignments in what is called RMarkdown. This is a rich-text version of R that allows us to create documents with the code embedded. In RStudio, click the “+” button in the far top left to open the New Document menu. Scroll down this list and click on R Markdown.\nA screen such as this will appear:\n\n\n\nA new file window for an RMarkdown file.\n\n\nAfter entering a title and your name and selecting document in the left hand menu, click OK.\n\n\n\nAn example of a markdown script.\n\n\nIn the image above, we can see what a “default” RMarkdown script looks like after creating the file. At the top of the document, between all of the dashes, we have the yaml header that tells R what kind of document will be created, who the author is, and tells it to use today’s date. In this class, we will be saving documents as html as they are the easiest documents to create and save. These documents will include all of your code, text, and even any plots you may create!\nPlain text in the document will be rendered as plain text in the document. (I.e., whatever you type normally will become “normal text” in the finished document). Lines preceded with # will become headers, with ## being a second level header and ### being a third level header, etc. Words can also be made italic by putting an asterisk on each side of the word (*italic*) and bold by putting two asterisks on each side (**bold**). URLs are also supported, with &lt;&gt; on each side of a URL making it clickable, and words being hyperlinked by typing [words to show](target URL).\nWe also have code “chunks” that are shown above. A code chunk can be manually typed out or inserted by pressing CTRL + ALT + I (Windows, Linux) or COMMAND + OPTION + I (Mac). Everything inside a “code chunk” will be read as R code and executed as such. Note that you can have additional commands in the R chunks, but we won’t cover that for now.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#using-code-chunks",
    "href": "intro_to_r.html#using-code-chunks",
    "title": "1  Intro to R",
    "section": "3.2 Using code chunks",
    "text": "3.2 Using code chunks\nIn your computer, erase all information except for the yaml header between the dashes on your computer. Save your file in a folder where you want your assignment to be located. It is important you do this step up front as the computer will sometimes save in random places if you don’t specify a file location at the beginning. Don’t forget to save your work frequently!\n\n\n\nText to type in your Rmarkdown document.\n\n\nAfter typing this into the document, hit knit near the top of the upper left window. R will now create an HTML document that should look like this:\n\n\n\nThe output from the above code knitted into a document.\n\n\nWe can see now that the HTML document has the title of the document, the author’s name, the date on which the code was run, and a greyed-out box with color coded R code followed by the output. Let’s try something a little more complex. Create a new code chunk and type the following:\n\nx &lt;- 1:10\n\nThis will create a variable in R, x, that is sequentially each whole number between 1 and 10. We can see this by highlighting or typing only the letter x and running that line of code by clicking CTRL + ENTER (Windows / Linux) or COMMAND + ENTER (Mac).\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIf you look at the top right window, you will also see the value x in the environment defined as int [1:10] 1 2 3 4 5 6 7 8 9 10. This indicates that x is integer data spanning ten positions numbered 1 to 10. Since the vector is small, it displays every number in the sequence.\n\n\n\nRStudio environment window showing saved objects. These are in the computer’s memory.\n\n\nLet’s create another vector y that is the squared values of x, such that \\(y=x^2\\). We can raise values to an exponent by using ^.\n\ny &lt;- x^2\ny\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nNow we have the value y in the environment that is the square of the values of x. This is a numeric vector of 10 values numbered 1 to 10 where each value corresponds to a square of the x value. We can raise things to any value however, including \\(x^x\\)!\n\nx^x\n\n [1]           1           4          27         256        3125       46656\n [7]      823543    16777216   387420489 10000000000\n\n\nAs we can see, since I didn’t “store” this value as a variable in R using &lt;-, the value is not in the environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#plotting",
    "href": "intro_to_r.html#plotting",
    "title": "1  Intro to R",
    "section": "3.3 Plotting",
    "text": "3.3 Plotting\nNow, let’s try creating a plot. This is easy in R, as we just use the command plot.\n\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nBy specifying the y and x components in plot, we can quickly generate a point plot. We can alter the visual parameters of this plot using a few different commands. I will outline these below with inline notes. Inline notes in the code can be made by using a # symbol before them, which basically tells R to ignore everything after the #. For example:\n\nprint(\"Test\")\n\n[1] \"Test\"\n\n# print(\"Test 2\")\n\nThis prints the word Test, but doesn’t print Test 2.\nNow let’s make the plot with some new visual parameters.\n\nplot(x = x, # specify x values\n     y = y, # specify y values\n     ylab = \"Y Values\", # specify Y label\n     xlab = \"X Values\", # specify X label\n     main = \"Plot Title\", # specify main title\n     pch = 19, # adjust point style\n     col = \"red\") # make points red",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#tab-complete",
    "href": "intro_to_r.html#tab-complete",
    "title": "1  Intro to R",
    "section": "3.4 Tab complete",
    "text": "3.4 Tab complete\nRStudio allows for “tab-completing” while typing code. Tab-completing is a way of typing the first part of a command, variable name, or file name and hitting “tab” to show all options with that spelling. You should use tab completing because it:\n\nreduces spelling mistakes\nreduces filepath mistakes\nincreases the speed at which you code\nprovides help with specific functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#help",
    "href": "intro_to_r.html#help",
    "title": "1  Intro to R",
    "section": "3.5 Help",
    "text": "3.5 Help\nAt any point in R, you can look up “help” for a specific function by typing ?functionname. Try this on your computer with the following:\n\n?mean",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#downloading-data",
    "href": "intro_to_r.html#downloading-data",
    "title": "1  Intro to R",
    "section": "4.1 Downloading data",
    "text": "4.1 Downloading data\nNow, we need to download our first data set. These datasets are stored on GitHub. We are going to be looking at data from Dr. Cooper’s dissertation concerning Afrotropical bird distributions (Cooper 2021). This website is in the data folder on this websites’ GitHub page, accessible here.\n\n# first, declare filepath\n# I will try to give you the filepath for each assignment\n# if not, check the URL pattern for the file\n\n# create\nranges.url &lt;- curl(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/datasets/lacustrine_range_size.csv\")\n# read comma separated file (csv) into R memory\nranges &lt;- read_csv(ranges.url)\n\nRows: 12 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (9): combined_current_km2, consensus_km2, bioclim_current_km2, 2050_comb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAlternatively, we can use the operator %&gt;% to simplify this process. %&gt;% means “take whatever you got from the previous step and pipe it into the next step”. So, the following does the exact same thing:\n\nranges &lt;- curl(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/datasets/lacustrine_range_size.csv\") %&gt;%\n  read_csv()\n\nRows: 12 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (9): combined_current_km2, consensus_km2, bioclim_current_km2, 2050_comb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUsing the %&gt;% is preferred as you can better set up a workflow and because it more closely mimics other coding languages, such as bash.\nLet’s view the data to see if it worked. We can use the command head to view the first few rows:\n\nhead(ranges)\n\n# A tibble: 6 × 10\n  species                combined_current_km2 consensus_km2 bioclim_current_km2\n  &lt;chr&gt;                                 &lt;dbl&gt;         &lt;dbl&gt;               &lt;dbl&gt;\n1 Batis_diops                          25209.         6694.              19241.\n2 Chamaetylas_poliophrys               68171.         1106.              68158.\n3 Cinnyris_regius                      60939.        13305.              53627.\n4 Cossypha_archeri                     27021.         6409.              11798.\n5 Cyanomitra_alinae                    78680.        34320.              63381.\n6 Graueria_vittata                      8770.          861.               8301.\n# ℹ 6 more variables: `2050_combined_km2` &lt;dbl&gt;, `2050_consensus_km2` &lt;dbl&gt;,\n#   `2070_combined_km2` &lt;dbl&gt;, `2070_consensus_km2` &lt;dbl&gt;,\n#   alltime_consensus_km2 &lt;dbl&gt;, past_stable_km2 &lt;dbl&gt;\n\n\nWe can perform a lot of summary statistics in R. Some of these we can view for multiple columns at once using summary.\n\nsummary(ranges)\n\n   species          combined_current_km2 consensus_km2     bioclim_current_km2\n Length:12          Min.   :  8770       Min.   :  861.3   Min.   :  3749     \n Class :character   1st Qu.: 24800       1st Qu.: 4186.2   1st Qu.: 10924     \n Mode  :character   Median : 43654       Median : 7778.1   Median : 31455     \n                    Mean   : 68052       Mean   :18161.8   Mean   : 42457     \n                    3rd Qu.: 70798       3rd Qu.:18558.7   3rd Qu.: 62835     \n                    Max.   :232377       Max.   :79306.6   Max.   :148753     \n 2050_combined_km2 2050_consensus_km2 2070_combined_km2  2070_consensus_km2\n Min.   :  1832    Min.   :    0.0    Min.   :   550.3   Min.   :    0.0   \n 1st Qu.:  6562    1st Qu.:  589.5    1st Qu.:  6583.8   1st Qu.:  311.4   \n Median : 26057    Median : 6821.9    Median : 24281.7   Median : 2714.6   \n Mean   : 33247    Mean   :14418.4    Mean   : 31811.0   Mean   : 8250.5   \n 3rd Qu.: 40460    3rd Qu.:18577.1    3rd Qu.: 38468.9   3rd Qu.:10034.4   \n Max.   :132487    Max.   :79236.2    Max.   :129591.0   Max.   :53291.8   \n alltime_consensus_km2 past_stable_km2 \n Min.   :    0.0       Min.   :   0.0  \n 1st Qu.:  790.9       1st Qu.:   0.0  \n Median : 8216.8       Median :   0.0  \n Mean   :15723.3       Mean   : 127.3  \n 3rd Qu.:19675.0       3rd Qu.:   0.0  \n Max.   :82310.5       Max.   :1434.8  \n\n\nAs seen above, we now have information for the following statistics for each variable:\n\nMin = minimum\n1st Qu. = 1st quartile\nMedian = middle of the dataset\nMean = average of the dataset\n3rd Qu. = 3rd quartile\nMax. = maximum\n\nWe can also calculate some of these statistics manually to see if we are doing everything correctly. It is easiest to do this by using predefined functions in R (code others have written to perform a particular task) or to create our own functions in R. We will do both to determine the average of combined_current_km2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#subsetting-data",
    "href": "intro_to_r.html#subsetting-data",
    "title": "1  Intro to R",
    "section": "4.2 Subsetting data",
    "text": "4.2 Subsetting data\nFirst, we need to select only the column of interest. In R, we have two ways of subsetting data to get a particular column.\n\nvar[rows,cols] is a way to look at a particular object (var in this case) and choose a specific combination of row number and column number (col). This is great if you know a specific index, but it is better to use a specific name.\nvar[rows,\"cols\"] is a way to do the above but by using a specific column name, like combined_current_km2.\nvar$colname is a way to call the specific column name directly from the dataset.\n\n\n# using R functions\n\nranges$combined_current_km2\n\n [1]  25209.4  68171.2  60939.2  27021.3  78679.9   8769.9 232377.2  17401.4\n [9]  51853.5  35455.1  23570.3 187179.1\n\n\nAs shown above, calling the specific column name with $ allows us to see only the data of interest. We can also save these data as an object.\n\ncurrent_combined &lt;- ranges$combined_current_km2\n\ncurrent_combined\n\n [1]  25209.4  68171.2  60939.2  27021.3  78679.9   8769.9 232377.2  17401.4\n [9]  51853.5  35455.1  23570.3 187179.1\n\n\nNow that we have it as an object, specifically a numeric vector, we can perform whatever math operations we need to on the dataset.\n\nmean(current_combined)\n\n[1] 68052.29\n\n\nHere, we can see the mean for the entire dataset. However, we should always round values to the same number of decimal points as the original data. We can do this with round.\n\nround(mean(current_combined),1) # round mean to one decimal\n\n[1] 68052.3\n\n\nNote that the above has a nested set of commands. We can write this exact same thing as follows:\n\n# pipe mean through round\nmean(current_combined) %&gt;%\n  round(1)\n\n[1] 68052.3\n\n\nUse the method that is easiest for you to follow!\nWe can also calculate the mean manually. The mean is \\(\\frac{\\sum_{i=1}^nx}{n}\\), or the sum of all the values within a vector divided by the number of values in that vector.\n\n# create function\n# use curly brackets to denote function\n# our data goes in place of \"x\" when finally run\nour_mean &lt;- function(x){\n  sum_x &lt;- sum(x) # sum all values in vector\n  n &lt;- length(x) # get length of vector\n  xbar &lt;- sum_x/n # calcualte mean\n  return(xbar) # return the value outside the function\n}\n\nLet’s try it.\n\nour_mean(ranges$combined_current_km2)\n\n[1] 68052.29\n\n\nAs we can see, it works just the same as mean! We can round this as well.\n\nour_mean(ranges$combined_current_km2) %&gt;%\n  round(1)\n\n[1] 68052.3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "2.1 Purposes of descriptive statistics\nDescriptive statistics enable researchers to quickly and easily examine the “behavior” of their datasets, identifying potential errors and allowing them to observe particular trends that may be worth further analysis. Here, we will cover how to calculate descriptive statistics for multiple different datasets, culminating in an assignment covering these topics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#preparing-r",
    "href": "descriptive_stats.html#preparing-r",
    "title": "2  Descriptive Statistics",
    "section": "2.2 Preparing R",
    "text": "2.2 Preparing R\nAs with every week, we will need to load our relevant packages first. This week, we are using the following:\n\n# allows for internet downloading\nlibrary(curl)\n\nUsing libcurl 8.7.1 with LibreSSL/3.3.6\n\n# enables data management tools\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::parse_date() masks curl::parse_date()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#downloading-the-data",
    "href": "descriptive_stats.html#downloading-the-data",
    "title": "2  Descriptive Statistics",
    "section": "2.3 Downloading the data",
    "text": "2.3 Downloading the data\nFor the example this week, we will be using the starbucks dataset, describing the number of drinks purchased during particular time periods during the day.\n\nstarbucks &lt;- curl(\"https://raw.githubusercontent.com/jacobccooper/biol105_unk/main/datasets/starbucks.csv\") %&gt;%\n  read_csv()\n\nRows: 9 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Hour\ndbl (1): Frap_Num\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#descriptive-statistics",
    "href": "descriptive_stats.html#descriptive-statistics",
    "title": "2  Descriptive Statistics",
    "section": "2.4 Descriptive statistics",
    "text": "2.4 Descriptive statistics\nDescriptive statistics are statistics that help us understand the shape and nature of the data on hand. These include really common metrics such as mean, median, and mode, as well as more nuanced metrics like quartiles that help us understand if there is any skew in the dataset. (Skew refers to a bias in the data, where more data points lie on one side of the distribution and there is a long tail of data in the other direction).\n\n\n\nExamples of skew compared to a symmetrical, non-skewed distribution. Source: machinelearningparatodos.com\n\n\nNote above that the relative position of the mean, median, and mode can be indicative of skew. Please also note that these values will rarely be exactly equal “in the real world”, and thus you need to weigh differences against the entire dataset when assessing skew. There is a lot of nuance like this in statistics; it is not always an “exact” science, but sometimes involves judgment calls and assessments based on what you observe in the data.\nUsing the starbucks dataset, we can look at some of these descriptive statistics to understand what is going on.\n\n2.4.1 Notation\nAs a quick reminder, we use Greek lettering for populations and Roman lettering for samples. For example:\n\n\\(\\sigma\\) is a population, but \\(s\\) is a sample (both these variables refer to standard deviation).\n\\(\\mu\\) is a population, but \\(\\bar{x}\\) is a sample (both of these variables refer to the mean).\n\n\n\n2.4.2 Mean\nThe mean is the “average” value within a set of data, specifically, the sum of all values divided by the length of those values: \\(\\frac{\\sum_{i=1}^nx}{n}\\).\n\nhead(starbucks)\n\n# A tibble: 6 × 2\n  Hour      Frap_Num\n  &lt;chr&gt;        &lt;dbl&gt;\n1 0600-0659        2\n2 0700-0759        3\n3 0800-0859        2\n4 0900-0959        4\n5 1000-1059        8\n6 1100-1159        7\n\n\nHere, we are specifically interested in the number of frappuccinos.\n\n# get vector of frappuccino number\nfraps &lt;- starbucks$Frap_Num\n\n# get mean of vector\nmean(fraps)\n\n[1] 6.222222\n\n\nNote that the above should be rounded to a whole number, since we were given the data in whole numbers!\n\nmean(fraps) %&gt;%\n  round(0)\n\n[1] 6\n\n# OR\n\nround(mean(fraps),0)\n\n[1] 6\n\n\nWe already covered calculating the average manually in our previous tutorial, but we can do that here as well:\n\n# sum values\n# divide by n, length of vector\n# round to 0 places\nround(sum(fraps)/length(fraps),0)\n\n[1] 6\n\n\n\n\n2.4.3 Range\nThe range is the difference between the largest and smallest units in a dataset. We can use the commands min and max to calculate this.\n\nmax(fraps) - min(fraps)\n\n[1] 13\n\n\nThe range of our dataset is 13.\n\n\n2.4.4 Median\nThe median is also known as the 50th percentile, and is the midpoint of the data when ordered from least to greatest. If there are an even number of data points, then it is the average point between the two center points. For odd data, this is the \\(\\frac{n+1}{2}\\)th observation. For even data, since we need to take an average, this is the \\(\\frac{\\frac{n}{2}+(\\frac{n}{2}+1)}{2}\\). You should be able to do these by hand and by using a program.\n\nmedian(fraps)\n\n[1] 4\n\n\nNow, to calculate by hand:\n\nlength(fraps)\n\n[1] 9\n\n\nWe have an odd length.\n\n# order gets the order\norder(fraps)\n\n[1] 1 3 7 2 4 6 5 9 8\n\n\n\n# [] tells R which elements to put where\nfrap_order &lt;- fraps[order(fraps)]\n\nfrap_order\n\n[1]  2  2  2  3  4  7  8 13 15\n\n\n\n# always use parentheses\n# make sure the math maths right!\n(length(frap_order)+1)/2\n\n[1] 5\n\n\nWhich is the fifth element in the vector?\n\nfrap_order[5]\n\n[1] 4\n\n\nNow let’s try it for an even numbers.\n\n# remove first element\neven_fraps &lt;- fraps[-1]\n\neven_fraps_order &lt;- even_fraps[order(even_fraps)]\n\neven_fraps_order\n\n[1]  2  2  3  4  7  8 13 15\n\n\n\nmedian(even_fraps)\n\n[1] 5.5\n\n\nNow, by hand: \\(\\frac{\\frac{n}{2}+(\\frac{n}{2}+1)}{2}\\).\n\nn &lt;- length(even_fraps_order)\n\n# get n/2 position from vector\nm1 &lt;- even_fraps_order[n/2]\n# get n/2+1 position\nm2 &lt;- even_fraps_order[(n/2)+1]\n\n# add these values, divide by two for \"midpoint\"\nmed &lt;- (m1+m2)/2\n\nmed\n\n[1] 5.5\n\n\nAs we can see, these values are equal!\n\n\n2.4.5 Other quartiles and quantiles\nWe also use the 25th percentile and the 75th percentile to understand data distributions. These are calculated similar to the above, but the bottom quartile is only \\(\\frac{1}{4}\\) of the way between values and the 75th quartile is \\(\\frac{3}{4}\\) of the way between values. We can use the R function quantile to calculate these.\n\nquantile(frap_order)\n\n  0%  25%  50%  75% 100% \n   2    2    4    8   15 \n\n\nWe can specify a quantile as well:\n\nquantile(frap_order, 0.75)\n\n75% \n  8 \n\n\nWe can also calculate these metrics by hand. Let’s do it for the even dataset, since this is more difficult.\n\nquantile(even_fraps_order)\n\n   0%   25%   50%   75%  100% \n 2.00  2.75  5.50  9.25 15.00 \n\n\nNote that the 25th and 75th percentiles are also between two different values. These can be calculated as a quarter and three-quarters of the way between their respective values.\n\n# 75th percentile\n\nn &lt;- length(even_fraps_order)\n\n# get position\np &lt;- 0.75*(n+1)\n\n# get lower value\n# round down\nm1 &lt;- even_fraps_order[trunc(p)]\n\n# get upper value\n# round up\nm2 &lt;- even_fraps_order[ceiling(p)]\n\n# position between\n# fractional portion of rank\nfrac &lt;- p-trunc(p)\n\n# calculate the offset from lowest value\nval &lt;- (m2 - m1)*frac\n\n# get value\nm1 + val\n\n[1] 11.75\n\n\nWait… why does our value differ?\nR, by default, calculates quantiles using what is called Type 7, in which the quantiles are calculated by \\(p_k = \\frac{k-1}{n-1}\\), where \\(n\\) is the length of the vector and \\(k\\) refers to the quantile being used. However, in our book and in this class, we use Type 6 interpretation - \\(p_k = \\frac{k}{n + 1}\\). Let’s try using Type 6:\n\nquantile(even_fraps_order, type = 6)\n\n   0%   25%   50%   75%  100% \n 2.00  2.25  5.50 11.75 15.00 \n\n\nNow we have the same answer as we calculated by hand!\nThis is a classic example of how things in R (and in statistics in general!) can depend on interpretation and are not always “hard and fast” rules.\nIn this class, we will be using Type 6 interpretation for the quantiles - you will have to specify this in the quantile function EVERY TIME! If you do not specify Type 6, you will get the questions incorrect and you will get answers that do not agree with the book, with Excel, or what you calculate by hand.\n\n\n2.4.6 Mode\nThere is no default method for finding the mode in R. However, websites like Statology provide wraparound functions.\n\n# Statology function\n# define function to calculate mode\nfind_mode &lt;- function(x) {\n  # get unique values from vector\n  u &lt;- unique(x)\n  # count number of occurrences for each value\n  tab &lt;- tabulate(match(x, u))\n  # return the value with the highest count\n  u[tab == max(tab)]\n}\n\nfind_mode(fraps)\n\n[1] 2\n\n\nWe can also do this by hand, by counting the number of occurrences of each value. This can be done in a stepwise fashion using commands in the above function.\n\n# unique counts\nu &lt;- unique(fraps)\nu\n\n[1]  2  3  4  8  7 15 13\n\n\n\n# which elements match\nmatch(fraps,u)\n\n[1] 1 2 1 3 4 5 1 6 7\n\n\n\n# count them\ntab &lt;- match(fraps,u) %&gt;%\n  tabulate()\n\ntab\n\n[1] 3 1 1 1 1 1 1\n\n\nGet the highest value.\n\nu[tab==max(tab)]\n\n[1] 2\n\n\nNotice this uses ==. This is a logical argument that means “is equal to” or “is the same as”. For example:\n\n2 == 2\n\n[1] TRUE\n\n\nThese values are the same, so TRUE is returned.\n\n2 == 3\n\n[1] FALSE\n\n\nThese values are unequal, so FALSE is returned. R will read TRUE as 1 and FALSE as ZERO, such that:\n\nsum(2==2)\n\n[1] 1\n\n\nand\n\nsum(2==3)\n\n[1] 0\n\n\nThis allows you to find how many arguments match your condition quickly, and even allows you to subset based on these indices as well. Keep in mind you can use greater than &lt;, less than &gt;, greater than or equal to &lt;=, less than or equal to &gt;=, is equal to ==, and is not equal to != to identify numerical relationships. Other logical arguments include:\n\n&: both conditions must be TRUE to match (e.g., c(10,20) & c(20,10)). Try the following as well: fraps &lt; 10 & fraps &gt; 3.\n&&: and, but works with single elements and allows for better parsing. Often used with if. E.g., fraps &lt; 10 && fraps &gt; 3. This will not work on our multi-element frap vector.\n|: or, saying at least one condition must be true. Try: fraps &gt; 10 | fraps &lt; 3.\n||: or, but for a single element, like && above.\n!: not, so “not equal to” would be !=.\n\n\n\n2.4.7 Variance\nWhen we are dealing with datasets, the variance is a measure of the total spread of the data. The variance is calculated using the following:\\[\\sigma^2=\\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\]\nEssentially, this means that for every value of \\(x\\), we are finding the difference between that value and the mean and squaring it, summing all of these quared differences, and dividing them by the number of samples in the dataset minus one. Let’s do this for the frappuccino dataset.\n\nfrap_order\n\n[1]  2  2  2  3  4  7  8 13 15\n\n\nNow to find the differences.\n\ndiffs &lt;- frap_order - mean(frap_order)\n\ndiffs\n\n[1] -4.2222222 -4.2222222 -4.2222222 -3.2222222 -2.2222222  0.7777778  1.7777778\n[8]  6.7777778  8.7777778\n\n\nNote that R is calculating the same thing for the entire vector! Since these are differences from the mean, they should sum to zero.\n\nsum(diffs)\n\n[1] 3.552714e-15\n\n\nThis is not quite zero due to rounding error, but is essentially zero as it is 0.0000000000000036.\n\n# square differences\ndiffs_sq &lt;- diffs^2\n\ndiffs_sq\n\n[1] 17.8271605 17.8271605 17.8271605 10.3827160  4.9382716  0.6049383  3.1604938\n[8] 45.9382716 77.0493827\n\n\nNow we have the squared differences. We need to sum these and divide by \\(n-1\\).\n\nn &lt;- length(frap_order)\n\nvar_frap &lt;- sum(diffs_sq)/(n-1)\n\nvar_frap\n\n[1] 24.44444\n\n\nLet’s check this against the built-in variance function in R.\n\nvar(frap_order)\n\n[1] 24.44444\n\n\nThey are identical! We can check this using a logical argument.\n\nvar_frap == var(frap_order)\n\n[1] TRUE\n\n\nSeeing as this is TRUE, we calculated it correctly.\n\n\n2.4.8 Standard deviation\nAnother common measurement of spread is the standard deviation (\\(\\sigma\\)). As you remember from class (or may have guessed from the notation on this site), the standard deviation is just the square root of the variance.\n\nsqrt(var_frap)\n\n[1] 4.944132\n\n\nWe can test this against the built in sd function in R:\n\nsqrt(var_frap) == sd(frap_order)\n\n[1] TRUE\n\n\nAs you can see, we calculated this correctly!\n\n\n2.4.9 Standard error\nThe standard error is used to help understand the spread of data and to help estimate the accuracy of our measurements for things like the mean. The standard error is calculated thusly:\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nThere is not built in function for the standard error in excel, but we can write our own:\n\nse &lt;- function(x){\n  n &lt;- length(x) # calculate n\n  s &lt;- sd(x) # calculate standard deviation\n  se_val &lt;- s/sqrt(n)\n  return(se_val)\n}\n\nLet’s test this code.\n\nse(frap_order)\n\n[1] 1.648044\n\n\nOur code works! And we can see exactly how the standard error is calculate. We can also adjust this code as needed for different situations, like samples.\nRemember, the standard error is used to help reflect our confidence in a specific measurement (e.g., how certain we are of the mean, and what values we believe the mean falls between). We want our estimates to be as precise as possible with as little uncertainty as possible. Given this, does having more samples make our estimates more or less confident? Mathematically, what happens as our sample size increases?\n\n\n2.4.10 Coefficient of variation\nThe coefficient of variation, another measure of data spread and location, is calculated by the following:\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\]\nWe can write a function to calculate this in R as well.\n\ncv &lt;- function(x){\n  sigma &lt;- sd(x)\n  mu &lt;- mean(x)\n  val &lt;- sigma/mu\n  return(val)\n}\n\ncv(frap_order)\n\n[1] 0.7945927\n\n\nRemember that we will need to round values.\n\n\n2.4.11 Outliers\nOutliers are any values that are outside of the 1.5 times the interquartile range. We can calculate this for our example dataset as follows:\n\nlowquant &lt;- quantile(frap_order,0.25,type = 6) %&gt;% as.numeric()\n\nhiquant &lt;- quantile(frap_order,0.75,type = 6) %&gt;% as.numeric()\n\niqr &lt;- hiquant - lowquant\n\nWe can also calculate the interquartile range using IQR. Remember, you must use type = 6!\n\niqr &lt;- IQR(frap_order, type = 6)\n\nNow, to calculate the “whiskers”.\n\nlowbound &lt;- lowquant - (1.5*iqr)\nhibound &lt;- hiquant + (1.5*iqr)\n\n# low outliers?\n# select elements that match\n# identify using logical \"which\"\nfrap_order[which(frap_order &lt; lowbound)]\n\nnumeric(0)\n\n\n\n# high outliers?\n# select elements that match\n# identify using logical \"which\"\nfrap_order[which(frap_order &gt; hibound)]\n\nnumeric(0)\n\n\nWe have no outliers for this particular dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#homework-descriptive-statistics",
    "href": "descriptive_stats.html#homework-descriptive-statistics",
    "title": "2  Descriptive Statistics",
    "section": "2.5 Homework: Descriptive Statistics",
    "text": "2.5 Homework: Descriptive Statistics\nNow that we’ve covered these basic statistics, it’s your turn! For this week, you will be completing homework that involves methods from Chapter 4 in your book.\n\n2.5.1 Homework instructions\nPlease create an RMarkdown document that will render as an .html file. You will submit this file to show your coding and your work. Please refer to the Introduction to R for refreshers on how to create an .html document in RMarkdown. You will need to do the following for each of these datasets:\n\nmean\nmedian\nrange\ninterquartile range\nvariance\nstandard deviation\ncoefficient of variation\nstandard error\nwhether there are any “outliers”\n\nPlease show all of your work for full credit.\n\n\n2.5.2 Data for homework problems\nFor each question, calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers”.\nPlease also write your own short response to the Synthesis question posed, which will involve thinking about the data and metrics you just analyzed.\n\n2.5.2.1 1: UNK Nebraskans\nEver year, the university keeps track of where students are from. The following are data on the umber of students admitted to UNK from the state of Nebraska:\n\n# create dataset in R\nnebraskans &lt;- c(5056,5061,5276,5244,5209,\n                5262,5466,5606,5508,5540,5614)\n\nyears &lt;- 2023:2013\n\nnebraskans_years &lt;- cbind(years,nebraskans) %&gt;% \n  as.data.frame()\n\nnebraskans_years\n\n   years nebraskans\n1   2023       5056\n2   2022       5061\n3   2021       5276\n4   2020       5244\n5   2019       5209\n6   2018       5262\n7   2017       5466\n8   2016       5606\n9   2015       5508\n10  2014       5540\n11  2013       5614\n\n\nUsing these data, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers” for the number of UNK students from Nebraska.\nIn order to do this, we will need to first select the column that denotes the number of Nebraskans from the dataset. Remember, we need to save this as an object in R to do the analyses. Here is a demonstration of getting the column to look at the mean, so that you can repeat this for the other questions. This relies heavily on the use of $, used to get the data from a specific column.\n\n# $ method\nnebraskans &lt;- nebraskans_years$nebraskans\n\nnebraskans\n\n [1] 5056 5061 5276 5244 5209 5262 5466 5606 5508 5540 5614\n\n\nNow we can get the mean of this vector.\n\nmean(nebraskans) %&gt;%\n  round(0) # don't forget to round!\n\n[1] 5349\n\n\nSynthesis question: Do you think that there are any really anomalous years? Do you feel data are similar between years? Note we are not looking at trends through time but whether any years are outliers.\n\n\n2.5.2.2 2: Piracy in the Gulf of Guinea\nThe following is a dataset looking at oceanic conditions and other variables associated with pirate attacks within the region between 2010 and 2021 (Moura et al. 2023). Using these data, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers” for distance from shore for each pirate attack (column Distance_from_Coast).\n\nurl_file &lt;- curl(\"https://figshare.com/ndownloader/files/42314604\")\npirates &lt;- url_file %&gt;% read_csv()\n\nNew names:\nRows: 595 Columns: 40\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(18): Period, Season, African_Season, Coastal_State, Coastal_Zone, Navi... dbl\n(20): ...1, Unnamed: 0, Year, Month, Lat_D, Lon_D, Distance_from_Coast,... dttm\n(2): Date_Time, Date\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nSynthesis question: Do you notice any patterns in distance from shore? What may be responsible for these patterns? Hint: Think about what piracy entails and also what other columns are available as other variables in the above dataset.\n\n\n2.5.2.3 3: Patient ages at presentation\nThe following is a dataset on skin sores in different communities in Australia and Oceania, specifically looking at the amount of time that passes between skin infections (Lydeamore et al. 2020a). This file includes multiple different datasets, and focuses on data from children in the first five years of their life, on househould visits, and on data collected during targeted studies (Lydeamore et al. 2020b).\n\nurl_file &lt;- curl(\"https://doi.org/10.1371/journal.pcbi.1007838.s006\")\n\nages &lt;- url_file %&gt;% read_csv()\n\nRows: 17150 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): dataset\ndbl (1): time_difference\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s see what this file is like real fast. We can use the command dim to see the rows and columns.\n\ndim(ages)\n\n[1] 17150     2\n\n\nAs you can see, this file has only two columns but 17,150 rows! For the column time_difference, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers”.\nSynthesis question: Folks will often think about probabilities of events being “low but never zero”. What does that mean in the context of these data? What about these data make you feel like probabilities may decrease through time but never become zero?\n\n\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W. Cuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon, S. Y. C. Tong, and J. M. McCaw (2020b). Patient ages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W. Cuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon, S. Y. C. Tong, and J. M. McCaw (2020a). Estimation of the force of infection and infectious period of skin sores in remote Australian communities using interval-censored data. PLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset. https://doi.org/10.6084/m9.figshare.24119643.v1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "visual.html",
    "href": "visual.html",
    "title": "3  Diagnosing data visually",
    "section": "",
    "text": "3.1 The importance of visual inspection\nInspecting data visually can give us a lot of information about whether data are normally distributed and about whether there are any major errors or issues with our dataset. It can also help us determine if data meet model assumptions, or if we need to use different tests more appropriate for our datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#sample-data-and-preparation",
    "href": "visual.html#sample-data-and-preparation",
    "title": "3  Diagnosing data visually",
    "section": "3.2 Sample data and preparation",
    "text": "3.2 Sample data and preparation\nFirst, we need to load our R libraries.\n\nlibrary(curl)\n\nUsing libcurl 8.7.1 with LibreSSL/3.3.6\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::parse_date() masks curl::parse_date()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNext, we can download our data sample.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#histograms",
    "href": "visual.html#histograms",
    "title": "3  Diagnosing data visually",
    "section": "3.3 Histograms",
    "text": "3.3 Histograms\nA histogram is a frequency diagram that we can use to visually diagnose data and their distributions. We are going to examine a histogram using a random string of data. R can generate random (though, actually pseudorandom) strings of data on command, pulling them from different distributions. These distributions are pseudorandom because we can’t actually program R to be random, so it starts from a wide variety of pseudorandom points.\n\n3.3.1 Histograms on numeric vectors\n\n# create random string from normal distribution\n# this step is not necessary for data analysis in homework\nx &lt;- rnorm(n = 1000, # 1000 values\n           mean = 0,\n           sd = 1)\n\n# make histogram\nhist(x)\n\n\n\n\n\n\n\n\nNOTE that a histogram can only be made on a vector of values. If you try to make a histogram on a data frame, you will get an error and it will not work. You have to specify which column you wish to use with the $ operator. (For example, for dataframe xy with columns x and y, you would use hist(xy$y)).\nWe can up the number of bins to see this better.\n\nhist(x,breaks = 100)\n\n\n\n\n\n\n\n\nThe number of bins can be somewhat arbitrary, but a value should be chosen based off of what illustrates the data well. R will auto-select a number of bins in some cases, but you can also select a number of bins. Some assignments will ask you to choose a specific number of bins as well.\n\n\n3.3.2 Histograms on frequency counts\nSay, for example, that we have a dataset where everything is already shown as frequencies. We can create a frequency histogram using barplot.\n\ncount_table &lt;- matrix(nrow = 4, ncol = 2, byrow = T,\n                      data = c(\"Cat 1\", 4,\n                               \"Cat 2\", 8,\n                               \"Cat 3\", 7,\n                               \"Cat 4\", 3)) %&gt;%\n  as.data.frame()\n\ncolnames(count_table) &lt;- c(\"Category\",\"Count\")\n\n# ensure counts are numeric data\ncount_table$Count &lt;- as.numeric(count_table$Count)\n\n# manually create histogram\nbarplot(count_table$Count, # response variable, counts for histogram\n        axisnames = T, # make names on plot\n        names.arg = count_table$Category) # make these the names\n\n\n\n\n\n\n\n\n\n\n3.3.3 ggplot histograms\nThe following is an optional workthrough on how to make really fancy plots.\nWe can also use the program ggplot, part of the tidyverse, to create histograms.\n\n# ggplot requires data frames\nx2 &lt;- x %&gt;% as.data.frame()\ncolnames(x2) &lt;- \"x\"\n\nggplot(data = x2, aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot is nice because we can also clean up this graph a little.\n\nggplot(x2,aes(x=x)) + geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can also do a histogram of multiple values at once in R.\n\nx2$cat &lt;- \"x\"\n\ny &lt;- rnorm(n = 1000,\n           mean = 1,\n           sd = 1) %&gt;%\n  as.data.frame()\n\ncolnames(y) &lt;- \"x\"\ny$cat &lt;- \"y\"\n\nxy &lt;- rbind(x2,y)\n\nhead(xy)\n\n          x cat\n1 -1.489024   x\n2 -1.034706   x\n3 -1.062882   x\n4  1.378976   x\n5 -1.397002   x\n6  2.122253   x\n\n\n\nggplot(xy,aes(x = x, fill = cat)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can also make this look a little nicer.\n\nggplot(xy, aes(x = x, colour = cat)) +\n  geom_histogram(fill = \"white\", alpha = 0.5, # transparency\n                 position = \"identity\") +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can show these a little differently as well.\n\nggplot(xy, aes(x = x, fill = cat))+\n  geom_histogram(position = \"identity\", alpha = 0.5) +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThere are lots of other commands you can incorporate as well if you so choose; I recommend checking sites like this one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#skewness",
    "href": "visual.html#skewness",
    "title": "3  Diagnosing data visually",
    "section": "3.4 Skewness",
    "text": "3.4 Skewness\nSkew is a measure of how much a dataset “leans” to the positive or negative directions (i.e., to the “left” or to the “right”). To calculate skew, we are going to use the moments library.\n\n# don't forget to install if needed!\nlibrary(moments)\n\nskewness(x)\n\n[1] 0.08467474\n\n\nGenerally, a value between \\(-1\\) and \\(+1\\) for skewness is “acceptable” and not considered overly skewed. Positive values indicate “right” skew and negative values indicate a “left” skew. If something is too skewed, it may violate assumptions of normality and thus need non-parametric tests rather than our standard parametric tests - something we will cover later!\nLet’s look at a skewed dataset. We are going to artificially create a skewed dataset from our x vector.\n\n# create more positive values\nx3 &lt;- c(x,\n        x[which(x &gt; 0)]*2,\n        x[which(x &gt; 0)]*4,\n        x[which(x &gt; 0)]*8)\n\nhist(x3)\n\n\n\n\n\n\n\n\n\nskewness(x3)\n\n[1] 2.205181\n\n\nAs we can see, the above is a heavily skewed dataset with a positive (“right”) skew.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#kurtosis",
    "href": "visual.html#kurtosis",
    "title": "3  Diagnosing data visually",
    "section": "3.5 Kurtosis",
    "text": "3.5 Kurtosis\nKurtosis refers to how sharp or shallow the peak of the distribution is (platykurtic vs. leptokurtic). Remember - platykyrtic are plateaukurtic, wide and broad like a plateau, and leptokurtic distributions are sharp. Intermediate distributions that are roughly normal are mesokurtic.\nMuch like skewness, kurtosis values of \\(&gt; 2\\) and \\(&lt; -2\\) are generally considered extreme, and thus not mesokurtic. This threshold can vary a bit based on source, but for this class, we will use a threshold of \\(\\pm 2\\) for both skewness and kurtosis.\nLet’s see the kurtosis of x. Note that when doing the equation, a normal distribution actually has a kurtosis of \\(3\\); thus, we are doing kurtosis \\(-3\\) to “zero” the distribution and make it comparable to skewness.\n\nhist(x)\n\n\n\n\n\n\n\n\n\n# non-zeroed\nkurtosis(x)\n\n[1] 2.973711\n\n\n\n# zeroed\nkurtosis(x)-3\n\n[1] -0.0262893\n\n\nAs expected, out values drawn from a normal distribution are not overly skewed. Let’s compare these to a more kurtic distribution:\n\nxk &lt;- x^3\n\nkurtosis(xk)-3\n\n[1] 35.51053\n\n\nWhat does this dataset look like?\n\nhist(xk,breaks = 100)\n\n\n\n\n\n\n\n\nAs we can see, this is a very leptokurtic distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#cumulative-frequency-plot",
    "href": "visual.html#cumulative-frequency-plot",
    "title": "3  Diagnosing data visually",
    "section": "3.6 Cumulative frequency plot",
    "text": "3.6 Cumulative frequency plot\nA cumulative frequency plot shows the overall spread of the data as a cumulative line over the entire dataset. This is another way to see the spread of the data and is often complementary to a histogram.\nThe following cumulative distribution plot is based on the method outlined in Geeks for Geeks.\n\n3.6.1 If data are not in histogram/frequency format\nYou will need to create a frequency table to make them be in histogram format.\n\n# declaring the break points\n# make break points based on data\n# always round UP with ceiling\nrange.x &lt;- ceiling(max(x)-min(x))\n\nrange.x\n\n[1] 7\n\nrange(x)\n\n[1] -3.366277  3.498893\n\n\nBased on this range, we need to create our bin sizes. We want our lowest bin to be below our lowest value, and our highest bin above our highest value. Here, I’m setting a step size of 1. You will have to examine your data and determine the best break size for your datasets.\n\n# make bins based on range\n# create sequential series\nbreak_points &lt;- seq(-3, # starting value, low\n                    4, # end value, high\n                    1) # increment size\n\n\n# transforming the data\ndata_transform = cut(x, # your data!\n                     break_points, # the breaks you defined\n                    right=FALSE) # closed on the left for intervals\n# creating the frequency table\nfreq_table = table(data_transform) # create a table\n\n# printing the frequency table\nprint(\"Frequency Table\")\n\n[1] \"Frequency Table\"\n\nprint(freq_table)\n\ndata_transform\n[-3,-2) [-2,-1)  [-1,0)   [0,1)   [1,2)   [2,3)   [3,4) \n     18     149     342     327     137      22       3 \n\n\n\n# calculating cumulative frequency\ncumulative_freq = c(0, # start at 0, no points\n                    cumsum(freq_table)) # get the cumulative frequency!\nprint(\"Cumulative Frequency\")\n\n[1] \"Cumulative Frequency\"\n\nprint(cumulative_freq)\n\n        [-3,-2) [-2,-1)  [-1,0)   [0,1)   [1,2)   [2,3)   [3,4) \n      0      18     167     509     836     973     995     998 \n\n\n\n# plotting the data\nplot(break_points, # x axis is break_points\n     cumulative_freq, # y axis is cumulative frequency\n    xlab=\"Data Distribution\", # x axis label\n    ylab=\"Cumulative Frequency\") # y axis label\n# creating line graph\nlines(break_points, # add lines to the graph, this is x\n      cumulative_freq) # this is y for lines\n\n\n\n\n\n\n\n\n\n\n3.6.2 If data are in histogram/frequency format\nIf you have a list of frequencies (say, for river discharge over several years), you only need to do the cumsum function. For example:\n\ny &lt;- c(1 ,2 ,4, 8, 16, 8, 4, 2, 1)\n\nsum_y &lt;- cumsum(y)\n\nprint(y)\n\n[1]  1  2  4  8 16  8  4  2  1\n\nprint(sum_y)\n\n[1]  1  3  7 15 31 39 43 45 46\n\n\nNow we can see we have out cumulative sums. Let’s plot these. NOTE that this method will not have the x variables match the dataset you started with, it will only plot the curve based on the number of values given.\n\nplot(x = 1:length(sum_y), # get length of sum_y, make x index\n     y = sum_y, # plot cumulative sums\n     type = \"l\") # make a line plot",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#homework-chapter-3",
    "href": "visual.html#homework-chapter-3",
    "title": "3  Diagnosing data visually",
    "section": "3.7 Homework: Chapter 3",
    "text": "3.7 Homework: Chapter 3\nFrom your book, complete problems 3.1, 3.4 & 3.5. Data for these problems are available on Canvas and in your book.\n\n3.7.1 Helpful hint\nHINT: For 3.5, consider just making a vector of the values of interest for a histogram.\nFor example, see the following. For reference:\n\nc means “concatenate”, or place things together in an object.\n\n\n# numeric vector data for counts\ny &lt;- c(17,24,16)\n\n# manually create a histogram using barplot\nbarplot(y, \n        # axis names must be true\n        axisnames = T, \n        # input names here\n        # each category as a separate quoted character string\n        names.arg = c(\"Cat 1\", \"Cat 2\", \"Cat 3\"))\n\n\n\n\n\n\n\n\n\n\n3.7.2 Directions\nPlease complete all computer portions in an rmarkdown document knitted as an html. Upload any “by hand” calculations as images in the HTML or separately on Canvas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#addendum",
    "href": "visual.html#addendum",
    "title": "3  Diagnosing data visually",
    "section": "3.8 Addendum",
    "text": "3.8 Addendum\nWith thanks to Hernan Vargas & Riley Grieser for help in formatting this page. Additional comments provided by BIOL 305 classes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "normality.html",
    "href": "normality.html",
    "title": "4  Normality & hypothesis testing",
    "section": "",
    "text": "4.1 Normal distributions\nA standard normal distribution is a mathematical model that describes a commonly observed phenomenon in nature. When measuring many different kinds of datasets, the data being measured often becomes something that resembles a standard normal distribution. This distribution is described by the following equation:\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^\\frac{(x-\\mu)^2}{2\\sigma^2}\\]\nThis equation is fairly well defined by the variance (\\(\\sigma^2\\)), the overall spread of the data, and by the standard deviation (\\(\\sigma\\)), which is defined by the square root of the variance.\nStandard normal distributions have a mean, median, and mode that are equal. The standard normal distribution is a density function, and we are interested in the “area under the curve” (AUC) to understand the relative probability of an event occurring. At the mean/median/mode, the probability on either side of the distribution is \\(50\\)%. When looking at a normal distribution distribution, it is impossible to say the probability of a specific event occurring, but it is possible to state the probability of an event as extreme or more extreme than the event observed occurring. This is known as the \\(p\\) value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#normal-distributions",
    "href": "normality.html#normal-distributions",
    "title": "4  Normality & hypothesis testing",
    "section": "",
    "text": "A standard normal distribution, illustrating the percentage of area found within each standard deviation away from the mean. By Ainali on Wikipedia; CC-BY-SA 3.0.\n\n\n\n\n4.1.1 Example in nature\nIn order to see an example of the normal distribution in nature, we are going to examine the BeeWalk survey database from the island of Great Britain (Comont 2020). We are not interested in the bee data at present, however, but in the climatic data from when the surveys were performed.\n\nbeewalk &lt;- curl(\"https://figshare.com/ndownloader/files/44726902\") %&gt;%\n  read_csv()\n\nRows: 306550 Columns: 49\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): Website.ID, Website.RecordKey, SiteName, Site.section, ViceCounty,...\ndbl (19): RecordKey, established, Precision, Transect.lat, Transect.long, tr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that this is another massive dataset - \\(306,550\\) rows of data!\nThe dataset has the following columns:\n\ncolnames(beewalk)\n\n [1] \"RecordKey\"            \"Website.ID\"           \"Website.RecordKey\"   \n [4] \"SiteName\"             \"Site.section\"         \"ViceCounty\"          \n [7] \"established\"          \"GridReference\"        \"Projection\"          \n[10] \"Precision\"            \"Transect.lat\"         \"Transect.long\"       \n[13] \"transect.OS1936.lat\"  \"Transect.OS1936.long\" \"transect_length\"     \n[16] \"section_length\"       \"section_grid_ref\"     \"H1\"                  \n[19] \"H2\"                   \"H3\"                   \"H4\"                  \n[22] \"habitat_description\"  \"L1\"                   \"L2\"                  \n[25] \"land_use_description\" \"start_time\"           \"end_time\"            \n[28] \"sunshine\"             \"wind_speed\"           \"temperature\"         \n[31] \"TaxonVersionKey\"      \"species\"              \"latin\"               \n[34] \"queens\"               \"workers\"              \"males\"               \n[37] \"unknown\"              \"Comment\"              \"transect_comment\"    \n[40] \"flower_visited\"       \"StartDate\"            \"EndDate\"             \n[43] \"DateType\"             \"Year\"                 \"Month\"               \n[46] \"Day\"                  \"Sensitive\"            \"Week\"                \n[49] \"TotalCount\"          \n\n\nWe are specifically interested in temperature to determine weather conditions. Let’s see what the mean of this variable is.\n\nmean(beewalk$temperature)\n\n[1] NA\n\n\nHmmm… we are getting an NA value, indicating that not every cell has data recorded. Let’s view summary.\n\nsummary(beewalk$temperature)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   16.00   19.00   18.65   21.00   35.00   16151 \n\n\nAs we can see, \\(16,151\\) rows do not have temperature recorded! We want to remove these NA rows, which we can do by using using na.omit.\n\nbeewalk$temperature %&gt;%\n  na.omit() %&gt;%\n  mean() %&gt;%\n  round(2) # don't forget to round!\n\n[1] 18.65\n\n\nNow we can record the mean.\nLet’s visualize these data using a histogram. Note I do not use na.omit as the hist function automatically performs this data-cleaning step!\n\nhist(beewalk$temperature,breaks = 5)\n\n\n\n\n\n\n\n\nEven with only five breaks, we can see an interesting, normal-esque distribution in the data. Let’s refine the bin number.\n\nhist(beewalk$temperature,breaks = 40)\n\n\n\n\n\n\n\n\nWith forty breaks, the pattern becomes even more clear. Let’s see what a standard normal distribution around these data would look like.\n\n# save temperature vector without NA values\ntemps &lt;- beewalk$temperature %&gt;% na.omit()\n\nmu &lt;- mean(temps)\nt.sd &lt;- sd(temps)\n\n# sample random values\nnormal.temps &lt;- rnorm(length(temps), # sample same size vector\n                      mean = mu,\n                      sd = t.sd)\n\nhist(normal.temps, breaks = 40)\n\n\n\n\n\n\n\n\nAs we can see, our normal approximation of temperatures is not too dissimilar from the distribution of temperatures we actually see!\nLet’s see what kind of data we have for temperatures:\n\n# load moments library\nlibrary(moments)\n\nskewness(temps)\n\n[1] 0.02393257\n\n\nData do not have any significant skew.\n\nkurtosis(temps)-3\n\n[1] 0.3179243\n\n\nData do not show any significant kurtosis.\n\n\n4.1.2 Effect of sampling\nOftentimes, we will see things approach the normal distribution as we collect more samples. We can model this by subsampling our temperature vector.\n\n# make reproducible\nset.seed(1839)\n\nsub.temps &lt;- sample(temps,\n                    size = 10,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"10 samples\")\n\n\n\n\n\n\n\n\nWith only ten values sampled, we do not have much of a normal distribution. Let’s up this to \\(100\\) samples.\n\nsub.temps &lt;- sample(temps,\n                    size = 100,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"100 samples\",breaks = 10)\n\n\n\n\n\n\n\n\nNow we are starting to see more of a normal distribution! Let’s increase this to \\(1000\\) temperatures.\n\nsub.temps &lt;- sample(temps,\n                    size = 1000,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"1000 samples\", breaks = 40)\n\n\n\n\n\n\n\n\nNow the normal distribution is even more clear. As we can also see, the more we sample, the more we approach the true means and distribution of the actual dataset. Because of this, we can perform experiments and observations of small groups and subsamples and make inferences about the whole, given that most systems naturally approach statistical distributions like the normal!\n\n\n4.1.3 Testing if data are normal\nThere are two major methods we can use to see if data are normally distributed.\n\n4.1.3.1 QQ Plots\nAnother way to see if data are normal is to use a QQ plot. These plots data quantiles to theoretical quantiles to see how well they align, with a perfectly normal distribution having a completely linear QQ plot. Let’s look at these with our beewalk data.\n\nqqnorm(temps)\n\n\n\n\n\n\n\n\nAs we can see above, the data are roughly linear, which means are data are very normal. The “stairsteps” are from the accuracy in measuring temperature, which was likely rounded and thus created a distribution that is not completely continuous.\n\n\n4.1.3.2 Shapiro-Wilk test\nAnother way to test for normality is to use a Shapiro-Wilk test of normality. We will not get into the specific of this distribution, but this tests the null hypothesis that data originated in a normal distribution, with the alternative hypothesis that the data originated in a non-normal distribution. You will read next about the specifics of hypothesis testing, but this test uses an \\(\\alpha = 0.05\\), and we reject the null hypothesis if our \\(p &lt; \\alpha\\), with \\(p\\) representing the probability of observing something as extreme or more extreme than the result we observe. Our temps dataset is too large, as shapiro.test requires vectors of \\(&lt; 5000\\). Let’s take a random sample and try again.\n\nsample(temps, 200) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98922, p-value = 0.1371\n\n\nFor our subsets of temperature, we find that the temperature is normally distributed. Note that having all \\(5000\\) temperatures included makes this test find a “non-normal” result, likely from the same stairstepping phenomenon from rounding that we discussed before, which may result in model overfitting to the rounded data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#hypothesis-testing",
    "href": "normality.html#hypothesis-testing",
    "title": "4  Normality & hypothesis testing",
    "section": "4.2 Hypothesis testing",
    "text": "4.2 Hypothesis testing\nSince we can define specific areas under the curve within these distributions, we can look at the percentage of area within a certain bound to determine how likely a specific outcome would be. Thus, we can begin to test what the probability of observing an event is within a theoretical, probabilistic space. A couple of important conceptual ideas:\n\nWe may not be able to know the probability of a specific event, but we can figure out the probability of events more extreme or less extreme as that event.\nIf the most likely result is the mean, then the further we move away from the mean, the less likely an event becomes.\nIf we look away from the mean at a certain point, then the area represents the chances of getting a result as extreme or more extreme than what we observe. This probability is known as the \\(p\\) value.\n\nOnce we have a \\(p\\) value, we can make statements about the event that we’ve seen relative to the overall nature of the dataset, but we do not have sufficient information to declare if this result is statistically significant.\n\n4.2.1 Critical Values - \\(\\alpha\\)\nIn order to determine if something is significant, we compare things to a critical value, known as \\(\\alpha\\). This value is traditionally defined as \\(0.05\\), essentially stating that we deem an event as significant if \\(5\\)% or fewer of observed or predicted events are as extreme or more extreme than what we observe.\nYour value should always set your \\(\\alpha\\) critical value before you do your experiments and analyses.\nOur critical value of \\(\\alpha\\) represents our criterion for rejecting the null hypothesis. We set our \\(\\alpha\\) to try to minimize the chances of error.\nType I Error is also known as a false-positive, and is when we reject the null hypothesis when the null is true.\nType II Error is also known as a false-negative, and is when we support the null hypothesis when the null is false.\nBy setting an \\(\\alpha\\), we are creating a threshold of probability at which point we can say, with confidence, that results are different.\n\n\n4.2.2 Introduction to \\(p\\) values\nLet’s say that we are looking at a dataset defined by a standard normal distribution with \\(\\mu=0\\) and \\(\\sigma=1\\). We draw a random value, \\(x\\), with \\(x=1.6\\). What is the probability of drawing a number this extreme or more extreme from the dataset?\nFirst, let’s visualize this distribution:\n\n###THIS WILL TAKE A WHILE TO RUN###\n\n# create gigantic normal distribution dataset\n# will be essentially normal for plotting\n# rnorm gets random values\nx &lt;- rnorm(100000000)\n\n# convert to data frame\nx &lt;- as.data.frame(x)\n# rename column\ncolnames(x) &lt;- c(\"values\")\n\n# thank you stack overflow for the following\n# Creating density plot\np = ggplot(x, \n           aes(x = values)\n          ) + \n  # generic density plot, no fill\n  geom_density(fill=\"lightblue\")\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # rename axes\n  geom_vline(xintercept = 1.6) +\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nAbove, the solid black line represents \\(x\\), with the illustrated standard normal distribution being filled in blue.\nLet’s see how much of the area represents values as extreme or more extreme as our value \\(x\\).\n\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  &lt;-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x &lt; 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nNow we can see that it is only a portion of the distribution as extreme or more extreme than the value we placed on the graph. The area of this region is our \\(p\\) value. This represents the probability of an event as extreme or more extreme occurring given the random variation observed in the dataset or in the distribution approximating the dataset. This is the value we compare to \\(\\alpha\\) - our threshold for rejecting the null hypothesis - to determine whether or not we are going to reject the null hypothesis.\nLet’s look at the above graph again, but let’s visualize a two-tailed \\(\\alpha\\) around the mean with a \\(95\\)% confidence interval. First, we need to get the \\(Z\\) scores for our \\(\\alpha=0.05\\), which we calculate by taking \\(\\frac{\\alpha}{2}\\) to account for the two tails. (Two tails essentially meaning we reject the null mean if we see things greater than or less than our expected value to a significant extent). We can calculate \\(Z\\) scores using qnorm.\n\nlow_alpha &lt;- qnorm(0.025) # looks left\nhi_alpha &lt;- qnorm(0.975) # looks left\n\nprint(paste0(\"Our Z scores are: \", \n             round(low_alpha,2), \n             \" & \", \n             round(hi_alpha,2)))\n\n[1] \"Our Z scores are: -1.96 & 1.96\"\n\n\nThe above values make sense, given the distribution is symmetrical. Our above dashed line is as \\(Z = 1.6\\), which means we should have a \\(p = 0.05\\), so the dashed line should be closer to the mean than our cutoffs.\n\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  &lt;-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x &lt; 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  geom_vline(xintercept = low_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  geom_vline(xintercept = hi_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nExactly as we calculated, we see that our \\(p &lt; \\alpha\\) and thus we do not see an area (in blue) less than the area that would be further than the mean as defined by the red lines.\n\n\n4.2.3 Calculating a \\(Z\\) score\nWhen we are trying to compare our data to a normal distribution, we need to calculate a \\(Z\\) score for us to perform the comparison. A \\(Z\\) score is essentially a measurement of the number of standard deviations we are away from the mean on a standard normal distribution. The equation for a \\(Z\\) score is:\n\\[\nZ = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nWhere \\(\\bar{x}\\) is either a sample mean or a sample value, \\(\\mu\\) is the population mean, \\(\\sigma\\) is the population standard deviation and \\(n\\) is the number of individuals in your sample (\\(1\\) if comparing to a single value).\nWe can calculate this in R using the following function:\n\nzscore &lt;- function(xbar, mu, sd.x, n = 1){\n  z &lt;- (xbar - mu)/(sd.x/sqrt(n))\n  return(z)\n}\n\nNOTE that is the above isn’t working for you, you have a mistake somewhere in your code. Try comparing - character by character - what is listed above to what you have.\nLet’s work through an example, where we have a sample mean of \\(62\\) with \\(5\\) samples compared to a sample mean of \\(65\\) with a standard deviation of \\(3.5\\).\n\nZ &lt;- zscore(xbar = 62, # sample mean\n            mu = 65, # population mean\n            sd.x = 3.5, # population standard deviation\n            n = 5) # number in sample\n\nprint(Z)\n\n[1] -1.91663\n\n\nNow, we can calculate the \\(p\\) value for this \\(Z\\) score.\n\npnorm(Z)\n\n[1] 0.0276425\n\n\nAfter rounding, we get \\(p = 0.03\\), a \\(p\\) that is significant if for a one-tailed \\(\\alpha = 0.05\\) but insignificant for a two-tailed \\(\\alpha = 0.05\\).\n\n\n4.2.4 Calculated the \\(p\\) value\nWe have two different methods for calculating a \\(p\\) value:\n\n4.2.4.1 Comparing to a \\(z\\) table\nWe can compare the \\(z\\) value we calculate to a \\(z\\) table, such as the one at ztable.net. On this webpage, you can scroll and find tables for positive and negative \\(z\\) scores. Note that normal distributions are symmetrical, so you can also transform from negative to positive to get an idea of the area as well. Given that a normal distribution is centered at \\(0\\), a \\(z\\) score of \\(0\\) will have a \\(p\\) value of \\(0.50\\).\nOn the \\(z\\) tables, you will find the tenths place for your decimal in the rows, and then go across to the columns for the hundredths place. For example, go to the website and find the \\(p\\) value for a \\(z\\) score of \\(-1.33\\). You should find the cell marked \\(0.09176\\). Note the website uses naked decimals, which we do not use in this class.\nFor values that aren’t on the \\(z\\) table, we can approximate its position between different points on the \\(z\\) table or, if it is extremely unlikely, denote that \\(p &lt; 0.0001\\).\n\n\n4.2.4.2 Using R\nIn R, we can calculate a \\(p\\) value using the function pnorm. This function uses the arguments of p for our \\(p\\) value, mean for the mean of our distribution, sd for the standard deviation of our distribution, and also information on whether we want to log-transform \\(p\\) or if we are testing a specific hypothesis (lower tail, upper tail, or two-tailed). The function pnorm defaults to a standard normal distribution, which would be a \\(z\\) score, but it can also perform the \\(z\\) transformations for us if we define the mean and standard deviation.\nFor example, if we have a \\(z\\) score of \\(-1.33\\):\n\npnorm(-1.33)\n\n[1] 0.09175914\n\n\nAs we can see, we get the same result as our \\(z\\) table, just with more precision!\nThere are other functions in this family as well in R, including dnorm for quantiles, qnorm for determining the \\(z\\) score for a specific \\(p\\) value, and rnorm for getting random values from a normal distribution with specific dimensions. For now, we will focus on pnorm.\n\n\n\n4.2.5 Workthrough Example\nWhen this class was being designed, Hurricane Milton was about to make contact with Florida. Hurricane Milton is considered one of the strongest hurricanes of all time, so we can look at historical hurricane data to determine just how powerful this storm really was. We can get information on maximum wind speeds of all recorded Atlantic hurricanes as of 2024 from Wikipedia.\n\nhurricanes &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/assignments/hurricane_speeds.csv\")\n\nRows: 889 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): Hurricane_Windspeed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAbove, we have loaded a .csv of one column that has all the hurricane speeds up to 2024. Hurricane Milton is the last row - the most recent hurricane. Let’s separate this one out. We will use [ , ], which defines [rows,columns] to subset data.\n\nmilton &lt;- hurricanes$Hurricane_Windspeed[nrow(hurricanes)]\n\nother_hurricanes &lt;- hurricanes[-nrow(hurricanes),]\n\nWe want to compare the windspeed of Milton (180 mph) to the overall distribution of hurricane speeds. We can visualize this at first.\n\n# all hurricanes\nhist(hurricanes$Hurricane_Windspeed)\n\n\n\n\n\n\n\n\nWindspeeds are more towards the lower end of the distribution, with strong storms being rarer.\nFor the sake of this class, we will assume we can use a normal distribution for these data, but if we were doing an official study we would likely need to use a non-parametric test (we will cover these later, but they cover non-normal data).\n\nmu &lt;- mean(other_hurricanes$Hurricane_Windspeed)\n\nmu\n\n[1] NA\n\n\nHmmm… we need to use na.omit to be sure we do this properly.\n\nother_hurricanes_windspeed &lt;- na.omit(other_hurricanes$Hurricane_Windspeed)\n\nmu &lt;- mean(other_hurricanes_windspeed)\nmean(other_hurricanes_windspeed)\n\n[1] 102.5254\n\n\nNext, we need the standard deviation.\n\nsd.hurricane &lt;- sd(other_hurricanes_windspeed)\n\nsd.hurricane\n\n[1] 23.08814\n\n\nNow, we can calculate our \\(Z\\) value.\n\nZ &lt;- (milton - mu)/sd.hurricane\n\nZ\n\n[1] 3.355603\n\n\nHow significant is this?\n\npnorm(Z)\n\n[1] 0.999604\n\n\nThis is greater than \\(0.5\\), so we need to do \\(1-p\\) to figure things out.\n\n1 - pnorm(Z)\n\n[1] 0.000395961\n\n\nThis rounds to \\(0.0004\\), which means that this is an extremely strong hurricane.\n\n4.2.5.1 Non-normality, for those curious\nWe can do a Shapiro-Wilk test of normality to see if this dataset is normal.\n\nshapiro.test(other_hurricanes_windspeed)\n\n\n    Shapiro-Wilk normality test\n\ndata:  other_hurricanes_windspeed\nW = 0.88947, p-value &lt; 2.2e-16\n\n\nA \\(p &lt; 0.05\\) indicates that these data are non-normal.\nWe can do a Wilcoxon-Test since these data are extremely non-normal.\n\nwilcox.test(other_hurricanes_windspeed,\n            milton)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  other_hurricanes_windspeed and milton\nW = 6.5, p-value = 0.08678\nalternative hypothesis: true location shift is not equal to 0\n\n\nUsing non-normal corrections, we find that this is not an extremely strong hurricane, but it is near the upper end of what we would consider “normal” under historical conditions. Still an extremely bad hurricane!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#confidence-intervals",
    "href": "normality.html#confidence-intervals",
    "title": "4  Normality & hypothesis testing",
    "section": "4.3 Confidence Intervals",
    "text": "4.3 Confidence Intervals\nBecause we can figure out the probability of an event occurring, we can also calculate confidence intervals. A confidence interval provides a range of numbers around a value of interest that indicates where we believe the mean of a population lies and our confidence that it lies within that range. Note that nothing is ever 100% certain, but this helps us determine where a mean is and demonstrates our confidence in our results.\nSpecifically, if the tails of the distribution, our \\(\\alpha\\), are \\(0.05\\), then we have an area of \\(0.95\\) around the mean where we do not reject results. Another perspective on this area is that we can say with \\(95\\)% certainty that a mean is within a certain area, and that if values fall within that confidence area then we do not reject the null hypothesis that the means are equal.\nWe will cover several different ways to calculate confidence intervals, but for normal distributions, we use the following equation, with the \\(0.95\\) confidence interval shown as an example:\n\\[\nCI=\\bar{x} \\pm Z_{1-\\frac{\\alpha}{2}}\\cdot \\frac{\\sigma}{\\sqrt{n}}\n\\]\nThis interval gives us an idea of where the mean should lie. For example, if we are looking at the aforemention beewalk temperature data, we can calulate a \\(0.95\\) confidence interval around the mean.\n\ntemps &lt;- na.omit(beewalk$temperature)\n\nxbar &lt;- mean(temps)\nn &lt;- length(temps)\nsdtemp &lt;- sd(temps)\n# Z for 0.95 as P, so for 0.975, 0.025\n# get value from P\nZ &lt;- qnorm(0.975)\n\nCI &lt;- Z*(sdtemp/sqrt(n))\n\nCI\n\n[1] 0.01458932\n\n\nWe have a very narrow confidence zone, because we have so many measurements. Let’s round everything and present it in a good way.\nIf I want numbers to show up in text in RMarkdown, I can add code to a line of plain text using the following syntax:\n\n# DO NOT RUN\n# Format in plaintext\n`r xbar`\n\n\n\n\nThis is the “coded” version of the text below. Compare the above window to the text below this image.\n\n\nTyping that into the plaintext should render as the following: 18.645963. Then I can also type my answer as follows:\nThe \\(95\\)% Confidence Interval for the mean for this temperature dataset is 18.65 \\(\\pm\\) 0.01.\nNote that the above is rounded to two decimal places to illustrative purposes ONLY, and should be rounded to one decimal place if it was a homework assignment because the original data has only one decimal place.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#homework-chapter-8",
    "href": "normality.html#homework-chapter-8",
    "title": "4  Normality & hypothesis testing",
    "section": "4.4 Homework: Chapter 8",
    "text": "4.4 Homework: Chapter 8\nPlease complete problems 8.1, 8.2, 8.3 & 8.6. Follow the directions as written in the book. Submit one html file, as derived from RStudio. For maximum clarity, create headings to separate your problems. (Remember, a header can be invoked by placing ‘#’ in front of a line of text. For example: the header here is written as # Homework: Chapter 8).\n\n\n\n\nComont, R. (2020). BeeWalk dataset 2008-23. https://doi.org/10.6084/m9.figshare.12280547.v4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "exam2.html",
    "href": "exam2.html",
    "title": "5  Exam 2 practice",
    "section": "",
    "text": "5.1 Exam 2 Practice\nThe following is practice for the exam. Please work through these problems and be ready to discuss them in class.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exam 2 practice</span>"
    ]
  },
  {
    "objectID": "exam2.html#question-1-cyclones",
    "href": "exam2.html#question-1-cyclones",
    "title": "5  Exam 2 practice",
    "section": "5.2 Question 1: Cyclones",
    "text": "5.2 Question 1: Cyclones\nConsider this short data set:\n\n\n\nLatitude band\nSeason\nNumber of cyclones\n\n\n\n\n40 - 49° S\nFall\n370\n\n\n40 - 49° S\nWinter\n452\n\n\n40 - 49° S\nSpring\n273\n\n\n40 - 49° S\nSummer\n422\n\n\n50 - 59° S\nFall\n526\n\n\n50 - 59° S\nWinter\n624\n\n\n50 - 59° S\nSpring\n513\n\n\n50 - 59° S\nSummer\n1,059\n\n\n60 - 69° S\nFall\n980\n\n\n60 - 69° S\nWinter\n1,200\n\n\n60 - 69° S\nSpring\n995\n\n\n60 - 69° S\nSummer\n1,751\n\n\n\nAll of the data was collected in the same year to determine whether the occurrences of cyclones differed by latitude and season around Antarctica. Use this data to answer the following questions.\n\nClassify each of the three variables above as either an explanatory or a response variable. Justify your answer.\nClassify each of the three variables above as either a nominal, ordinal, interval, or ratio variable. Justify your answer.\nState the null and alternative hypotheses for this scenario. Note: there are two sets of null/alternative hypotheses.\nCalculate the mean, median, and mode of your response variable. Based on your results, would you expect your data to be normally distributed or not? Justify your answer.\nLet’s say now that you’re testing the null hypothesis that the mean number of cyclones in this one year is similar to the average year. The mean number of cyclones per year across all years of data collected is 650 with a standard deviation of 425. Calculate the z-score and make a decision regarding the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exam 2 practice</span>"
    ]
  },
  {
    "objectID": "exam2.html#question-2-minnows",
    "href": "exam2.html#question-2-minnows",
    "title": "5  Exam 2 practice",
    "section": "5.3 Question 2: Minnows",
    "text": "5.3 Question 2: Minnows\nConsider this scenario: You have discovered a never-before-documented population of minnow in the Kearney Canal near campus. During your first sampling trip, you notice that the total length (i.e., measured from the tip of the snout to the very tip of the tail) of the fish you measure appear to be smaller than the average total length of the species as recorded among all known individuals across their range. The mean total length noted in one publication is 85 mm with a standard deviation of 4.50. Below is your data the data from 20 minnows that you captured during your first sampling trip to the Kearney Canal:\n\n\n\nFish ID\nLength (mm)\n\n\n\n\n1\n89\n\n\n2\n75\n\n\n3\n86\n\n\n4\n74\n\n\n5\n69\n\n\n6\n100\n\n\n7\n73\n\n\n8\n69\n\n\n9\n96\n\n\n10\n79\n\n\n11\n61\n\n\n12\n62\n\n\n13\n95\n\n\n14\n98\n\n\n15\n100\n\n\n16\n57\n\n\n17\n70\n\n\n18\n78\n\n\n19\n65\n\n\n20\n65\n\n\n\nHINT: Try combining numbers using the c command. See the Glossary for more information!\n\nState the null and alternative hypothesis for your study.\nCalculate the median, first and third quartiles, and interquartile range of your response variable. Create a boxplot. Based on your results, are there any outliers in your data? Explain.\nCalculate the \\(z\\)-score for this scenario. Please show all of your work.\nWhat is the probability that, by random chance alone, you would find your observed mean or something more extreme?\nAssume that you set your \\(\\alpha\\) for your study prior to your data collection to \\(0.05\\). Based on this information and the \\(p\\)-value you obtained for part \\(c\\) above, is your null hypothesis supported or rejected?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exam 2 practice</span>"
    ]
  },
  {
    "objectID": "probdist.html",
    "href": "probdist.html",
    "title": "6  Probability distributions",
    "section": "",
    "text": "6.1 Probability distributions\nWe rely on multiple different probability distributions to help us understand what probable outcomes are for a specific scenario. All of the tests that we are performing are comparing our results to what we would expect under perfectly random scenarios. For example, if we are flipping a coin, we are interested in whether the the observation we have of the flips on our coin matches our expectation given the probability of getting heads or tails on a perfectly fair coin. While it is possible to get all heads or all tails on a coin flip, it is highly unlikely and may lead us to believe we have an unfair coin. The more trails we perform, the more confident we can be that out coin is atypical.\nWe perform similar comparisons for other distributions. If we are comparing sets of events, we can look at the probability of those events occurring if events are occurring randomly. If we are comparing counts, we can compare our counts to our expectation of counts if events or subjects are distributed randomly throughout the matrix or whether two sets of counts are likely under the same sets of assumptions.\nRemember, for our specific tests, we are setting an \\(\\alpha\\) value in advance (traditionally \\(0.05\\), or \\(5\\)%) against which we compare our \\(p\\) value, with \\(p\\) representing the probability of observing an event as extreme or more extreme than the event we observe given a specific probability distribution.\nPreviously, we talked about the normal distribution, which is used to approximate a lot of datasets in nature. However, several other probability distributions are also useful for biological systems, which are outlined here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#binomial-distribution",
    "href": "probdist.html#binomial-distribution",
    "title": "6  Probability distributions",
    "section": "6.2 Binomial distribution",
    "text": "6.2 Binomial distribution\nA binomial distribution is one in which only two outcomes are possible - often coded as \\(0\\) and \\(1\\) and usually representing failure and success, respectively. The binomial is described by the following function:\n\\[\np(x)=\\binom{n}{x}p^{x}(1-p)^{n-x}\n\\]\nwhere \\(n =\\) number of trials, \\(x =\\) the number of successes, and \\(p =\\) the probability of a success under random conditions.\nIn R, the binomial distribution is represented by the following functions:\n\ndbinom: the density of a binomial distribution\npbinom: the distribution function, or the probability of a specific observation\nqbinom: the value at which a specific probability is found (the quantile function)\nrbinom: generates random values according to a binomial.\n\nLet’s see what this looks like. Let’s consider a scenario where we flip a coin 10 times and get 9 heads. How likely is this outcome?\n\nx &lt;- pbinom(q = 9, # number successes, 9 heads\n            size = 10, # number of trials, 10 flips\n            prob = 0.5) # probability with a fair coin\n\nround(x,4)\n\n[1] 0.999\n\n\nNOTE that the trailing \\(0\\) is dropped, such that the real answer is \\(0.9990\\). However, we mentioned before that the \\(p\\) value should be the probability of a result as extreme or more extreme, meaning that it should always be less than \\(0.5\\). If we are reporting a value of greater than \\(0.5\\), then we are comparing to the upper tail of the distribution. For a one-tailed \\(\\alpha\\) of \\(0.05\\), this would mean that we are looking for a value greater than \\(0.95\\) (\\(1-\\alpha\\)).\nSo, our real \\(p\\) is:\n\n1-round(x,4)\n\n[1] 0.001\n\n\nAgain, the trailing zero is missing. Given that \\(p &lt; \\alpha\\), we reject the null hypothesis that this is a fair coin.\nHow does this distribution look?\n\n# number of successes\n# start at 0 for no heads\nx &lt;- 0:10\n# cumulative probability to left of outcome\ny &lt;- pbinom(x,size = 10, prob = 0.5, lower.tail = T)\n\n# cumulative probability of results to the left\nplot(x,y,\n     type=\"l\") # line plot\n\n\n\n\n\n\n\n\nWhat about if we always have \\(p\\) less than \\(0.5\\) to reflect two tails?\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")\n\n\n\n\n\n\n\n\nWhat if we do this with a bigger dataset, like for \\(50\\) flips?\n\n# number of successes\n# start at 0 for no heads\nx &lt;- 0:50\n# cumulative probability to left of outcome\ny &lt;- pbinom(x,size = length(x), prob = 0.5, lower.tail = T)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")\n\n\n\n\n\n\n\n\nAs we increase the number of flips, we can see that the probability of success forms a normal distribution centered on the outcome given the default probability. Thus, as we deviate from our expected outcome (initial probability multiple by the number of trials), then our results become less likely.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#poisson-distribution",
    "href": "probdist.html#poisson-distribution",
    "title": "6  Probability distributions",
    "section": "6.3 Poisson distribution",
    "text": "6.3 Poisson distribution\nThe Poisson distribution is used to reflect random count data. Specifically, the Poisson is used to determine if success events are overdispersed (i.e., regularly spaced), random, or underdispersed (i.e., clustered). The Poisson introduces the variable lambda (\\(\\lambda\\)) which represents the mean (\\(\\mu\\)) and the variance (\\(\\sigma^2\\)), which are equal in a Poisson distribution. A Poisson distribution is described by the following function:\n\\[\np(x)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\]\nThe Poisson is represented by the following functions in R which closely resemble the functions for the normal and binomial distributions:\n\ndpois: the log density function\nppois: log distribution (probability) function\nqpois: quantile function\nrpois: random values from a Poisson.\n\nLet’s look at the probability of \\(0\\) to \\(10\\) successes when we have our \\(\\lambda=1\\).\n\nx &lt;- 0:10\ny &lt;- ppois(x,lambda = 1)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")\n\n\n\n\n\n\n\n\nAs we can see, the probability of rare events is high, whereas the probability quickly decreases as the number of successes increases.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#chi2-squared-distribution",
    "href": "probdist.html#chi2-squared-distribution",
    "title": "6  Probability distributions",
    "section": "6.4 \\(\\chi^2\\)-squared distribution",
    "text": "6.4 \\(\\chi^2\\)-squared distribution\n\\(\\chi^2\\)-squared (pronounced “kai”, and spelled “chi”) is a distribution used to understand if count data between different categories matches our expectation. For example, if we are looking at students in the class and comparing major vs. number of books read, we would expect no association, however we may find an association for a major such as English which required reading more literature. The \\(\\chi^2\\) introduces a new term degrees of freedom (\\(df\\)) which reflects the number of individuals in the study. For many tests, \\(df\\) are needed to reflect how a distribution changes with respect the number of individuals (and amount of variation possible) within a dataset. The equation for the \\(\\chi^2\\) is as follows, with the \\(\\chi^2\\) being a special case of the gamma (\\(\\gamma\\) or \\(\\Gamma\\)) distribution that is affected by the \\(df\\), which is defined as the number of rows minus one multiplied by the number of columns minus one \\(df = (rows-1)(cols-1)\\):\n\\[\nf_n(x)=\\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}x^\\frac{n}{2-1}e^\\frac{-x}{2}\n\\]\nThe \\(\\chi^2\\)-squared distribution is also represented by the following functions, which perform the same things as the previous outlined equivalents for Poisson and binomial:\n\ndchisq\npchisq\nqchisq\nrchisq\n\nWe can view these probabilities as well:\n\nx &lt;- 0:10\n\ny &lt;- pchisq(x, df = 9)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")\n\n\n\n\n\n\n\n\n\n6.4.1 Calculating the test statistic\nWe evaluate \\(\\chi^2\\) tests by calculating a \\(\\chi^2\\) value based on our data and comparing it to an expected \\(\\chi^2\\) distribution. This test statistic can be evaluated by looking at a \\(\\chi^2\\) table or by using R. Note that you need to know the degrees of freedom in order to properly evaluate a \\(\\chi^2\\) test. We calculate our test statistic as follows:\n\\[\n\\chi^2=\\Sigma\\frac{(o-e)^2}{e}\n\\]\nwhere \\(e =\\) the number of expected individuals and \\(o =\\) the number of observed individuals in each category. Since we are squaring these values, we will only have positive values, and thus this will always be a one-tailed test.\n\n\n6.4.2 \\(\\chi^2\\) estimations in R\nUsually when we use a \\(\\chi^2\\), we are looking at count data. Let’s consider the following hypothetical scenario, comparing experience with R between English majors (who, in this theoretical scenario, do not regularly use R) and Biology majors who are required to take R for this class:\n\n\n\nMajor\nR experience\nNo R experience\n\n\n\n\nEnglish\n2\n8\n\n\nBiology\n9\n1\n\n\n\nIntuitively, we can see a difference, but we want to perform a statistical test to see just how likely these counts would be if both groups were equally likely to use R:\n\nchisq.test(x = c(2,8),\n           y = c(9,1))\n\nWarning in chisq.test(x = c(2, 8), y = c(9, 1)): Chi-squared approximation may\nbe incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  c(2, 8) and c(9, 1)\nX-squared = 0, df = 1, p-value = 1\n\n\nHere we get \\(p = 1\\), which reflects a one-tailed interpretation.\n\n\n6.4.3 Calculating \\(\\chi^2\\) by hand",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#fishers-exact-test",
    "href": "probdist.html#fishers-exact-test",
    "title": "6  Probability distributions",
    "section": "6.5 Fisher’s exact test",
    "text": "6.5 Fisher’s exact test\nTo be completed; involves calculations on smaller datasets.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#homework",
    "href": "probdist.html#homework",
    "title": "6  Probability distributions",
    "section": "6.6 Homework",
    "text": "6.6 Homework\n\n6.6.1 Chapter 5\n\n\n6.6.2 Chapter 7",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "7  Single population means testing",
    "section": "",
    "text": "7.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single population means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#t-distribution",
    "href": "ttest.html#t-distribution",
    "title": "7  Single population means testing",
    "section": "7.2 t-distribution",
    "text": "7.2 t-distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single population means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#t-tests",
    "href": "ttest.html#t-tests",
    "title": "7  Single population means testing",
    "section": "7.3 t-tests",
    "text": "7.3 t-tests",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single population means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#wilcoxon-tests",
    "href": "ttest.html#wilcoxon-tests",
    "title": "7  Single population means testing",
    "section": "7.4 Wilcoxon tests",
    "text": "7.4 Wilcoxon tests",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single population means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#confidence-intervals",
    "href": "ttest.html#confidence-intervals",
    "title": "7  Single population means testing",
    "section": "7.5 Confidence intervals",
    "text": "7.5 Confidence intervals",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single population means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#homework-chapter-9",
    "href": "ttest.html#homework-chapter-9",
    "title": "7  Single population means testing",
    "section": "7.6 Homework: Chapter 9",
    "text": "7.6 Homework: Chapter 9",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Single population means testing</span>"
    ]
  },
  {
    "objectID": "two_ttest.html",
    "href": "two_ttest.html",
    "title": "8  Two sample tests",
    "section": "",
    "text": "8.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Two sample tests</span>"
    ]
  },
  {
    "objectID": "two_ttest.html#t-tests",
    "href": "two_ttest.html#t-tests",
    "title": "8  Two sample tests",
    "section": "8.2 t-tests",
    "text": "8.2 t-tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Two sample tests</span>"
    ]
  },
  {
    "objectID": "two_ttest.html#mann-whitney-u-tests",
    "href": "two_ttest.html#mann-whitney-u-tests",
    "title": "8  Two sample tests",
    "section": "8.3 Mann-Whitney U tests",
    "text": "8.3 Mann-Whitney U tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Two sample tests</span>"
    ]
  },
  {
    "objectID": "two_ttest.html#error",
    "href": "two_ttest.html#error",
    "title": "8  Two sample tests",
    "section": "8.4 Error",
    "text": "8.4 Error",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Two sample tests</span>"
    ]
  },
  {
    "objectID": "two_ttest.html#homework-chapter-10",
    "href": "two_ttest.html#homework-chapter-10",
    "title": "8  Two sample tests",
    "section": "8.5 Homework: Chapter 10",
    "text": "8.5 Homework: Chapter 10",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Two sample tests</span>"
    ]
  },
  {
    "objectID": "anova1.html",
    "href": "anova1.html",
    "title": "9  ANOVA: Part 1",
    "section": "",
    "text": "9.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#anova-by-hand",
    "href": "anova1.html#anova-by-hand",
    "title": "9  ANOVA: Part 1",
    "section": "9.2 ANOVA: By hand",
    "text": "9.2 ANOVA: By hand",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#anova-by-r",
    "href": "anova1.html#anova-by-r",
    "title": "9  ANOVA: Part 1",
    "section": "9.3 ANOVA: By R",
    "text": "9.3 ANOVA: By R",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#kruskal-wallis-tests",
    "href": "anova1.html#kruskal-wallis-tests",
    "title": "9  ANOVA: Part 1",
    "section": "9.4 Kruskal-Wallis tests",
    "text": "9.4 Kruskal-Wallis tests",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#homework-chapter-11",
    "href": "anova1.html#homework-chapter-11",
    "title": "9  ANOVA: Part 1",
    "section": "9.5 Homework: Chapter 11",
    "text": "9.5 Homework: Chapter 11",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "10  ANOVA: Part 2",
    "section": "",
    "text": "10.1 Two-way ANOVA",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#designs",
    "href": "anova2.html#designs",
    "title": "10  ANOVA: Part 2",
    "section": "10.2 Designs",
    "text": "10.2 Designs\n\n10.2.1 Randomized block design\n\n\n10.2.2 Repeated measures\n\n\n10.2.3 Factorial ANOVA",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#friedmans-test",
    "href": "anova2.html#friedmans-test",
    "title": "10  ANOVA: Part 2",
    "section": "10.3 Friedman’s test",
    "text": "10.3 Friedman’s test",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#homework-chapter-12",
    "href": "anova2.html#homework-chapter-12",
    "title": "10  ANOVA: Part 2",
    "section": "10.4 Homework: Chapter 12",
    "text": "10.4 Homework: Chapter 12",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "cor_reg.html",
    "href": "cor_reg.html",
    "title": "11  Correlation & regression",
    "section": "",
    "text": "11.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#correlation",
    "href": "cor_reg.html#correlation",
    "title": "11  Correlation & regression",
    "section": "11.2 Correlation",
    "text": "11.2 Correlation\n\n11.2.1 Pearson’s\n\n\n11.2.2 Spearman’s\n\n\n11.2.3 Other non-parametric methods",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#correlation-1",
    "href": "cor_reg.html#correlation-1",
    "title": "11  Correlation & regression",
    "section": "11.3 Correlation",
    "text": "11.3 Correlation\n\n11.3.1 Parametric\n\n\n11.3.2 Non-parametric",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#homework",
    "href": "cor_reg.html#homework",
    "title": "11  Correlation & regression",
    "section": "11.4 Homework",
    "text": "11.4 Homework\n\n11.4.1 Chapter 13\n\n\n11.4.2 Chapter 14",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "final_exam.html",
    "href": "final_exam.html",
    "title": "12  Final exam & review",
    "section": "",
    "text": "12.1 Pick the test",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "final_exam.html#final-review",
    "href": "final_exam.html#final-review",
    "title": "12  Final exam & review",
    "section": "12.2 Final review",
    "text": "12.2 Final review",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "13  Conclusions",
    "section": "",
    "text": "13.1 ᏙᏓᏓᎪᎲᎢ\nᏙᏓᏓᎪᎲᎢ (pronounced doh-dah-dah-go-huh-ee) is a traditional Cherokee farewell. It does not mean goodbye, but rather reflects a parting of ways until a group of folks meet again.\nI enjoyed getting to know all of you in class, and please feel free to reach out or stop by and say hi if you are ever passing through Kearney in the future or if you need help with something biology related.\nWishing you the best,\nDr. Cooper",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Comont, R. (2020). BeeWalk dataset 2008-23.\nhttps://doi.org/10.6084/m9.figshare.12280547.v4\n\n\nCooper, J. C. (2021). Biogeographic and Ecologic\nDrivers of Avian Diversity.\n[Online.] Available at https://doi.org/10.6082/uchicago.3379.\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020a). Patient\nages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020b). Estimation of the\nforce of infection and infectious period of skin sores in remote\nAustralian communities using interval-censored data.\nPLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset.\nhttps://doi.org/10.6084/m9.figshare.24119643.v1",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "14  Glossary",
    "section": "",
    "text": "14.1 Common Commands\nThe following are common useful commands used in R, with examples of their use.\nx &lt;- 10\nx\n\n[1] 10\nx &lt;- c(10,11)\nx\n\n[1] 10 11",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#common-commands",
    "href": "glossary.html#common-commands",
    "title": "14  Glossary",
    "section": "",
    "text": "&lt;- / = - save a value as an object\n\n\n\nc - concatenate, place two values together",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#basic-statistics",
    "href": "glossary.html#basic-statistics",
    "title": "14  Glossary",
    "section": "14.2 Basic statistics",
    "text": "14.2 Basic statistics\nFor these examples, we will create a random vector of number to demonstrate how they work.\n\nx &lt;- rnorm(1000)\n\n\nmean - get the mean / average of a set of data\n\n\nmean(x)\n\n[1] -0.03533576",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Glossary</span>"
    ]
  }
]