[
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "12  ANOVA: Part 2",
    "section": "",
    "text": "12.1 Two-way ANOVA\nPreviously, we discussed one-way ANOVAs, where we are looking at a single factor split across three or more groups and trying to determine if the means of these groups are equal (i.e., \\(H_0: \\mu_1=\\mu_2=...\\mu_i\\)). ANOVA specifically allows us to analyze the variance of these different groups to ascertain which factors are most responsible for the variation we observe in the data. Because of the way ANOVA operates, we can actually test multiple different combinations of variables simultaneously in what we call a two-way ANOVA.\nDon’t forget to load your required packages - some we have used before, like agricolae, plyr, and tidyverse, but others are new for this section: multcomp and nlme! As a reminder, these packages are designed for the following:\nlibrary(agricolae)\nlibrary(plyr)\nlibrary(tidyverse)\n\n# NEW PACKAGES NEEDED\n# Don't forget to install these on your machine\nlibrary(multcomp)\nlibrary(nlme)\nlibrary(PMCMRplus)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#two-way-anova",
    "href": "anova2.html#two-way-anova",
    "title": "12  ANOVA: Part 2",
    "section": "",
    "text": "agricolae: originally written as a Master’s thesis at the Universidad Nacional de Ingeniería (Lima, Perú), this package is designed to help with agricultural research.\nplyr: tools for common problems, including splitting data, applying functions across data, and combining datasets together.\ntidyverse: one we are already familiar with; a wrapper for installing ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats.\nmultcomp: more in depth and better MULTiple COMParisons via linear models and related models.\nnlme: a package for fitting Gaussian and non-linear mixed-effect models.\nPMCMRplus: a math package with post-hoc tests for Friedman’s test",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#designs",
    "href": "anova2.html#designs",
    "title": "12  ANOVA: Part 2",
    "section": "12.2 Designs",
    "text": "12.2 Designs\nThere are several different designs for two-way ANOVAs, and we will cover some of the most common designed here.\nFor these examples, we are going to randomly generated examples. I will refer to the variables as Response and Explanatory for simplicity’s sake.\n\n12.2.1 Randomized block design\nRandomized block designs look at combinations of variables that could be affecting the results. More specifically, we are looking at two strata or factors and their effects on a continuous response variable.\n\nset.seed(8675309)\n\n# random example\n\n# Blocking variable\nBlocking_Variable &lt;- c(\"Group 1\", \"Group 2\", \"Group 3\")\n\n# explanatory variables\n# these are your columns\n# these are your primary hypothesis\nExplanatory_1 &lt;- c(10.1, 9.4, 11.1)\nExplanatory_2 &lt;- c(12, 13.0, 15.4)\nExplanatory_3 &lt;- c(11.2, 10.1, 11.9)\n\n# create \"data table\" as we normally see it\n# combine all columns\ndata_expanded &lt;- cbind(Blocking_Variable,\n                       Explanatory_1,\n                       Explanatory_2,\n                       Explanatory_3) |&gt; \n  as.data.frame() # create data frame\n\ndata_expanded\n\n  Blocking_Variable Explanatory_1 Explanatory_2 Explanatory_3\n1           Group 1          10.1            12          11.2\n2           Group 2           9.4            13          10.1\n3           Group 3          11.1          15.4          11.9\n\n\nNote that this table is in the format that we most often see, but we need to reshape these data to make it easier for us to perform our analyses. I created the data here as a matrix with named columns and rows; the following code may need to be adjusted if you do things differently.\n\n# expand to \"long\" format\n# if not done earlier, convert to data frame\n\ndata &lt;- data_expanded |&gt;\n  # !by column for aggregating\n  # names_to = what to name column aggregation\n  # values_to = what the measurements should be called\n  pivot_longer(!Blocking_Variable, names_to = \"Explanatory_Variables\", values_to = \"Measurements\")\n\ndata\n\n# A tibble: 9 × 3\n  Blocking_Variable Explanatory_Variables Measurements\n  &lt;chr&gt;             &lt;chr&gt;                 &lt;chr&gt;       \n1 Group 1           Explanatory_1         10.1        \n2 Group 1           Explanatory_2         12          \n3 Group 1           Explanatory_3         11.2        \n4 Group 2           Explanatory_1         9.4         \n5 Group 2           Explanatory_2         13          \n6 Group 2           Explanatory_3         10.1        \n7 Group 3           Explanatory_1         11.1        \n8 Group 3           Explanatory_2         15.4        \n9 Group 3           Explanatory_3         11.9        \n\n\nNow we can do our ANOVA. Note that I put factor around the blocking variable.\n\n# mark block by factor\n# best to always use\ndata_aov &lt;- aov(Measurements ~ Explanatory_Variables + factor(Blocking_Variable), data)\n\nsummary(data_aov)\n\n                          Df Sum Sq Mean Sq F value Pr(&gt;F)  \nExplanatory_Variables      2 17.182   8.591  14.412 0.0149 *\nfactor(Blocking_Variable)  2  6.829   3.414   5.728 0.0670 .\nResiduals                  4  2.384   0.596                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this particular example, the blocking variable does not significantly differ, however the explanatory variable does differ.\nRemember, the columns represent your primary hypothesis. You will only plot your results if your primary hypothesis is significant!\nGiven that our primary null hypothesis is rejected (that is to say, not all means are equal), we need to plot our results.\nTo determine which mean(s) differ, we will use a Tukey Test. Unfortunately, the agricolae function HSD.test does not work as well for these multi-directional ANOVAs, so we need to use TukeyHSD.\n\ntukey_data_aov &lt;- TukeyHSD(data_aov)\n\ntukey_data_aov\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Measurements ~ Explanatory_Variables + factor(Blocking_Variable), data = data)\n\n$Explanatory_Variables\n                                  diff       lwr        upr     p adj\nExplanatory_2-Explanatory_1  3.2666667  1.019919  5.5134144 0.0144034\nExplanatory_3-Explanatory_1  0.8666667 -1.380081  3.1134144 0.4333834\nExplanatory_3-Explanatory_2 -2.4000000 -4.646748 -0.1532523 0.0406301\n\n$`factor(Blocking_Variable)`\n                      diff        lwr      upr     p adj\nGroup 2-Group 1 -0.2666667 -2.5134144 1.980081 0.9082398\nGroup 3-Group 1  1.7000000 -0.5467477 3.946748 0.1118461\nGroup 3-Group 2  1.9666667 -0.2800810 4.213414 0.0745795\n\n\nAs we can see above, each pairwise comparison is given a \\(p\\) value for the level of difference. We need to manually label these groups based on these \\(p\\) values, with groups being considered different if \\(p &lt; 0.05\\). We can do this as follows, but unfortunately, we need to do it by hand since we don’t have a short-form code (yet) for this conversion.\n\n# change Explanatory_Variables to your data\nTreatments &lt;- unique(data$Explanatory_Variables)\n\nsig_labels &lt;- Treatments |&gt; \n  as.data.frame() |&gt; \n  mutate(Significance = rep(NA, length(Treatments)))\n\n# Change Explanatory_Variables to your data\ncolnames(sig_labels) &lt;- c(\"Explanatory_Variables\", # MUST BE SAME AS DATA\n                          \"Significance\")\n\ntukey_data_aov$Explanatory_Variables\n\n                                  diff       lwr        upr      p adj\nExplanatory_2-Explanatory_1  3.2666667  1.019919  5.5134144 0.01440339\nExplanatory_3-Explanatory_1  0.8666667 -1.380081  3.1134144 0.43338343\nExplanatory_3-Explanatory_2 -2.4000000 -4.646748 -0.1532523 0.04063012\n\n\nNOTE that I am going to have to adjust column names and variable names a few times. Based on the above, we can see that Explanatory_3 and Explanatory_1 are not different from each other, but everything else is in a different group relative to each other. We can label these by hand.\n\nsig_labels$Significance &lt;- c(\"A\", \"B\", \"A\")\n\nsig_labels\n\n  Explanatory_Variables Significance\n1         Explanatory_1            A\n2         Explanatory_2            B\n3         Explanatory_3            A\n\n\nAs we can see above, now only Explanatory_2 is given a different letter category.\nNow, we can plot these different factors.\n\n# summarize by group\n# slight adjustment from previous\nsummary_data &lt;- ddply(data, \"Explanatory_Variables\", summarise,\n                          N = length(as.numeric(Measurements)),\n                          mean = mean(as.numeric(Measurements)),\n                          sd = sd(as.numeric(Measurements)),\n                          se = sd / sqrt(N))\n\nsummary_data\n\n  Explanatory_Variables N     mean        sd        se\n1         Explanatory_1 3 10.20000 0.8544004 0.4932883\n2         Explanatory_2 3 13.46667 1.7473790 1.0088497\n3         Explanatory_3 3 11.06667 0.9073772 0.5238745\n\n\n\n# SET Y LIMITS\n# change based on observed data\nylims &lt;- c(0, 20)\n\n# set label height, can change before plotting function\nlabel_height &lt;- 4\n\nggplot(summary_data, # plot summary data\n       # Define plotting - x by group, y is mean, grouping by group\n       aes(x = Explanatory_Variables, y = mean)) +\n  # add points to plot for y values\n  geom_point() +\n  # add error bars around points\n  geom_errorbar(data = summary_data, \n                # define error bars\n                aes(ymin = mean - 2*se, ymax = mean+2*se), \n                # width of bar\n                width = 0.1) +\n  # set vertical limits for plot\n  ylim(ylims) +\n  # make it a classic theme - more legible\n  theme_classic() +\n  # add text to plot\n  geom_text(data = sig_labels,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = Explanatory_Variables, \n                # define height of label\n                y = 4, \n                # what are the labels?\n                label = paste0(Significance))) +\n  xlab(\"Treatment\") +\n  ylab(\"Mean\") +\n  # remove legend - not needed here\n  theme(legend.position = \"none\",\n        # make label text vertical, easier to read\n        axis.text.x = element_text(angle = 90, \n                                   # vertical offset of text\n                                   vjust = 0.5, \n                                   # text size\n                                   size = 12))\n\n\n\n\n\n\n\n\n\n\n12.2.2 Repeated measures\nNow, we are going to do a repeated measures ANOVA, where we have the same individuals being measured multiple times. Consider the following imaginary dataset:\n\nVisit_1 &lt;- c(5.5,6.2,5.8)\nVisit_2 &lt;- c(4.6,5.4,5.2)\nVisit_3 &lt;- c(3.8,4.0,3.9)\n\nIndividuals &lt;- c(paste0(\"Individual\",\" \",1:3))\n\ndata &lt;- cbind(Individuals, \n                   Visit_1,\n                   Visit_2,\n                   Visit_3) |&gt;\n  as.data.frame() |&gt; \n  pivot_longer(!Individuals,\n               names_to = \"Visits\",\n               values_to = \"Measurements\")\n\ndata\n\n# A tibble: 9 × 3\n  Individuals  Visits  Measurements\n  &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;       \n1 Individual 1 Visit_1 5.5         \n2 Individual 1 Visit_2 4.6         \n3 Individual 1 Visit_3 3.8         \n4 Individual 2 Visit_1 6.2         \n5 Individual 2 Visit_2 5.4         \n6 Individual 2 Visit_3 4           \n7 Individual 3 Visit_1 5.8         \n8 Individual 3 Visit_2 5.2         \n9 Individual 3 Visit_3 3.9         \n\n\nWe need to perform the ANOVA again, but we need to account for the factor of which locations are repeated.\n\nrepeated_aov &lt;- aov(Measurements ~ factor(Visits) + Error(factor(Individuals)), data)\n\nsummary(repeated_aov)\n\n\nError: factor(Individuals)\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  2 0.4867  0.2433               \n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(Visits)  2  5.687  2.8433   89.79 0.000475 ***\nResiduals       4  0.127  0.0317                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnfortunately, because of the model this is, we cannot perform a Tukey Test on the “object” that is created from this ANOVA analysis. We can, however, approach this from a different direction and get our Tukey results (thanks to Henrik on StackOverflow!). For this to work, we need to install the packages nlme and multcomp.\n\n# ensure data is proper format\ndata$Individuals &lt;- as.factor(data$Individuals)\ndata$Visits &lt;- as.factor(data$Visits)\ndata$Measurements &lt;- as.numeric(data$Measurements)\n\nThe next part of the code fits a linear model to the data. A linear model, which we will cover later in the class, is mathematically very similar to an ANOVa. However, we can data from this model and extract the ANOVA data to understand more about the interactions. We need to use a linear model for this to account with the relationships between the two.\n\n# fit a linear mixed-effects model\n# similar to ANOVA\n\nlme_data &lt;- lme(Measurements ~ Visits, \n                         data = data, \n                         # define repeated section\n                         random = ~1|Individuals)\n\n# perform ANOVA on model\nanova(lme_data)\n\n            numDF denDF  F-value p-value\n(Intercept)     1     4 900.1654  &lt;.0001\nVisits          2     4  89.7894   5e-04\n\n\nAs we can see above, we can get the ANOVA results from this linear mixed-effects model fit to the dataset. Now, we need to know post-hoc which sets are different:\n\nlme_data |&gt;\n  # \"general linear hypothesis\"\n  # define a comparison to make\n  # can add corrections like test = adjusted (type = \"bonferroni\")\n  glht(linfct = mcp(Visits = \"Tukey\")) |&gt;\n  # return a summary of the above\n  summary()\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lme.formula(fixed = Measurements ~ Visits, data = data, random = ~1 | \n    Individuals)\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \nVisit_2 - Visit_1 == 0  -0.7667     0.1453  -5.277   &lt;1e-06 ***\nVisit_3 - Visit_1 == 0  -1.9333     0.1453 -13.306   &lt;1e-06 ***\nVisit_3 - Visit_2 == 0  -1.1667     0.1453  -8.030   &lt;1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe can see that every visit is different.\n\n# manually labeling\n\nsig_levels_repeated &lt;- matrix(data = c(\"Visit_1\", \"A\",\n                                       \"Visit_2\", \"B\",\n                                       \"Visit_3\", \"C\"), \n                              byrow = T, ncol = 2) |&gt; as.data.frame()\n\n# make labels match!\ncolnames(sig_levels_repeated) &lt;- c(\"Visits\", \"Significance\")\n\nLet’s plot these. Note that we are not summarizing these the same way, since things are varying based on individual as well.\nNote: For reasons I am not certain, you need to put the locations and then ggplot uses these colors to define everything. I really don’t know why this is happening, so if you have a solution, let me know.\n\nsummary_data &lt;- ddply(data, \"Visits\", summarise,\n                          N = length(as.numeric(Measurements)),\n                          mean = mean(as.numeric(Measurements)),\n                          sd = sd(as.numeric(Measurements)),\n                          se = sd / sqrt(N))\n\n\nggplot(summary_data,\n       aes(x = Visits, y = mean)) +\n  geom_point() +\n  geom_errorbar(data = summary_data, \n                # define error bars\n                aes(ymin = mean - 2*se, ymax = mean+2*se), \n                # width of bar\n                width = 0.1) +\n  # set vertical limits for plot\n  ylim(c(0,10)) +\n  # make it a classic theme - more legible\n  theme_classic() +\n  # add text to plot\n  geom_text(data = sig_levels_repeated,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = Visits, \n                # define height of label\n                y = 2, \n                # what are the labels?\n                label = paste0(Significance))) +\n  xlab(\"Visits\") +\n  ylab(\"Measurement\") +\n  # remove legend - not needed here\n  theme(legend.position = \"none\",\n        # make label text vertical, easier to read\n        axis.text.x = element_text(angle = 90, \n                                   # vertical offset of text\n                                   vjust = 0.5, \n                                   # text size\n                                   size = 12))\n\n\n\n\n\n\n\n\n\n\n12.2.3 Similar ANOVA\nMathematically, a factorial ANOVA is the same as a randomized block ANOVA; please see that section for information on how to run this test.\n\n\n12.2.4 ANOVA with interaction\nSometimes when we running a model, we want to look for interactive effects. Interactive effects are situations where one (or both) variables on their own do not effect the data, but there is a cumulative effect between variables that effects things. Let’s look at an example, based on our initial example but with the data altered.\n\nset.seed(8675309)\n\n# we are using data from the randomized black ANOVA again\n\ndata_expanded\n\n  Blocking_Variable Explanatory_1 Explanatory_2 Explanatory_3\n1           Group 1          10.1            12          11.2\n2           Group 2           9.4            13          10.1\n3           Group 3          11.1          15.4          11.9\n\n\n\n### YOU DO NOT NEED TO DO THIS\n### CREATING DATA FOR EXAMPLE\n\ndata_expanded$Explanatory_1 &lt;- as.numeric(data_expanded$Explanatory_1)\ndata_expanded$Explanatory_2 &lt;- as.numeric(data_expanded$Explanatory_2)\ndata_expanded$Explanatory_3 &lt;- as.numeric(data_expanded$Explanatory_3)\n\n# create some pseudorandom data\n# [,-1] excludes first column - group data\ndata_expanded2 &lt;- cbind(Blocking_Variable,\n                        data_expanded[,-1] - 0.75)\ndata_expanded3 &lt;- cbind(Blocking_Variable,\n                        data_expanded[,-1]*1.05)\n\ndata_expanded &lt;- rbind(data_expanded,\n                       data_expanded2,\n                       data_expanded3)\n\n# expand to \"long\" format\ndata &lt;- data_expanded |&gt;\n  # convert to data frame\n  as.data.frame() |&gt;\n  # !by column for aggregating\n  # names_to = what to name column aggregation\n  # values_to = what the measurements should be called\n  pivot_longer(!Blocking_Variable, names_to = \"Treatments\", values_to = \"Measurements\")\n\n# specifying factor to be safe\n\ninteractive_aov &lt;- aov(Measurements ~ factor(Treatments) + \n                         factor(Blocking_Variable) + \n                         factor(Treatments)*factor(Blocking_Variable), \n                       data)\n\nsummary(interactive_aov)\n\n                                             Df Sum Sq Mean Sq F value   Pr(&gt;F)\nfactor(Treatments)                            2  53.28  26.640  59.680 1.14e-08\nfactor(Blocking_Variable)                     2  21.18  10.588  23.719 9.01e-06\nfactor(Treatments):factor(Blocking_Variable)  4   7.39   1.848   4.141    0.015\nResiduals                                    18   8.03   0.446                 \n                                                \nfactor(Treatments)                           ***\nfactor(Blocking_Variable)                    ***\nfactor(Treatments):factor(Blocking_Variable) *  \nResiduals                                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs we can see above, we have very significant effects for Treatment and Blocking_Variable, but a less significant effect for the interaction between the two. Remember - we only need to plot our primary hypothesis. Note however, that Tukey gives us our differences and \\(p\\) values for each set of tests and comparisons:\n\nTukeyHSD(interactive_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Measurements ~ factor(Treatments) + factor(Blocking_Variable) + factor(Treatments) * factor(Blocking_Variable), data = data)\n\n$`factor(Treatments)`\n                                  diff         lwr       upr     p adj\nExplanatory_2-Explanatory_1  3.3211111  2.51730768  4.124915 0.0000000\nExplanatory_3-Explanatory_1  0.8811111  0.07730768  1.684915 0.0304521\nExplanatory_3-Explanatory_2 -2.4400000 -3.24380343 -1.636197 0.0000011\n\n$`factor(Blocking_Variable)`\n                      diff        lwr       upr     p adj\nGroup 2-Group 1 -0.2711111 -1.0749145 0.5326923 0.6710682\nGroup 3-Group 1  1.7283333  0.9245299 2.5321368 0.0000928\nGroup 3-Group 2  1.9994444  1.1956410 2.8032479 0.0000159\n\n$`factor(Treatments):factor(Blocking_Variable)`\n                                                     diff         lwr\nExplanatory_2:Group 1-Explanatory_1:Group 1  1.931667e+00  0.02027781\nExplanatory_3:Group 1-Explanatory_1:Group 1  1.118333e+00 -0.79305553\nExplanatory_1:Group 2-Explanatory_1:Group 1 -7.116667e-01 -2.62305553\nExplanatory_2:Group 2-Explanatory_1:Group 1  2.948333e+00  1.03694447\nExplanatory_3:Group 2-Explanatory_1:Group 1 -1.776357e-15 -1.91138886\nExplanatory_1:Group 3-Explanatory_1:Group 1  1.016667e+00 -0.89472219\nExplanatory_2:Group 3-Explanatory_1:Group 1  5.388333e+00  3.47694447\nExplanatory_3:Group 3-Explanatory_1:Group 1  1.830000e+00 -0.08138886\nExplanatory_3:Group 1-Explanatory_2:Group 1 -8.133333e-01 -2.72472219\nExplanatory_1:Group 2-Explanatory_2:Group 1 -2.643333e+00 -4.55472219\nExplanatory_2:Group 2-Explanatory_2:Group 1  1.016667e+00 -0.89472219\nExplanatory_3:Group 2-Explanatory_2:Group 1 -1.931667e+00 -3.84305553\nExplanatory_1:Group 3-Explanatory_2:Group 1 -9.150000e-01 -2.82638886\nExplanatory_2:Group 3-Explanatory_2:Group 1  3.456667e+00  1.54527781\nExplanatory_3:Group 3-Explanatory_2:Group 1 -1.016667e-01 -2.01305553\nExplanatory_1:Group 2-Explanatory_3:Group 1 -1.830000e+00 -3.74138886\nExplanatory_2:Group 2-Explanatory_3:Group 1  1.830000e+00 -0.08138886\nExplanatory_3:Group 2-Explanatory_3:Group 1 -1.118333e+00 -3.02972219\nExplanatory_1:Group 3-Explanatory_3:Group 1 -1.016667e-01 -2.01305553\nExplanatory_2:Group 3-Explanatory_3:Group 1  4.270000e+00  2.35861114\nExplanatory_3:Group 3-Explanatory_3:Group 1  7.116667e-01 -1.19972219\nExplanatory_2:Group 2-Explanatory_1:Group 2  3.660000e+00  1.74861114\nExplanatory_3:Group 2-Explanatory_1:Group 2  7.116667e-01 -1.19972219\nExplanatory_1:Group 3-Explanatory_1:Group 2  1.728333e+00 -0.18305553\nExplanatory_2:Group 3-Explanatory_1:Group 2  6.100000e+00  4.18861114\nExplanatory_3:Group 3-Explanatory_1:Group 2  2.541667e+00  0.63027781\nExplanatory_3:Group 2-Explanatory_2:Group 2 -2.948333e+00 -4.85972219\nExplanatory_1:Group 3-Explanatory_2:Group 2 -1.931667e+00 -3.84305553\nExplanatory_2:Group 3-Explanatory_2:Group 2  2.440000e+00  0.52861114\nExplanatory_3:Group 3-Explanatory_2:Group 2 -1.118333e+00 -3.02972219\nExplanatory_1:Group 3-Explanatory_3:Group 2  1.016667e+00 -0.89472219\nExplanatory_2:Group 3-Explanatory_3:Group 2  5.388333e+00  3.47694447\nExplanatory_3:Group 3-Explanatory_3:Group 2  1.830000e+00 -0.08138886\nExplanatory_2:Group 3-Explanatory_1:Group 3  4.371667e+00  2.46027781\nExplanatory_3:Group 3-Explanatory_1:Group 3  8.133333e-01 -1.09805553\nExplanatory_3:Group 3-Explanatory_2:Group 3 -3.558333e+00 -5.46972219\n                                                    upr     p adj\nExplanatory_2:Group 1-Explanatory_1:Group 1  3.84305553 0.0464895\nExplanatory_3:Group 1-Explanatory_1:Group 1  3.02972219 0.5317567\nExplanatory_1:Group 2-Explanatory_1:Group 1  1.19972219 0.9173224\nExplanatory_2:Group 2-Explanatory_1:Group 1  4.85972219 0.0010200\nExplanatory_3:Group 2-Explanatory_1:Group 1  1.91138886 1.0000000\nExplanatory_1:Group 3-Explanatory_1:Group 1  2.92805553 0.6439270\nExplanatory_2:Group 3-Explanatory_1:Group 1  7.29972219 0.0000003\nExplanatory_3:Group 3-Explanatory_1:Group 1  3.74138886 0.0667267\nExplanatory_3:Group 1-Explanatory_2:Group 1  1.09805553 0.8457970\nExplanatory_1:Group 2-Explanatory_2:Group 1 -0.73194447 0.0032271\nExplanatory_2:Group 2-Explanatory_2:Group 1  2.92805553 0.6439270\nExplanatory_3:Group 2-Explanatory_2:Group 1 -0.02027781 0.0464895\nExplanatory_1:Group 3-Explanatory_2:Group 1  0.99638886 0.7519790\nExplanatory_2:Group 3-Explanatory_2:Group 1  5.36805553 0.0001577\nExplanatory_3:Group 3-Explanatory_2:Group 1  1.80972219 0.9999999\nExplanatory_1:Group 2-Explanatory_3:Group 1  0.08138886 0.0667267\nExplanatory_2:Group 2-Explanatory_3:Group 1  3.74138886 0.0667267\nExplanatory_3:Group 2-Explanatory_3:Group 1  0.79305553 0.5317567\nExplanatory_1:Group 3-Explanatory_3:Group 1  1.80972219 0.9999999\nExplanatory_2:Group 3-Explanatory_3:Group 1  6.18138886 0.0000097\nExplanatory_3:Group 3-Explanatory_3:Group 1  2.62305553 0.9173224\nExplanatory_2:Group 2-Explanatory_1:Group 2  5.57138886 0.0000766\nExplanatory_3:Group 2-Explanatory_1:Group 2  2.62305553 0.9173224\nExplanatory_1:Group 3-Explanatory_1:Group 2  3.63972219 0.0947841\nExplanatory_2:Group 3-Explanatory_1:Group 2  8.01138886 0.0000000\nExplanatory_3:Group 3-Explanatory_1:Group 2  4.45305553 0.0047491\nExplanatory_3:Group 2-Explanatory_2:Group 2 -1.03694447 0.0010200\nExplanatory_1:Group 3-Explanatory_2:Group 2 -0.02027781 0.0464895\nExplanatory_2:Group 3-Explanatory_2:Group 2  4.35138886 0.0069891\nExplanatory_3:Group 3-Explanatory_2:Group 2  0.79305553 0.5317567\nExplanatory_1:Group 3-Explanatory_3:Group 2  2.92805553 0.6439270\nExplanatory_2:Group 3-Explanatory_3:Group 2  7.29972219 0.0000003\nExplanatory_3:Group 3-Explanatory_3:Group 2  3.74138886 0.0667267\nExplanatory_2:Group 3-Explanatory_1:Group 3  6.28305553 0.0000070\nExplanatory_3:Group 3-Explanatory_1:Group 3  2.72472219 0.8457970\nExplanatory_3:Group 3-Explanatory_2:Group 3 -1.64694447 0.0001097\n\n\nI do not plot this here, but it would be similar to the other parts of this test.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#friedmans-test",
    "href": "anova2.html#friedmans-test",
    "title": "12  ANOVA: Part 2",
    "section": "12.3 Friedman’s test",
    "text": "12.3 Friedman’s test\n\n12.3.1 Using R\nFriedman’s test is a non-parametric alternative to a two-way ANOVA, so as you would guess, it can be painful to implement. We will use an altered version of the same test we’ve used before:\n\n# set seed - make reproducible\nset.seed(8675309)\n\n### DO NOT NEED TO REPEAT THIS\n### CREATING DATA FOR EXAMPLE \n\n# new set of foods - this time, ten of them\nTreatments &lt;- c(paste(\"Treatment\",1:10)) |&gt;\n  as.factor()\n\n# pre-created data frame of locations from earlier\nBlocking_Factor &lt;- c(paste(\"Block\", 1:10)) |&gt;\n  as.factor()\n\nlong_data &lt;- crossing(Blocking_Factor, Treatments)\n\nlong_data$Measurements &lt;- NA\n\nfor(i in 1:length(unique(long_data$Treatments))){\n  subset_rows &lt;- which(long_data$Treatments==long_data$Treatments[i])\n  long_data$Measurements[subset_rows] &lt;- runif(n = length(subset_rows),\n                                               min = i-2, max = i+2) |&gt; \n    round(1)\n}\n\nlong_data\n\n# A tibble: 100 × 3\n   Blocking_Factor Treatments   Measurements\n   &lt;fct&gt;           &lt;fct&gt;               &lt;dbl&gt;\n 1 Block 1         Treatment 1          -0.4\n 2 Block 1         Treatment 10          3.4\n 3 Block 1         Treatment 2           1.2\n 4 Block 1         Treatment 3           5.9\n 5 Block 1         Treatment 4           6.9\n 6 Block 1         Treatment 5           7  \n 7 Block 1         Treatment 6           7.6\n 8 Block 1         Treatment 7           7.6\n 9 Block 1         Treatment 8           8.1\n10 Block 1         Treatment 9           9.8\n# ℹ 90 more rows\n\n\nNow that we have our expanded and randomized table, we can get started with our test.\nOur calculation for the Friedman’s test statistic \\(Q\\) (not to be confused with Tukey’s \\(q\\)!) is: \\[Q = \\frac{12}{nk(k+1)} \\cdot \\Sigma R_j^2 - 3n(k+1)\\]\nwhere \\(n\\) is the total number of individuals in each sample in the dataset, \\(k\\) is the number of groups, and \\(R_j^2\\) is the sum of the ranks.\nIn this class, we will do this in R.\n\nfriedman_long_data &lt;- friedman.test(y = long_data$Measurements,\n                                    groups = long_data$Treatments,\n                                    blocks = long_data$Blocking_Factor)\n\nprint(friedman_long_data)\n\n\n    Friedman rank sum test\n\ndata:  long_data$Measurements, long_data$Treatments and long_data$Blocking_Factor\nFriedman chi-squared = 79.983, df = 9, p-value = 1.629e-13\n\n\nNote you will get a different answer if you are switching the blocks and the groups.\nWe can use the following, from package PMCMRplus, to find the adjacent and non-adjacent groups.\n\n# find differences\nfrdAllPairsConoverTest(y = long_data$Measurements,\n                       groups = long_data$Treatments,\n                       blocks = long_data$Blocking_Factor, \n                       p.adjust.method = \"bonf\")\n\n\n    Pairwise comparisons using Conover's all-pairs test for a two-way balanced complete block design\n\n\ndata: y, groups and blocks\n\n\n             Treatment 1 Treatment 10 Treatment 2 Treatment 3 Treatment 4\nTreatment 10 1.00000     -            -           -           -          \nTreatment 2  1.00000     1.00000      -           -           -          \nTreatment 3  0.00094     0.10167      0.80301     -           -          \nTreatment 4  3.8e-09     1.6e-06      3.4e-05     0.19031     -          \nTreatment 5  3.2e-11     1.6e-08      4.0e-07     0.00637     1.00000    \nTreatment 6  &lt; 2e-16     9.2e-16      2.5e-14     1.5e-09     0.00042    \nTreatment 7  &lt; 2e-16     &lt; 2e-16      3.6e-16     2.0e-11     9.3e-06    \nTreatment 8  &lt; 2e-16     &lt; 2e-16      &lt; 2e-16     6.1e-15     3.8e-09    \nTreatment 9  &lt; 2e-16     &lt; 2e-16      &lt; 2e-16     &lt; 2e-16     1.1e-12    \n             Treatment 5 Treatment 6 Treatment 7 Treatment 8\nTreatment 10 -           -           -           -          \nTreatment 2  -           -           -           -          \nTreatment 3  -           -           -           -          \nTreatment 4  -           -           -           -          \nTreatment 5  -           -           -           -          \nTreatment 6  0.01886     -           -           -          \nTreatment 7  0.00063     1.00000     -           -          \nTreatment 8  4.0e-07     0.34617     1.00000     -          \nTreatment 9  1.4e-10     0.00094     0.02676     1.00000    \n\n\n\nP value adjustment method: bonferroni\n\n\nAs we can see, some pairs are inseparable and others are separable. We can now plot as for the other problems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#homework-chapter-12",
    "href": "anova2.html#homework-chapter-12",
    "title": "12  ANOVA: Part 2",
    "section": "12.4 Homework: Chapter 12",
    "text": "12.4 Homework: Chapter 12\nFor problems 12.1, 12.2, 12.3, 12.4, and 12.5, state your hypotheses in sentence form and mathematically. Then, identify the appropriate ANOVA and perform the analysis. If you reject the null, complete a Tukey test and plot your results, showing letters denoting each group. Note that 12.4 requires a Friedman’s test, but all other problems require some form of ANOVA.\nNext, for problems 12.7, 12.8, and 12.9, identify the appropriate test and justify your reasoning. State the null and alternative hypothesis in word form and mathematically, and perform your analysis. If you perform an ANOVA and you reject the null hypothesis, plot your results and label the groups by letter.\nRemember, only plot the results if you reject your primary hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "cor_reg.html",
    "href": "cor_reg.html",
    "title": "13  Correlation & regression",
    "section": "",
    "text": "13.1 Introduction\nWhen we are comparing two continuous variables, we use two forms of tests: correlation to understand if there is a relationship between two variables, and linear regression to determine what that relationships is.\nRemember - “if” is always correlation, and “what is it” is always linear regression when choosing a test for an exam.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#correlation",
    "href": "cor_reg.html#correlation",
    "title": "13  Correlation & regression",
    "section": "13.2 Correlation",
    "text": "13.2 Correlation\nCorrelation - denoted by \\(\\rho\\) (“rho”) and not to be confused with \\(p\\) - is a measure of how closely related to continuous variables appear to be. This can vary from a purely negative relationship to a purely positive relationship, such that \\(-1 \\le \\rho \\le 1\\) with \\(\\rho = 0\\) indicating a random relationship between data.\nWe can visualize these as follows:\n\n### ILLUSTRATIVE PURPOSES ONLY\n\n# create two random uniform distributions\ny &lt;- runif(10)\nx &lt;- runif(10)\n\nFor example, two things compared to themselves have a \\(\\rho = 1\\).\n\nplot(y, y, pch = 19)\n\n\n\n\n\n\n\n\n\ncor(y, y)\n\n[1] 1\n\n\nAs we can see above, the correlation is 1.\nPlotting by the negative will be a correlation of -1.\n\nplot(y, -y, pch = 19)\n\n\n\n\n\n\n\n\n\ncor(y, -y)\n\n[1] -1\n\n\nLastly, two random variables plotted against each other should have \\(\\rho \\approx 0\\).\n\nplot(y, x, \n     pch = 19, \n     asp = 1) # aspect ratio\n\n\n\n\n\n\n\n\n\ncor(y, x) |&gt; \n  round(2)\n\n[1] -0.2\n\n\n\n13.2.1 Pearson’s\nPearson’s correlation coefficient is our value for parametric tests. We often denote our correlation coefficient as \\(r\\) and not \\(\\rho\\) for this particular test. It is calculated as follows:\n\\[\nr = \\frac{\\Sigma xy - (\\frac{\\Sigma x \\Sigma y}{n})}{\\sqrt{(\\Sigma x^2 - \\frac{(\\Sigma x)^2}{n}})(\\Sigma y^2 - \\frac{(\\Sigma y)^2}{n})}\n\\]\nwhere \\(x\\) is variable 1, \\(y\\) is variable 2, and \\(n\\) is the total number of data point pairs.\nIn this class, we will be using R to calculate \\(r\\), which is done using the command cor. To ensure we are using the correct method, we need to set method = \"pearson\".\n\nset.seed(8675309)\n\n### EXAMPLE DATA\nx &lt;- c(1,2,5,3,4,5,8,7,9,6,10,12,15,20,25)\ny &lt;- c(2,5,4,3,8,6,4,2,8,9,15,13,10,18,19)\n\nplot(x, y, pch = 19)\n\n\n\n\n\n\n\n\n\ncor(x, y, method = \"pearson\") |&gt; \n  round(2)\n\n[1] 0.86\n\n\nAs we can see, these data are fairly positively correlated. As x increases, so does y. But how significant is this relationship?\nWell, we can calculate two things - the amount of variation explained, which is \\(r^2\\), and the significance of the relationships, which is determined via a \\(t\\) test and the equation \\(t=r \\sqrt{\\frac{n-2}{1-r^2}}\\). This is a two-tailed distribution, with \\(df = n-2\\).\nWe can write a function to perform all of these options:\n\nbiol305_cor &lt;- function(x=NA, y=NA, method = \"pearson\"){\n  if(is.data.frame(x)==T){\n    if(ncol(x)==2){\n      r &lt;- cor(x[,1], x[,2], method = method)\n    }else{\n      r &lt;- cor(x, method = method)\n    }\n\n    r2 &lt;- r\n    r[r==1|r==-1] &lt;- 0\n    \n    n &lt;- 2*nrow(x)\n  }else{\n    r &lt;- cor(x, y, method = method)\n    \n    n &lt;- 2*length(x)\n  }\n  \n  t_val &lt;- r*sqrt((n-2)/(1-r^2))\n  \n  p &lt;- pt(t_val, df = n - 2)\n  \n  p[p &gt; 0.5] &lt;- 1 - p[p &gt; 0.5]\n  p[p &gt; 0.005] &lt;- round(p[p &gt; 0.005],2)\n  p[p &gt; 0.0005] &lt;- round(p[p &gt; 0.0005],3)\n  p[p &gt; 0.00005] &lt;- round(p[p &gt; 0.00005],4)\n  p[p &lt; 0.00005] &lt;- \"&lt; 0.0001\"\n  \n  if(is.data.frame(x)==T){\n    print(\"Correlation:\")\n    print(round(r, 2))\n    if(ncol(x) == 2){\n      print(paste0(\"Degrees of freedom: \", n - 2))\n      print(paste0(\"t value: \", round(t_val, 2)))\n      print(paste0(\"P value: \", p))\n    }\n    if(ncol(x) &gt; 2){\n      print(paste0(\"Degrees of freedom: \", n - 2))\n      print(\"\")\n      print(\"t value: \")\n      print(round(t_val, 2))\n      print(\"\")\n      print(\"P value: \")\n      print(p)\n    }\n  }else{\n    print(paste0(\"Correlation: \", round(r, 2)))\n    print(paste0(\"Degrees of freedom: \", n - 2))\n    print(paste0(\"t value: \", round(t_val, 2)))\n    print(paste0(\"P value: \", p))\n  }\n}\n\nLet’s test our function.\n\nbiol305_cor(x, y, method = \"pearson\")\n\n[1] \"Correlation: 0.86\"\n[1] \"Degrees of freedom: 28\"\n[1] \"t value: 8.93\"\n[1] \"P value: &lt; 0.0001\"\n\n\nThere we go! Our function printed out everything that we need.\n\n\n13.2.2 Spearman’s\nSpearman’s correlation is one of the non-parametric methods for our correlation tests. We can use this for ranked data or for non-parametric datasets. We do this the exact same way, except we change method = \"spearman\".\n\n\n13.2.3 Other non-parametric methods\nTo be expanded upon, but not necessary for the class at present.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#regression",
    "href": "cor_reg.html#regression",
    "title": "13  Correlation & regression",
    "section": "13.3 Regression",
    "text": "13.3 Regression\nRegression is used when we want to know what the relationship is between two variables. Regression operates similar to ANOVA and correlation, providing us with the nature of the relationship, the strength of the relationship, and gives us values for calculating the relationship. For this class, we are only focusing on linear regression for relationships between linear variables.\nThe equation for a regression line is often written as \\(y_i = \\alpha + \\beta x_i + e_i\\), where \\(\\alpha\\) is the \\(y\\) intercept, \\(\\beta\\) is the slope, and \\(e\\) is the error around each point. We will not perform regression calculations by hand in this class.\n\n13.3.1 Parametric\nWe will use out previous established x and y datasets that are strongly positively correlated for this example. The equation for calculating a linear relationship is lm, which stands for “linear model”. This uses equations like ANOVA, but can also use two vectors of data.\n\nxy_linear &lt;- lm(y ~ x)\n\nsummary(xy_linear)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1189 -1.4838 -0.5423  1.9757  5.7460 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1370     1.2825   1.666     0.12    \nx             0.7117     0.1169   6.086 3.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.964 on 13 degrees of freedom\nMultiple R-squared:  0.7402,    Adjusted R-squared:  0.7202 \nF-statistic: 37.04 on 1 and 13 DF,  p-value: 3.867e-05\n\n\nAs we can see, this returned an ANOVA table to use that tells us the value and significance of our intercept as well as the value and significance of the the slope (here, shown as x; it will always show the explanatory variable in this slot for the name).\nLooking at the above, we can see that the slope is not significantly non-zero with a \\(p = 0.12\\), but that the slope is significantly non-zero with \\(p &lt; 0.0001\\). We also have our \\(R^2\\) values returned, which is similar to the \\(r\\) we got for correlation. Indeed, our correlation was \\(r = 0.86\\), with \\(r^2 = 0.74\\), which is very similar to the Multiple R-squared shown in the above ANOVA table.\nR has a built in function within ggplot that will add a linear model to our plot and will show error regions as well. First, we need to make sure our data are in a data.frame.\n\ndata &lt;- data.frame(x, y)\n\ndata\n\n    x  y\n1   1  2\n2   2  5\n3   5  4\n4   3  3\n5   4  8\n6   5  6\n7   8  4\n8   7  2\n9   9  8\n10  6  9\n11 10 15\n12 12 13\n13 15 10\n14 20 18\n15 25 19\n\n\nNext, we can plot the data in ggplot.\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") + \n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nJust like that, we have created a plot of our linear regression. Note that you should always plot the lines only within the extent of the data; this is harder to do in other programs, but R does it for us!\nR can also allow us to predict different values using our linear model:\n\n# must be in data frame format\ntest_data &lt;- data.frame(1:25)\n\ncolnames(test_data) &lt;- \"x\" # must be same as explanatory in data\n\npredict(xy_linear, test_data)\n\n        1         2         3         4         5         6         7         8 \n 2.848692  3.560399  4.272105  4.983811  5.695517  6.407223  7.118929  7.830635 \n        9        10        11        12        13        14        15        16 \n 8.542341  9.254047  9.965753 10.677460 11.389166 12.100872 12.812578 13.524284 \n       17        18        19        20        21        22        23        24 \n14.235990 14.947696 15.659402 16.371108 17.082814 17.794521 18.506227 19.217933 \n       25 \n19.929639 \n\n\nLet’s visualize these points for illustration’s sake.\n\nplot(x, y, pch = 19)\npoints(1:25, as.numeric(predict(xy_linear, test_data)),\n       pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\nAs we can see above, our points follow the line from the plot. By using this format, however, we can make predictions of value for any point we want.\n\n\n13.3.2 Non-parametric\nWe are not doing non-parametric linear regression in this class.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#homework",
    "href": "cor_reg.html#homework",
    "title": "13  Correlation & regression",
    "section": "13.4 Homework",
    "text": "13.4 Homework\n\n13.4.1 Chapter 13\n\n\n13.4.2 Chapter 14",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "picktest.html",
    "href": "picktest.html",
    "title": "14  Pick the test",
    "section": "",
    "text": "14.1 Picking the test\nA major component of the final will be picking the correct test to run on some data. Here, I cover the specific ways in which you can determine what test to use.\nPlease use this web page as an interactive way to pick the test.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "picktest.html#exceptions",
    "href": "picktest.html#exceptions",
    "title": "14  Pick the test",
    "section": "14.2 Exceptions",
    "text": "14.2 Exceptions\nBelow is a walk through for the most common statistical analyses, but keep in mind there are a few “less common” ones that we are using as well:\n\nBinomial test - if we are looking at something with discrete outcomes - like coin tosses, die rolls, etc. - we are doing a binomial test to determine the probability of a specific outcome. You can do this with binom.test.\nPoisson test - if we are looking at the probability of obtaining certain counts over events - specifically, looking at the probability of rare events - we will use a Poisson. You can do this with poisson.test.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "picktest.html#overview-of-picking-the-test",
    "href": "picktest.html#overview-of-picking-the-test",
    "title": "14  Pick the test",
    "section": "14.3 Overview of picking the test",
    "text": "14.3 Overview of picking the test\nBelow is an overview of picking basic statistical tests. There are more complex tests, but these are the main ones to consider for exams in this course.\n\nWhich test to pick for which combinations of variables.\n\n\n\n\n\n\n\n\nCategorical Explanatory\nContinuous Explanatory\n\n\n\n\nCategorical Response\n\n\\(\\chi^2\\) tests, especially if count data for category\n\nYate’s correction auto-applied for 2x2 tables\n\nFisher test is a 2x2 table\nNull is that counts match a known proportion (goodness-of-fit) or counts match each other (independence)\n\n\nLogistic regression, modeling categorical responses across continuous variables.\n\nNot covered in BIOL 305 at present\n\n\n\n\nContinuous Response\n\nSingle mean to population\n\nThis is a \\(Z\\)-score comparison, when the population parameters are known\n\n\\(H_0: \\bar{x} = \\mu\\)\n\nAlmost always better to resort to \\(t\\)-test comparison if unsure is a population - \\(t\\) approaches a \\(Z\\) as \\(df \\rightarrow \\infty\\)\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\nTwo measurements\n\nRemember - all \\(t\\)-tests default to Welch’s in R, assuming unequal variance\n\nWhen \\(\\sigma^2_1 = \\sigma^2_2\\), Welch’s \\(t\\)-test is the same as a standard \\(t\\)-test\nUsually better to use Welch’s, but justify reasoning\n\nOne-sample \\(t\\)-test, comparing the two groups\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\nTwo samples\n\nTwo-sample \\(t\\)-test, comparing the two samples\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\nPaired samples\n\nSpecial case - used when there are repeated measurements of the same sampling units\n\nNeed to set paired = TRUE\n\\(H_0: \\mu_d = 0\\)\n\n\n\\(\\ge\\) 3 samples\n\nANOVA - analysis of variance across multiple samples, computing variance within samples and between samples\n\nFor all ANOVA, \\(H_0: \\mu_1 = \\mu_2 = ... \\mu_i\\)\nSingle type of measurement is a one-way ANOVA\nWhen things are “blocked” into categories - like litters - that are unique per row, use a randomized block ANOVA (remember to put factor() around anything you are worried won’t be read as categorical!)\n\naov(response ~ explanatory + block, data)\n\nWhen things are individuals being repeatedly measured, use a repeated measures ANOVA (mathematically like randomized block ANOVA)\n\naov(response ~ explanatory + individual, data)\n\nWhen we are looking at more than one variable at once, you are doing an interactive ANOVA\n\naov(response ~ explanatory1 + explanatory2 + explanatory1*explanatory2)\n\n\n\n\n\nWhen looking at if there is a relationship, use correlation\n\n\\(H_0: \\rho = 0\\)\nEvaluated using a \\(t\\) statistic\n\nWhen looking at what the relationship is, use linear regression\n\nLine formula is \\(y = \\beta x + \\alpha + \\epsilon_i\\)\nVery similar to your high-school \\(y = mx +b\\)\n\\(H_0: \\beta = 0\\)\n\nRemember - linear regression works like an ANOVA, and has similar outputs\n\nCheck the ANOVA page for more information!",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "picktest.html#another-method---checklist",
    "href": "picktest.html#another-method---checklist",
    "title": "14  Pick the test",
    "section": "14.4 Another method - checklist",
    "text": "14.4 Another method - checklist\nFeel free to go through the following headings to also help you pick a test.\n\n14.4.1 Explanatory Variable\nThe explanatory variable is continuous and numeric. Pick this one for a continuous measurement variable, for example, such as Longitude, concentration, or other ratio or integer data.\nThe explanatory variable is discontinuous and categorical. In these cases, the explanatory variable is a condition, like a control and a treatment.\n\n\n14.4.2 Continuous explanatory variable\n\n14.4.2.1 Continuous response variable\nIf you have a continuous response variable, then you need to see what the question is asking:\n\nIs there a relationship?\n\nIf you are looking at a problem and it is simply asking if there is a relationship, you are looking at a correlation analysis.\n\nWhat is the relationship?\n\nIf you are asking what the relationship is or looking to be able to predict a value based on what you know, you are doing a linear regression. Note there are other kinds of regression, but in this class, we focus on linear regression.\nIf we have multiple response variables, we can do multiple regression, which we do not cover here.\n\n\n14.4.2.2 Discrete response variable\nIf you are looking at a discrete response variable, such as a state of 1 or 0 in response to a certain amount of stimulus, then you are doing a logistic regression. We did not cover this analysis in this class.\n\n\n\n14.4.3 Discrete explanatory variable\nFor discrete explanatory variables, we are often looking at categorical treatments or distinct groups, like species or geographic locations.\n\n14.4.3.1 Continuous response variable\nFor a continuous response variable, we need to ask ourselves how many categories we are dealing with.\n\nIf we are dealing with two categories or two measurements from the same individual, we are using a \\(t\\)-test.\n\nMake sure you check the t-test page to understand what kind of t-test is being performed. For repeated measurements from the same individuals or populations under different conditions, we have the paired t-test. Otherwise, we have the Welch’s t-test as the default in R. NOTE that the default assumes unequal variance; you must set var.equal = TRUE to perform a “true” t-test. Make sure you familiarize yourself with why our code is assuming variances are unequal.\nIf we know what the population is, we will use a \\(Z\\)-score, but bear in mind that we will almost always use a \\(t\\)-test to account for error. A \\(t\\)-test with infinite degrees of freedom is the same as a \\(Z\\)-score, so it is better to default to a \\(t\\)-test.\n\nIf we have three or more categories or treatments then we need to perform an ANOVA.\n\nIf we are simply comparing multiple groups, we are performing a one-way ANOVA. We also need to make sure we aren’t doing some sort of factorial ANOVA, repeated-measures ANOVA, or interactive ANOVA. Please read the ANOVA page to ensure you are using the correct format.\nRemember to label ANOVA plots with letters to indicate the separate groups.\nIf we have multiple response variables, we can use a MANOVA; we do not cover that in this class.\n\n\n14.4.3.2 Discrete response variable\nIf our response variable and our explanatory variable are discrete (i.e., categorical or nominal), then we are doing a \\(\\chi^2\\) test. This tests looks at counts in different categories. For example, looking at proportions of men and women who do and do not smoke would be a classic \\(\\chi^2\\) test.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "final_exam.html",
    "href": "final_exam.html",
    "title": "15  Final exam & review",
    "section": "",
    "text": "15.1 Introduction\nThese are questions that are posed as they would be on a final. Please complete each part of each question; we will review the answers. The following data are either imaginary or pulled from publicly available sources like Wikipedia.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "final_exam.html#happiness",
    "href": "final_exam.html#happiness",
    "title": "15  Final exam & review",
    "section": "15.2 Happiness",
    "text": "15.2 Happiness\nYou decide to ask your friends to rate their happiness on a scale of 0-100 before and after you bother them to ask how happy they are, thereby inciting an existential crises within each of them. You predict that asking this question will decrease the happiness they are feeling at that moment. These data are ranked data, but assume they are parametric given the size of the scale being used.\n\nbefore_asking &lt;- c(96,80,86,92,100,92,95,91,87)\nafter_asking &lt;- c(72,53,90,85,90,86,83,79,62)\n\nhappiness &lt;- cbind(before_asking, after_asking) |&gt; \n  as.data.frame()\n\nhappiness\n\n  before_asking after_asking\n1            96           72\n2            80           53\n3            86           90\n4            92           85\n5           100           90\n6            92           86\n7            95           83\n8            91           79\n9            87           62\n\n\n\n15.2.1 What are the null and statistical hypotheses of this study?\n\n\n15.2.2 What statistical test should be used for this test? Justify your answer, and be specific.\n\n\n15.2.3 Calculate the appropriate test statistic, showing all or your work.\n\n\n15.2.4 Assume that \\(\\alpha = 0.05\\). State your final conclusion from the survey.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "final_exam.html#running-and-grades",
    "href": "final_exam.html#running-and-grades",
    "title": "15  Final exam & review",
    "section": "15.3 Running and grades",
    "text": "15.3 Running and grades\nYou hear that going running is good for your grades. You decide to look and see if the amount of time people spend running is related to their grades. You obtain the following data:\n\nrun_time &lt;- c(20,25,22,50,30,10,6)\ngrade &lt;- c(94,84,95,72,88,90,85)\n\nrun_grades &lt;- cbind(run_time, grade) |&gt; \n  as.data.frame()\n\nrun_grades\n\n  run_time grade\n1       20    94\n2       25    84\n3       22    95\n4       50    72\n5       30    88\n6       10    90\n7        6    85\n\n\n\n15.3.1 What is the appropriate analysis for this question?\n\n\n15.3.2 Perform the test. What is the test statistic?\n\n\n15.3.3 State your conclusion about this scenario, using \\(\\alpha = 0.05\\).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "final_exam.html#fosbury-flop",
    "href": "final_exam.html#fosbury-flop",
    "title": "15  Final exam & review",
    "section": "15.4 Fosbury Flop",
    "text": "15.4 Fosbury Flop\nHigh-jumpers use the “Fosbury Flop” because it improves their performance by allowing their center of mass to pass under the high-jump bar while their bodies pass over the bar. Below are jump heights (in meters) for world records from before the Fosbury Flop was widely used and after the Fosbury Flop was widely used. Does the flop significantly improve athlete performace?\n\npre_flop &lt;- c(2.09,2.12,2.15,2.18,2.17,2.28,2.29)\npost_flop &lt;- c(2.30,2.33,2.39,2.34,2.42,2.45,2.44)\n\njump_heights &lt;- cbind(pre_flop, post_flop) |&gt; \n  as.data.frame()\n\njump_heights\n\n  pre_flop post_flop\n1     2.09      2.30\n2     2.12      2.33\n3     2.15      2.39\n4     2.18      2.34\n5     2.17      2.42\n6     2.28      2.45\n7     2.29      2.44\n\n\n\n15.4.1 What is the appropriate test for this analysis? Justify your answer, and be specific.\n\n\n15.4.2 Perform the test. What is the test statistic?\n\n\n15.4.3 State your conclusion about this scenario. Set \\(\\alpha = 0.05\\).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "final_exam.html#sandhills-stonehills",
    "href": "final_exam.html#sandhills-stonehills",
    "title": "15  Final exam & review",
    "section": "15.5 Sandhills, Stonehills",
    "text": "15.5 Sandhills, Stonehills\nYou decide to look at concentrations of Greater Prairie-Chickens Tympanuchus cupido in fields a set amount of years after control burns. The following table shows the count of prairie-chickens in each field from the year of the burn until 5 years after the burn. Conditions are lettered to ensure sorting is performed correctly.\n\nLocation &lt;- c(rep(\"Sandhills\", 4), rep(\"Stonehills\", 4))\na_burn_year &lt;- c(1,0,3,2,1,0,0,2)\nb_one_year_post_burn &lt;- c(3,5,7,6,5,3,2,5)\nc_five_years_post_burn &lt;- c(20,21,15,8,8,7,10,8)\n\nprairie_chickens &lt;- cbind(Location, a_burn_year, b_one_year_post_burn, c_five_years_post_burn) |&gt; \n  as.data.frame()\n\nprairie_chickens\n\n    Location a_burn_year b_one_year_post_burn c_five_years_post_burn\n1  Sandhills           1                    3                     20\n2  Sandhills           0                    5                     21\n3  Sandhills           3                    7                     15\n4  Sandhills           2                    6                      8\n5 Stonehills           1                    5                      8\n6 Stonehills           0                    3                      7\n7 Stonehills           0                    2                     10\n8 Stonehills           2                    5                      8\n\n\n\n15.5.1 What is the appropriate test?\n\n\n15.5.2 What is / are the explanatory variables?\n\n\n15.5.3 Perform the appropriate test. Make a graph if necessary.\n\n\n15.5.4 State your conclusions about this test. Set \\(\\alpha = 0.05\\).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final exam & review</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "16  Conclusions",
    "section": "",
    "text": "16.1 Parting thoughts\nIn this class, we have covered two major things: (1) the basics of statistics for biological research and (2) the basics of using R to solve different computational problems. It is my hope that this class helps you both become a better researcher and also a more efficient researcher and student by using code to help you with your future projects.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusion.html#ꮩꮣꮣꭺꮂꭲ",
    "href": "conclusion.html#ꮩꮣꮣꭺꮂꭲ",
    "title": "16  Conclusions",
    "section": "16.2 ᏙᏓᏓᎪᎲᎢ",
    "text": "16.2 ᏙᏓᏓᎪᎲᎢ\nᏙᏓᏓᎪᎲᎢ (pronounced doh-dah-dah-go-huh-ee) is a traditional Cherokee farewell. It does not mean goodbye, but rather reflects a parting of ways until a group of folks meet again.\nI enjoyed getting to know all of you in class, and please feel free to reach out or stop by and say hi if you are ever passing through Kearney in the future or if you need help with something biology related.\nWishing you the best,\nDr. Cooper",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "17  Functions & Glossary",
    "section": "",
    "text": "17.1 Common Commands\nThe following are common useful commands used in R, with examples of their use.\nCommands are given as Windows & Linux / Mac.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#common-commands",
    "href": "glossary.html#common-commands",
    "title": "17  Functions & Glossary",
    "section": "",
    "text": "17.1.1 The basics\n\nSave your file with CTRL + S / ⌘ + S\nUndo your last action with CTRL + Z / ⌘ + Z\nCut text with CTRL + X / ⌘ + X\nCopy text with CTRL + C / ⌘ + C\nPaste text with CTRL + V / ⌘ + V\nSelect all text in a document with CTRL + A / ⌘ + A\n\n\n\n17.1.2 Code basics\n\nA code chunk can be inserted using CTRL + SHIFT + I / ⌘ + shift + I\nRun a line of code with CTRL + ENTER / ⌘ + return\nRun a code chunk with CTRL + SHIFT + ENTER / ⌘ + shift + return\nRun all code in the document above where you were CTRL + SHIFT + ALT +P / ⌘ + shift + ⌥ + P\nRun all code with CTRL + ALT + R / ⌘ + ⌥ + R\n\n\n\n17.1.3 Code formatting\n\n&lt;- - save a value as an object. On Mac, the keyboard shortcut is ⌥ + -. Windows can be formatted so that the shortcut is ALT + -.\n\n\nx &lt;- 10\nx\n\n[1] 10\n\n\n\n|&gt; - “pipe” a command or output into another command. You can use the shortcut CTRL+SHIFT+M / ^ + SHIFT + M.\n\n\n# make repeatable\nset.seed(930)\n\n# random string\nx &lt;- rnorm(20)\n\nx |&gt; \n  # pass to summary\n  summary() |&gt; \n  # pass summary through round\n  round(2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -1.94   -0.48   -0.06   -0.05    0.36    1.80 \n\n\n\n-&gt; - save a value as an object at the end of a pipeline\n\n\n# same pipeline as previous, but more sequential\n\n# get random values\nrnorm(20) |&gt; \n  # pass to summary\n  summary() |&gt; \n  # pass summary through round\n  round(2) -&gt;\n  # save as x\n  x\n\nx\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -2.60   -0.77   -0.24   -0.18    0.57    1.73 \n\n\n\nc - concatenate, place two values together\n\n\nx &lt;- c(10,11)\nx\n\n[1] 10 11",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#basic-statistics",
    "href": "glossary.html#basic-statistics",
    "title": "17  Functions & Glossary",
    "section": "17.2 Basic statistics",
    "text": "17.2 Basic statistics\nFor these examples, we will create a random vector of number to demonstrate how they work.\n\nx &lt;- rnorm(1000)\n\n\nmean - get the mean / average of a set of data\n\n\nmean(x)\n\n[1] 0.05326273",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#custom-functions-from-class-and-elsewhere",
    "href": "glossary.html#custom-functions-from-class-and-elsewhere",
    "title": "17  Functions & Glossary",
    "section": "17.3 Custom functions from class and elsewhere",
    "text": "17.3 Custom functions from class and elsewhere\nThe following functions are those developed for this class or adapted from code posted to sources like StackOverflow.\n\n17.3.1 Basic stats\nFor these basic stats, we are using the following example data:\n\n### EXAMPLE DATA ###\nx &lt;- c(1,2,3,5,5,4,6,8,7,9,5)\n\n\n17.3.1.1 Mode calculations\n\n# Based on Statology function\n# define function to calculate mode\n# works on vectors of data\nfind_mode &lt;- function(x) {\n  # get unique values from vector\n  u &lt;- unique(x)\n  # count number of occurrences for each value\n  tab &lt;- tabulate(match(x, u))\n  \n  # if no mode, say so\n  if(length(x)==length(u[tab == max(tab)])){\n    print(\"No mode.\")\n  }else{\n    # return the value with the highest count\n    u[tab == max(tab)]\n  }\n}\n\nfind_mode(x)\n\n[1] 5\n\n\n\n\n17.3.1.2 Standard error\n\nse &lt;- function(x){\n  n &lt;- length(x) # calculate n\n  s &lt;- sd(x) # calculate standard deviation\n  se_val &lt;- s/sqrt(n)\n  return(se_val)\n}\n\nse(x)\n\n[1] 0.7385489\n\n\n\n\n17.3.1.3 Coefficient of variation\n\ncv &lt;- function(x){\n  sigma &lt;- sd(x)\n  mu &lt;- mean(x)\n  val &lt;- sigma/mu*100\n  return(val)\n}\n\ncv(x)\n\n[1] 48.98979\n\n\n\n\n\n17.3.2 Normal distributions\n\n17.3.2.1 \\(Z\\) score\nRemember - in the \\(Z\\)-score code below, if no \\(n\\) is specified, then it will default to \\(n = 1\\).\n\nzscore &lt;- function(xbar, mu, sd.x, n = 1){\n  z &lt;- (xbar - mu)/(sd.x/sqrt(n))\n  return(z)\n}\n\nzscore(xbar = 62,\n       mu = 65,\n       sd.x = 3.5,\n       n = 5)\n\n[1] -1.91663\n\n\n\n\n\n17.3.3 ANOVA\nThe following example data are going to be used to illustrate these functions.\n\n#### EXAMPLE DATA ####\nset.seed(8675309)\n\nfor(i in 1:4){\n  x &lt;- rnorm(10)\n  if(i == 1){\n    x &lt;- rnorm(10, mean = 2)\n    data &lt;- x |&gt; as.data.frame()\n    colnames(data) &lt;- \"Response\"\n    data$Explanatory &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x |&gt; as.data.frame()\n    colnames(newdat) &lt;- \"Response\"\n    newdat$Explanatory &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# split into \"typical\" table\nexpanded_data &lt;- NULL\nexpanded_data$x1 &lt;- data$Response[which(data$Explanatory==\"x1\")]\nexpanded_data$x2 &lt;- data$Response[which(data$Explanatory==\"x2\")]\nexpanded_data$x3 &lt;- data$Response[which(data$Explanatory==\"x3\")]\nexpanded_data$x4 &lt;- data$Response[which(data$Explanatory==\"x4\")]\n\nexpanded_data &lt;- expanded_data |&gt;\n  as.data.frame()\n\nThe above is a one-way ANOVA. As a reminder, we would calculate the test as follows:\n\n# pivot longer does not work for one-way ANOVA, requires blocking factor\n# can rbind things with same colnames to make longer\n\nexample_aov &lt;- aov(Response ~ Explanatory, data)\n\nsummary(example_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nExplanatory  3  23.40   7.801   11.54 1.89e-05 ***\nResiduals   36  24.33   0.676                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAbove, we can see a significant result of the ANOVA. We can follow this up with a Tukey test. This requires the package agricolae! Check the ANOVA pages, however, as not all ANOVA can use this agricolae shortcut method.\n\nexample_tukey &lt;- HSD.test(example_aov,\n                          # what to group by?\n                          \"Explanatory\",\n                          # significance level?\n                          alpha = 0.05, \n                          # are data unbalanced\n                          unbalanced = FALSE,\n                          # show answer?\n                          console = TRUE)\n\n\nStudy: example_aov ~ \"Explanatory\"\n\nHSD Test for Response \n\nMean Square Error:  0.6758192 \n\nExplanatory,  means\n\n     Response       std  r        se        Min      Max        Q25         Q50\nx1  1.7620153 1.0505466 10 0.2599652  0.4504476 3.972459  1.1175485  1.44911720\nx2  0.2841495 0.7532422 10 0.2599652 -0.4729986 1.985826 -0.3497379  0.21347543\nx3  0.2197337 0.7019368 10 0.2599652 -0.6150452 1.574903 -0.2436023 -0.04493909\nx4 -0.2890524 0.7345336 10 0.2599652 -1.9769014 0.684072 -0.5394534 -0.07741642\n          Q75\nx1 2.07491533\nx2 0.64579865\nx3 0.53544151\nx4 0.04323417\n\nAlpha: 0.05 ; DF Error: 36 \nCritical Value of Studentized Range: 3.808798 \n\nMinimun Significant Difference: 0.9901551 \n\nTreatments with the same letter are not significantly different.\n\n     Response groups\nx1  1.7620153      a\nx2  0.2841495      b\nx3  0.2197337      b\nx4 -0.2890524      b\n\n\n\n17.3.3.1 Summarize data (for plotting)\nRemember - you need to change \"Explanatory\" to your explanatory variable (in quotes!) and you need to change Response to your response column (no quotes!). The following requires plyr to work, but the function itself should call up plyr if you do not yet have it loaded.\n\n# summarize by group\nsummary_data &lt;- function(data, explanatory){\n  require(plyr)\n  ddply(data, paste(explanatory), summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n}\n\nexample_summary &lt;- summary_data(data = data, explanatory = \"Explanatory\")\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nexample_summary\n\n  Explanatory  N       mean        sd        se\n1          x1 10  1.7620153 1.0505466 0.3322120\n2          x2 10  0.2841495 0.7532422 0.2381961\n3          x3 10  0.2197337 0.7019368 0.2219719\n4          x4 10 -0.2890524 0.7345336 0.2322799\n\n\n\n\n17.3.3.2 Significant label maker\nThis command requires a Tukey HSD object from agricolae. You can manually create a table like this for some other scenarios; see relevant pages for documentation.\n\n# note first group must be EXACT MATCH to your summary_data object\n# groups are saved in the Tukey object\n# this is true for Tukey later as well\n\n# the following is a function that will make the significant label table\nsig.label.maker &lt;- function(tukey_test, group_name){\n  sig.labels &lt;- tukey_test$groups |&gt; \n    # convert to a data.frame\n    as.data.frame() |&gt;\n    # create a new column - place rownames into the column\n    # converts to a format better for ggplot\n    mutate(Explanatorys = rownames(tukey_test$groups)) |&gt;\n    # rename column to prevent confusion\n    # specify dplyr; default function may be from plyr and not work\n    dplyr::rename(Significance = groups)\n  colnames(sig.labels)[which(colnames(sig.labels) == \"Explanatorys\")] &lt;- group_name\n  return(sig.labels)\n}\n\n# Function requires explanatory groups in quotes\nsig_labels &lt;- sig.label.maker(example_tukey, \"Explanatory\")\n\nsig_labels\n\n     Response Significance Explanatory\nx1  1.7620153            a          x1\nx2  0.2841495            b          x2\nx3  0.2197337            b          x3\nx4 -0.2890524            b          x4\n\n\n\n\n17.3.3.3 ANOVA plotter\nThe following function plots ANOVAS if you have a summary_data object and a sig_labels object, as shown above. This does not work on ANOVA with interactive components.\n\nanova_plotter &lt;- function(summary_data, explanatory, \n                          response, sig_labels,\n                          y_lim=NA, label_height=NA, \n                          y_lab=NA, x_lab=NA){\n  require(tidyverse)\n  \n  plot_data_1 &lt;- summary_data[,c(explanatory, response, \"se\")]\n  plot_data_2 &lt;- sig_labels[,c(explanatory,\"Significance\")]\n  \n  colnames(plot_data_1) &lt;- c(\"explanatory\", \"response\", \"se\")\n  colnames(plot_data_2) &lt;- c(\"explanatory\", \"Significance\")\n  \n  plot_data &lt;- plot_data_1 |&gt; \n    full_join(plot_data_2, by = \"explanatory\")\n  \n  if(is.na(y_lim)){\n    if(min(plot_data$response) &lt; 0){\n      y_lim &lt;- c(min(plot_data$response) - \n          4*max(plot_data$se),\n        max(plot_data$response) + \n          4*max(plot_data$se))\n    }else{\n      y_lim &lt;- c(0,max(plot_data$response) + \n                                4*max(plot_data$se))\n    }\n  }\n  if(is.na(label_height)){label_height &lt;- 0.25*max(y_lim)}\n  if(is.na(y_lab)){y_lab &lt;- \"Response\"}\n  if(is.na(x_lab)){x_lab &lt;- \"Treatment\"}\n  \n  plot_1 &lt;- ggplot(plot_data,\n         aes(x = explanatory, y = response)) +\n    geom_point() +\n    geom_errorbar(data = plot_data,\n                  aes(ymin = response - 2*se,\n                      ymax = response + 2*se,\n                      width = 0.1)) +\n    ylim(y_lim) +\n    theme_classic() + \n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5)) +\n  geom_text(data = plot_data,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = explanatory, \n                # define height of label\n                y = label_height, \n                # what are the labels?\n                label = paste0(Significance))) +\n  xlab(x_lab) +\n  ylab(y_lab)\n  \n  print(plot_1)\n}\n\nanova_plotter(summary_data = example_summary,\n              explanatory = \"Explanatory\",\n              response = \"mean\", # from summary_data table! What is to be plotted\n              sig_labels = sig_labels)\n\n\n\n\n\n\n\n\nNote that in the above, the default label height is not working for us. We can adjust this with label_height.\n\nanova_plotter(summary_data = example_summary,\n              explanatory = \"Explanatory\",\n              response = \"mean\", # from summary_data table! What is to be plotted\n              sig_labels = sig_labels,\n              label_height = -1)\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Comont, R. (2020). BeeWalk dataset 2008-23.\nhttps://doi.org/10.6084/m9.figshare.12280547.v4\n\n\nCooper, J. C. (2021). Biogeographic and Ecologic\nDrivers of Avian Diversity.\n[Online.] Available at https://doi.org/10.6082/uchicago.3379.\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth\nScores. https://doi.org/10.6084/m9.figshare.8081108.v1\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020a). Patient\nages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020b). Estimation of the\nforce of infection and infectious period of skin sores in remote\nAustralian communities using interval-censored data.\nPLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset.\nhttps://doi.org/10.6084/m9.figshare.24119643.v1",
    "crumbs": [
      "References"
    ]
  }
]