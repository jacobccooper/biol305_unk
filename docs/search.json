[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biology 305: Biostatistics",
    "section": "",
    "text": "Introduction\nWelcome to Biology 305 at the University of Nebraska at Kearney!\nIn this class, you will learn:\n\nThe basics of study design, the importance of understanding your research situation before embarking on a full study, and practice creating research frameworks based on different scenarios.\nThe basics of data analysis, including understanding what kind of variables are being collected, why understanding variable types are important, and basic tests to understand univariate distributions.\nBasic multivariate statistics, including ANOVA, correlation, and regression, for comparing multiple different groups.\nThe basics of coding and working in R for performing statistical analyses.\n\nThis site will help you navigate different homework assignments to perform the necessary R tests.\nWelcome to class!\n\n\nClass authors\n\nDr. Melissa Wuellner\n\nPrimary lecture designer, author of many of the example problems.\n\nDr. Jacob C. Cooper\n\nPrimary website designer, primary R coding for site\n\nCaleb Rother\n\nTeaching assistant, website designer, works with homework and coding",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "install_r.html",
    "href": "install_r.html",
    "title": "1  Installing R",
    "section": "",
    "text": "1.1 Setup\nFirst, we need to download R onto your machine. We are also going to download RStudio to assist with creating R scripts and documents.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R</span>"
    ]
  },
  {
    "objectID": "install_r.html#setup",
    "href": "install_r.html#setup",
    "title": "1  Installing R",
    "section": "",
    "text": "1.1.1 Installing R\nFirst, navigate to the R download and install page. Download the appropriate version for your operating system (Windows, Mac, or Linux). Note that coding will be formatted slightly different for Windows than for other operating systems. If you have a Chromebook, you will have to follow the online instructions for installing both programs on Chrome.\nFollow the installation steps for R, and verify that the installation was successful by searching for R on your machine. You should be presented with a coding window that looks like the following, but may not be an exact match:\nR version 4.4.1 (2024-06-14) -- \"Race for Your Life\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: aarch64-apple-darwin20\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\nIf that screen appears, congratulations! R is properly installed. If the install was not successful, please talk to your instructor and teaching assistant(s) for help with installation.\n\n\n1.1.2 Installing RStudio\nRStudio is a GUI (graphics user interface) that helps make R easier to use. Furthermore, it allows you to create documents in R, including websites (such as this one), PDFs, and even presentations. This can greatly streamline the research pipeline and help you publish your results and associated code in a quick and efficient fashion.\nHead over the the RStudio download website and download “RStudio Desktop”, which is free. Be sure to pick the correct version for your machine.\nOpen RStudio on your machine. You should be presented with something like the following:\n\n\n\nRStudio start window. Note that the screen is split into four different quadrants. Top left: R documents; bottom left: R program; top right: environment window; bottom right: plots, help, and directories.\n\n\nIn RStudio, the top left window is always going to be our coding window. This is where we will type all of our code and create our documents. In the bottom left we will see R executing the code. This will show what the computer is “thinking” and will help us spot any potential issues. The top right window is the “environment”, which shows what variables and datasets are stored within the computers’ memory. (It can also show some other things, but we aren’t concerned with that at this point). The bottom right window is the “display” window. This is where plots and help windows will appear if they don’t appear in the document (top left) window itself.\n\n\n1.1.3 Setting up the “native pipe” operator\nThere are two ways to type commands in R: in a “nested” fashion and in a “piped” fashion. We will be using the “piped” fashion for ease of use.\nIn RStudio, click on the tabs at the top of the screen to go to Tools &gt; Global Options, and in the pop up screen select Code. On this screen, you should see a tick box for “Use native pipe operator”. Make sure this box is checked. Now, we can use CTRL + SHIFT + M (a.k.a., ^ + SHIFT + M on Mac) to insert the “pipe” operator |&gt;.\n\n\n\nScreenshot showing where to activate the native pipe operator within RStudio’s settings (i.e., options).\n\n\n\n1.1.3.1 Using |&gt; pipes\nLet’s say that we have a random series of 100 numbers from a random normal distribution, which we can get by copying and running the following in the lower left coding window:\n\n# make repeatable\nset.seed(42)\n\n# get 100 values from normal\nx &lt;- rnorm(100)\n\nWe want to get the mean of these values and round these values to two decimal places for our final answer. To do this in a nested fashion, we would do the following:\n\n# round mean of x\nround(mean(x), 2)\n\n[1] 0.03\n\n\nNote that we call this the “nested” method because one command (mean) is located inside another command (round). This is fine for short series of commands, but can get confusing when too many commands are combined. Thus, we can use the |&gt; pipe method instead.\nThe pipe operator takes whatever came previously and puts it through the next command. For example, mean(x) could also be written as x |&gt; mean(), where x would be placed inside of mean().\n\n# take x\nx |&gt; \n  # get mean\n  mean() |&gt; \n  # round answer\n  round(2)\n\n[1] 0.03\n\n\nAs you can see, the above breaks things down into a more step-by-step fashion, and makes code easier to follow. We will be using this extensively in this course.\n\n\n\n1.1.4 Customizing R\nRStudio’s interfacecan be customized within the global options under “appearance”. You can explore different fonts, different font sizes, and different color schemes. Please choose one that makes it easy for you to:\n\nHas a color scheme that makes the difference between different commands, numbers, and objects easy to see and is easily readable for you.\nHas a font that is a type and size that is easily readable for you.\n\n\n\n\nVisual settings on Dr. Cooper’s machine. Note that the colors make the numbers and commands stand out, and the font and color scheme are easily legible.\n\n\nCONGRATS! You now have R installed on your computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "2  Intro to R",
    "section": "",
    "text": "2.0.0.1 Windows/Linux keys",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#creating-an-rmarkdown-document",
    "href": "intro_to_r.html#creating-an-rmarkdown-document",
    "title": "2  Intro to R",
    "section": "2.1 Creating an RMarkdown document",
    "text": "2.1 Creating an RMarkdown document\n\n2.1.1 Setup\n\n2.1.1.1 Creating a new document\nIn this class, we will be creating documents in what is called RMarkdown. This is a rich-text version of R that allows us to create documents with the code embedded. In RStudio, click the “+” button in the far top left to open the New Document menu. Scroll down this list and click on R Markdown.\nA screen like this will appear:\n\n\n\nA new file window for an RMarkdown file.\n\n\nAfter entering a title and your name and selecting document in the left hand menu, click OK.\n\n\n2.1.1.2 Document setup\n\n\n\nAn example of a markdown script.\n\n\nIn the image above, we can see what a “default” RMarkdown script looks like after creating the file. At the top of the document, between all of the dashes, we have the yaml header that tells R what kind of document will be created, who the author is, and what the date is for the document (by default, this is 2025-09-03, which just prints the data on which the code was run). In this class, we will be saving documents as html as they are the easiest documents to create and save. These documents will include all of your code, text, and even any plots you may create!\n\nThe first thing you should do is save your document. Since this is a “first go”, feel free to save it as test.rmd or something to that effect, but save the document in the folder you want to use for assignments!\nAfter the document is saved, you can hit the knit button at the top of the screen and see what happens. It should create a document in the same location as your saved file that is an html format.\nA new document will appear in a pop up window as well. The information in this document will be the default text loaded above.\nAfter running the document once, delete all text below the --- at the bottom of the yaml header so we can create our own document.\n\n\n\n2.1.1.3 Writing text\nPlain text in your RStudio window will be rendered as plain text in the final document. (i.e., whatever you type normally will become “normal text” in the finished document).\n\nImportant shortcut: An important shortcut is the “double click”. Write a word, any word (like potato) in your document. If you double click, it will select the whole word. If you have a word or sentence selected, you can hit \", (, [, {, or * and RStudio will automatically put the closing equivalent of the symbol at the end of your selection.\nHeaders: Lines preceded with # will become headers; the number of # determines the level of the header, with # being main headers, ## being a second level header, and ### being a third level header, etc.\nFormatting: Words can be italicized by putting an asterisk on each side of the word (*italic*) and bold by putting two asterisks on each side (**bold**).\nHyperlinks: URLs are also supported, with &lt;&gt; on each side of a URL making it clickable. To turn a word into a hyperlink, type [words to show](https://target_URL.com). It is important that the words are in brackets and the URL is in parentheses, or it will not format correctly.\n\n\n\n\n2.1.2 Coding\nIn your computer, you should have erased all information except for the yaml header between the dashes at the top of the document. Make sure to save your file in a folder where you want your assignment to be located. It is important you do this step up front as the computer will sometimes save in random places if you don’t specify a file location at the beginning. Don’t forget to save your work frequently!\n\n2.1.2.1 Code chunks\nCode chunks are sections within the document where we can write code and it will be executed. These chunks are always preceded by a series of three “hashes” and the code language specified in brackes (i.e., {r}) and are followed by a set of three hashes as well. In your window, these will appear with a greyed-out background to show that the area is a code chunk.\nWe will create an example set of text, and then insert a code chunk to see how this works.\n\n\n2.1.2.2 Test text and chunks\n\nType This is a test of the *Rmarkdown* code. as plaintext in your document.\nHit Enter twice.\nInsert a code chunk by using the correct shortcut:\n\nWindows/Linux: CTRL + ALT + I\nMac: ⌘ + ⌥ + I\n\nPush the down arrow until you are between the sets of hashes in the code chunk.\nType print(\"hello world\")\n\nprint: a command that returns a string of characters. A string is any set of characters that should be read together as a single object or line.\n\"hello world\": placing the letters inside quotes \" tells R that these are character data that should be read as a string.\nThe full command should return an output of \"hello world\".\n\n\n\n\n\nText to type in your Rmarkdown document.\n\n\nAfter typing this into the document, hit knit near the top of the upper left window. R will now create an HTML document that should look like this:\n\n\n\nThe output from the above code knitted into a document.\n\n\nWe can see now that the HTML document has the yaml header at the top of the document for the title of the document, the author’s name, the date on which the code was run. The plain text we used has been printed, and the greyed-out box with color coded R code is followed by the output we expected.\n\nLet’s try something a little more complex. Create a new code chunk and type the following:\n\nNOTE: Mac users can use ⌥ + - to automatically insert &lt;-, saving a lot of time. Unfortunately, there is no equivalent shortcut for Windows.\nWe are avoiding using = to create objects as this can lead to issues in saving objects to R’s memory in some cases and can lead to confusion between objects and parts of functions.\n\n\n\nx &lt;- 1:10\n\n\nx: putting a plain letter, character, or word in R asks it to refer to something in its memory. If it does not exist in its memory, it can be created by using something like &lt;-. When we define a set of characters as numbers or as something else, we are creating an object.\n1:10: this will print all whole integers between the listed numbers, inclusive of those numbers. In this case, 1:10 is the same as c(1,2,3,4,5,6,7,8,9,10), where c concatenates and puts together the numbers in a single string.\nRun your code by placing your cursor on the same line as x &lt;- 1:10 and pressing CTRL + Enter / ⌘ + return.\nAfter running, you will see the following appear in your top right window, showing you that x was saved:\n\n\n\n\nThe object x now being saved to the environment, as a set of ten integers.\n\n\n\nHit Enter / return to create a new line within the code chunk. Type the letter x and hit CTRL + Enter / ⌘ + return. You should see the following:\n\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nCreate a new object, y, that is the square of x, as follows:\n\n^: this is the “power” symbol; thus, x^2 is \\(x^2\\), x^3 is \\(x^3\\), etc.\nDon’t forget to use CTRL + Enter / ⌘ + return to run the line of code.\n\n\n\ny &lt;- x^2\n\nAfter running the code, you should have both objects in your memory, like so:\n\n\n\nRStudio environment window showing saved objects. These are in the computer’s memory.\n\n\nWe can now print y, just as we did for x, to see what the object contains in the document:\n\ny\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\n\nNote that you can multiply objects together as well. When we have strings, the strings will be multiplied together. Try the following:\n\n\nx*y\n\n [1]    1    8   27   64  125  216  343  512  729 1000\n\n\n\ny-x\n\n [1]  0  2  6 12 20 30 42 56 72 90\n\n\n\nNote: Since I didn’t “store” this value as a variable in R using &lt;-, the value is not in the environment.\n\n\n\n2.1.2.3 Taking notes\nWe can take notes and annotate code within a code chunk by using the character # before anything we wish to annotate. Note that the color of the text after # inside the code chunk will change to show us that the code will not run.\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# y\n\nAbove, we can see only x printed, as y was blocked with a #. We can use this to add text inside the code as well:\n\n# subtract x from y\ny - x\n\n [1]  0  2  6 12 20 30 42 56 72 90\n\n\nYou are highly encouraged to add notes to your code, both to help others understand your logic and to remind yourself of what you did in the future!\n\n\n\n2.1.3 Plotting\nNow, let’s try creating a plot. This is easy in R, as we just use the command plot. At minimum, we must define an x variable and a y variable for the command plot. It will calculate and make everything else automatically.\n\nNote: Within commands, we must use =.\n\nType and run the following:\n\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nNow let’s make the plot with some new visual parameters. I describe what these arguments do using # to make notes within the code chunk. Note that you can split commands over multiple lines after typing , but that the command must all be enclosed within parentheses ().\n\nplot(x = x, # specify x values\n     y = y, # specify y values\n     ylab = \"Y Values\", # specify Y label\n     xlab = \"X Values\", # specify X label\n     main = \"Plot Title\", # specify main title\n     pch = 19, # adjust point style\n     col = \"red\") # make points red",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "intro_to_r.html#stop-touching-your-mouse",
    "href": "intro_to_r.html#stop-touching-your-mouse",
    "title": "2  Intro to R",
    "section": "2.2 Stop touching your mouse",
    "text": "2.2 Stop touching your mouse\nRStudio has lots of keyboard shortcuts that will make your life easier. You will be a lot faster if you rely on these shortcuts. I outline some here, but see the Glossary.\n\nUse arrow keys to move around your document\nHighlight code by holding shift and pushing the arrow keys\nCopy text using CTRL + C / ⌘ + C\nPaste text using CTRL + V / ⌘ + V\nCut text using CTRL + X / ⌘ + X\nUndo last edit or mistake using CTRL + Z / ⌘ + Z\nSave your document using CTRL + S / ⌘ + S\n\nThese shortcuts are worth learning; they will greatly increase the speed at which you get things done.\n\n2.2.1 Tab complete\nRStudio allows for “tab-completing” while typing code. Tab-completing is a way of typing the first part of a command, variable name, or file name and hitting “tab” to show all options with that spelling. You should use tab completing because it:\n\nreduces spelling mistakes\nreduces filepath mistakes\nincreases the speed at which you code\nprovides help with specific functions\n\n\n\n2.2.2 Help\nAt any point in R, you can look up “help” for a specific function by typing ?functionname. Try this on your computer with the following:\n\n?mean\n\nNOTE that you cannot knit a document with a help command in it! So you need to remove any lines with them before creating your document or R will be very unhappy.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to *R*</span>"
    ]
  },
  {
    "objectID": "first_analysis.html",
    "href": "first_analysis.html",
    "title": "3  Running your first analysis",
    "section": "",
    "text": "3.1 Create a new document\nRefer to the previous section to create a new document. Name it LastName_first_analysis.rmd, with LastName replaced by your surname. Save this document in your assignments folder, and make sure it will knit as an html document.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running your first analysis</span>"
    ]
  },
  {
    "objectID": "first_analysis.html#create-a-new-document",
    "href": "first_analysis.html#create-a-new-document",
    "title": "3  Running your first analysis",
    "section": "",
    "text": "You should always have your last name in your file name when you submit documents in this class.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running your first analysis</span>"
    ]
  },
  {
    "objectID": "first_analysis.html#working-with-data",
    "href": "first_analysis.html#working-with-data",
    "title": "3  Running your first analysis",
    "section": "3.2 Working with data",
    "text": "3.2 Working with data\nThroughout this course, we are going to have to work with datasets that are from our book or other sources. Here, we are going to work through an example dataset.\n\nFirst, we need to install libraries. A library is a collated, pre-existing batch of code that is designed to assist with data analysis or to perform specific functions. These libraries make life a lot easier, and create short commands for completing relatively complex tasks.\n\n\n3.2.1 Libraries\nIn this class, there is one major library that you will need almost every week! Even if I don’t declare this library, you should load it in your documents.\n\nLibraries are declared at the very beginning of the document\nLibraries must be in a code chunk at the top of the document or else they will not be loaded before the code requiring them, preventing your document from being knit.\n\nMake sure you read your error codes - many times it will say that your command is not found, and that often means your library is not loaded. That, or you misspelled something.\n\n\n\ntidyverse: this package is actually a group of packages designed to help with data analysis, management, and visualization.\n\nNOTE: If you leave the install prompts in your RMarkdown document, it will not knit! Install the following using the bottom left coding window of your RStudio session.\n\n# run this code the first time ONLY\n# DO NOT INCLUDE IN RMD FILE\n# does not need to be run every time you use R!\n\n# tidyverse has a bunch of packages in it!\n# great for data manipulation\ninstall.packages(\"tidyverse\")\n\n# if you ever need to update:\n# leaving brackets open means \"update everything\"\nupdate.packages()\n\n\nAfter packages are installed, we will need to load them into our R environment. While we only need to do install.packages once on our machine, we need to load libraries every time we restart the program!\n\nNOTE: The following is required in EVERY DOCUMENT that uses the tidyverse commands!\n\n### MUST BE RUN AT THE TOP OF EVERY DOCUMENT ###\n# Load tidyverse into R session / document\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nYou should have an output like the above. What this means is:\n\nThe core packages that comprise the tidyverse loaded successfully, and version numbers for each are shown.\nThe conflicts basically means that certain commands will not work as they used to because R has “re-learned” a particular word.\n\n\n3.2.1.1 Conflicts\nTo clarify the conflicts, pretend that you can only know one definition of a word at a time, in this case, the word “cola”.\n\nEnglish: Cola is a type of soda pop or soft drink.\nSpanish: Cola refers to a line or a tail.\n\nWhile we can figure out which definition is being used based on context, R can’t. It will always use the most recent definition, such that it may interpret something as “Mi perro movió la refresco” or “I bought a tail from the vending machine”. To avoid this confusion in R, we specify which “cola” we are referring to. In the above, this would look like “Mi perro movió la español::cola” and “I bought a english::cola from the vending machine”.\nWe should not have many conflicts in this class, but be aware they may exist.\n\n\n\n3.2.2 Downloading data\nNow, we need to download our first data set. These datasets are stored on GitHub. We are going to be looking at data from Dr. Cooper’s dissertation concerning Afrotropical bird distributions (Cooper 2021). This website is in the data folder on this websites’ GitHub page, accessible here.\n\n# read comma separated file (csv) into R memory\n# reads directly from URL\nranges &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/datasets/lacustrine_range_size.csv\")\n\nRows: 12 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (9): combined_current_km2, consensus_km2, bioclim_current_km2, 2050_comb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAlternatively, we can use the operator |&gt; to simplify this process. |&gt; means “take whatever you got from the previous step and pipe it into the next step”. So, the following does the exact same thing:\n\nranges &lt;- \"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/datasets/lacustrine_range_size.csv\" |&gt;\n  read_csv()\n\nRows: 12 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (9): combined_current_km2, consensus_km2, bioclim_current_km2, 2050_comb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUsing the |&gt; is preferred as you can better set up a workflow and because it more closely mimics other coding languages, such as bash.\nLet’s view the data to see if it worked. We can use the command head to view the first few rows:\n\nhead(ranges)\n\n# A tibble: 6 × 10\n  species                combined_current_km2 consensus_km2 bioclim_current_km2\n  &lt;chr&gt;                                 &lt;dbl&gt;         &lt;dbl&gt;               &lt;dbl&gt;\n1 Batis_diops                          25209.         6694.              19241.\n2 Chamaetylas_poliophrys               68171.         1106.              68158.\n3 Cinnyris_regius                      60939.        13305.              53627.\n4 Cossypha_archeri                     27021.         6409.              11798.\n5 Cyanomitra_alinae                    78680.        34320.              63381.\n6 Graueria_vittata                      8770.          861.               8301.\n# ℹ 6 more variables: `2050_combined_km2` &lt;dbl&gt;, `2050_consensus_km2` &lt;dbl&gt;,\n#   `2070_combined_km2` &lt;dbl&gt;, `2070_consensus_km2` &lt;dbl&gt;,\n#   alltime_consensus_km2 &lt;dbl&gt;, past_stable_km2 &lt;dbl&gt;\n\n\nWe can perform a lot of summary statistics in R. Some of these we can view for multiple columns at once using summary.\n\nsummary(ranges)\n\n   species          combined_current_km2 consensus_km2     bioclim_current_km2\n Length:12          Min.   :  8770       Min.   :  861.3   Min.   :  3749     \n Class :character   1st Qu.: 24800       1st Qu.: 4186.2   1st Qu.: 10924     \n Mode  :character   Median : 43654       Median : 7778.1   Median : 31455     \n                    Mean   : 68052       Mean   :18161.8   Mean   : 42457     \n                    3rd Qu.: 70798       3rd Qu.:18558.7   3rd Qu.: 62835     \n                    Max.   :232377       Max.   :79306.6   Max.   :148753     \n 2050_combined_km2 2050_consensus_km2 2070_combined_km2  2070_consensus_km2\n Min.   :  1832    Min.   :    0.0    Min.   :   550.3   Min.   :    0.0   \n 1st Qu.:  6562    1st Qu.:  589.5    1st Qu.:  6583.8   1st Qu.:  311.4   \n Median : 26057    Median : 6821.9    Median : 24281.7   Median : 2714.6   \n Mean   : 33247    Mean   :14418.4    Mean   : 31811.0   Mean   : 8250.5   \n 3rd Qu.: 40460    3rd Qu.:18577.1    3rd Qu.: 38468.9   3rd Qu.:10034.4   \n Max.   :132487    Max.   :79236.2    Max.   :129591.0   Max.   :53291.8   \n alltime_consensus_km2 past_stable_km2 \n Min.   :    0.0       Min.   :   0.0  \n 1st Qu.:  790.9       1st Qu.:   0.0  \n Median : 8216.8       Median :   0.0  \n Mean   :15723.3       Mean   : 127.3  \n 3rd Qu.:19675.0       3rd Qu.:   0.0  \n Max.   :82310.5       Max.   :1434.8  \n\n\nAs seen above, we now have information for the following statistics for each variable:\n\nMin = minimum\n1st Qu. = 1st quartile\nMedian = middle of the dataset\nMean = average of the dataset\n3rd Qu. = 3rd quartile\nMax. = maximum\n\nWe can also calculate some of these statistics manually to see if we are doing everything correctly. It is easiest to do this by using predefined functions in R (code others have written to perform a particular task) or to create our own functions in R. We will do both to determine the average of combined_current_km2.\n\n\n3.2.3 Subsetting data\nFirst, we need to select only the column of interest. In R, we have two ways of subsetting data to get a particular column.\n\nvar[rows,cols] is a way to look at a particular object (var in this case) and choose a specific combination of row number and column number (col). This is great if you know a specific index, but it is better to use a specific name.\nvar[rows,\"cols\"] is a way to do the above but by using a specific column name, like combined_current_km2.\nvar$colname is a way to call the specific column name directly from the dataset.\n\n\n# using R functions\n\nranges$combined_current_km2\n\n [1]  25209.4  68171.2  60939.2  27021.3  78679.9   8769.9 232377.2  17401.4\n [9]  51853.5  35455.1  23570.3 187179.1\n\n\nAs shown above, calling the specific column name with $ allows us to see only the data of interest. We can also save these data as an object.\n\ncurrent_combined &lt;- ranges$combined_current_km2\n\ncurrent_combined\n\n [1]  25209.4  68171.2  60939.2  27021.3  78679.9   8769.9 232377.2  17401.4\n [9]  51853.5  35455.1  23570.3 187179.1\n\n\nNow that we have it as an object, specifically a numeric vector, we can perform whatever math operations we need to on the dataset.\n\nmean(current_combined)\n\n[1] 68052.29\n\n\nHere, we can see the mean for the entire dataset. However, we should always round values to the same number of decimal points as the original data. We can do this with round.\n\nround(mean(current_combined),1) # round mean to one decimal\n\n[1] 68052.3\n\n\nNote that the above has a nested set of commands. We can write this exact same thing as follows:\n\n# pipe mean through round\ncurrent_combined |&gt; \n  mean() |&gt; \n  round(1)\n\n[1] 68052.3\n\n\nUse the method that is easiest for you to follow!\nWe can also calculate the mean manually. The mean is \\(\\frac{\\sum_{i=1}^nx}{n}\\), or the sum of all the values within a vector divided by the number of values in that vector.\n\n# create function\n# use curly brackets to denote function\n# our data goes in place of \"x\" when finally run\nour_mean &lt;- function(x){\n  sum_x &lt;- sum(x) # sum all values in vector\n  n &lt;- length(x) # get length of vector\n  xbar &lt;- sum_x/n # calculate mean\n  return(xbar) # return the value outside the function\n}\n\nLet’s try it.\n\nour_mean(ranges$combined_current_km2)\n\n[1] 68052.29\n\n\nAs we can see, it works just the same as mean! We can round this as well.\n\nranges$combined_current_km2 |&gt; \n  our_mean() |&gt; \n  round(1)\n\n[1] 68052.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running your first analysis</span>"
    ]
  },
  {
    "objectID": "first_analysis.html#creating-a-new-column",
    "href": "first_analysis.html#creating-a-new-column",
    "title": "3  Running your first analysis",
    "section": "3.3 Creating a new column",
    "text": "3.3 Creating a new column\nThe package tidyverse allows for easily creating new columns within a dataset using the command mutate.\nFor example, we can create a column of combined_current_km2 + 1.\n\nranges &lt;- ranges |&gt; \n  mutate(combined_plus_one = combined_current_km2 + 1)\n\nhead(ranges$combined_plus_one)\n\n[1] 25210.4 68172.2 60940.2 27022.3 78680.9  8770.9",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running your first analysis</span>"
    ]
  },
  {
    "objectID": "first_analysis.html#your-turn",
    "href": "first_analysis.html#your-turn",
    "title": "3  Running your first analysis",
    "section": "3.4 Your turn!",
    "text": "3.4 Your turn!\nPlease complete the following:\n\nCreate an RMarkdown document that will save as an .html.\nLoad the data, as shown here, and print the summary statistics in the document.\nCalculate the value of combined_current_km2 divided by 2050_combined_km2 and print the results.\n\nHint: you can divide and multiply objects in R, like a + b, a/b, etc.\n\nknit your results, with your name(s) and date, as an HTML document.\n\nLet me know if you have any issues.\n\n\n\n\nCooper, J. C. (2021). Biogeographic and Ecologic Drivers of Avian Diversity. [Online.] Available at https://doi.org/10.6082/uchicago.3379.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running your first analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "4  Descriptive Statistics",
    "section": "",
    "text": "4.1 Purposes of descriptive statistics\nDescriptive statistics allow us to quickly get an idea of how our datasets are “behaving”. This allows us to identify potential errors, trends, or particular outliers for further analysis. While you are likely familiar with many of the calculations we will go over, these remain powerful calculations for initial analysis. In this page, we will go over how to complete these calculations in R. You will then complete an assignment based on these topics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#sample-data-and-preparation",
    "href": "descriptive_stats.html#sample-data-and-preparation",
    "title": "4  Descriptive Statistics",
    "section": "4.2 Sample data and preparation",
    "text": "4.2 Sample data and preparation\nAs with every week, we will need to load our relevant packages first. This week, we are using the following:\n\n# enables data management tools\nlibrary(tidyverse)\n# DescTools can also be used\n# see section on MODE\nlibrary(DescTools)\n\nREMEMBER: An R package must be installed before it can be used with the command install.packages or via the RStudio interface.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#downloading-the-data",
    "href": "descriptive_stats.html#downloading-the-data",
    "title": "4  Descriptive Statistics",
    "section": "4.3 Downloading the data",
    "text": "4.3 Downloading the data\nFor this week, we will be using a hypothetical dataset on the number of drinks sold by hour at Starbucks.\n\n# download data\nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol105_unk/main/datasets/starbucks.csv\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#descriptive-statistics",
    "href": "descriptive_stats.html#descriptive-statistics",
    "title": "4  Descriptive Statistics",
    "section": "4.4 Descriptive statistics",
    "text": "4.4 Descriptive statistics\nDescriptive statistics are statistics that help us understand the shape and nature of the data on hand. These include really common metrics such as mean, median, and mode, as well as more nuanced metrics like quartiles that help us understand if there is any skew in the dataset. (Skew refers to a bias in the data, where more data points lie on one side of the distribution and there is a long tail of data in the other direction).\n\n\n\nExamples of skew compared to a symmetrical, non-skewed distribution. Source: machinelearningparatodos.com\n\n\nNote above that the relative position of the mean, median, and mode can be indicative of skew. Please also note that these values will rarely be exactly equal “in the real world”, and thus you need to weigh differences against the entire dataset when assessing skew. There is a lot of nuance like this in statistics; it is not always an “exact” science, but sometimes involves judgment calls and assessments based on what you observe in the data.\nUsing the starbucks dataset, we can look at some of these descriptive statistics to understand what is going on.\n\n \n\n\n4.4.1 Notation\nWe use Greek lettering for populations and Roman lettering for samples. For example:\n\n\\(\\sigma\\) is a population, but \\(s\\) is a sample (both these variables refer to standard deviation).\n\\(\\mu\\) is a population, but \\(\\bar{x}\\) is a sample (both of these variables refer to the mean).\n\n\n\n4.4.2 Mean\nThe mean is the “average” value within a set of data, specifically, the sum of all values divided by the length of those values: \\(\\frac{\\sum_{i=1}^nx}{n}\\).\n\n\nClick to see how to calculate Mean\n\n\nhead(starbucks)\n\n# A tibble: 6 × 2\n  Hour      Frap_Num\n  &lt;chr&gt;        &lt;dbl&gt;\n1 0600-0659        2\n2 0700-0759        3\n3 0800-0859        2\n4 0900-0959        4\n5 1000-1059        8\n6 1100-1159        7\n\n\nHere, we are specifically interested in the number of frappuccinos.\n\n# get vector of frappuccino number\nfraps &lt;- starbucks$Frap_Num\n\n# get mean of vector\nmean(fraps)\n\n[1] 6.222222\n\n\nNote that the above should be rounded to a whole number, since we were given the data in whole numbers!\n\nmean(fraps) |&gt;\n  round(0)\n\n[1] 6\n\n# OR\n\nround(mean(fraps),0)\n\n[1] 6\n\n\nWe already covered calculating the average manually in our previous tutorial, but we can do that here as well:\n\n# sum values\n# divide by n, length of vector\n# round to 0 places\nround(sum(fraps)/length(fraps),0)\n\n[1] 6\n\n\n\n\n \n\n\n\n4.4.3 Range\nThe range is the difference between the largest and smallest units in a dataset. I.e., it is the maximum value in a dataset minus the smallest value in a dataset.\n\n\nClick to see how to calculate Range\n\nWe can use the commands min and max to calculate this.\n\nmax(fraps) - min(fraps)\n\n[1] 13\n\n\nThe range of our dataset is 13.\nNOTE that the R function range will not return the statistic of range, it will only return the maximum and minimum values from the dataset. This is not the correct answer when asked for the range for any homework or test questions.\n\nrange(fraps)\n\n[1]  2 15\n\n\n\n\n \n\n\n\n4.4.4 Median\nThe median is also known as the 50th percentile, and is the midpoint of the data when ordered from least to greatest. If there are an even number of data points, then it is the average point between the two center points. For odd data, this is the \\(\\frac{n+1}{2}\\)th observation. For even data, since we need to take an average, this is the \\(\\frac{\\frac{n}{2}+(\\frac{n}{2}+1)}{2}\\).\n\n\nClick to see how to calculate the Median\n\nR has a default method for calculating median - median.\n\n4.4.4.1 Odd number of values\n\n# use default\nmedian(fraps)\n\n[1] 4\n\n\nNow, to calculate by hand:\n\nlength(fraps)\n\n[1] 9\n\n\nWe have an odd length.\n\n# order gets the order\norder(fraps)\n\n[1] 1 3 7 2 4 6 5 9 8\n\n\n\n# [] tells R which elements to put where\nfrap_order &lt;- fraps[order(fraps)]\n\nfrap_order\n\n[1]  2  2  2  3  4  7  8 13 15\n\n\n\n# always use parentheses\n# make sure the math maths right!\n(length(frap_order)+1)/2\n\n[1] 5\n\n\nWhich is the fifth element in the vector?\n\nfrap_order[5]\n\n[1] 4\n\n\n\n\n4.4.4.2 Even number of values\nGiven that we are trying to find the “middle” number, this is more complicated when using a vector that has an even number of values. Let’s calculate this by hand:\n\n# remove first element\neven_fraps &lt;- fraps[-1]\n\neven_fraps_order &lt;- even_fraps[order(even_fraps)]\n\neven_fraps_order\n\n[1]  2  2  3  4  7  8 13 15\n\n\n\nmedian(even_fraps)\n\n[1] 5.5\n\n\nNow, by hand: \\(\\frac{\\frac{n}{2}+(\\frac{n}{2}+1)}{2}\\).\n\nn &lt;- length(even_fraps_order)\n\n# get n/2 position from vector\nm1 &lt;- even_fraps_order[n/2]\n# get n/2+1 position\nm2 &lt;- even_fraps_order[(n/2)+1]\n\n# add these values, divide by two for \"midpoint\"\nmed &lt;- (m1+m2)/2\n\nmed\n\n[1] 5.5\n\n\nAs we can see, these values are equal!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#homework-descriptive-statistics",
    "href": "descriptive_stats.html#homework-descriptive-statistics",
    "title": "4  Descriptive Statistics",
    "section": "4.5 Homework: Descriptive Statistics",
    "text": "4.5 Homework: Descriptive Statistics\nNow that we’ve covered these basic statistics, it’s your turn!\n\n\n4.5.1 Homework instructions\nPlease create an RMarkdown document that will render as an .html file. You will submit this file to show your coding and your work. Please refer to the Introduction to R for refreshers on how to create an .html document in RMarkdown.\nPlease show all of your work for full credit.\n\n\n\n4.5.2 Data for homework problems\nFor each question, calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers”.\nPlease also write your own short response to the Synthesis question posed, which will involve thinking about the data and metrics you just analyzed.\n\n4.5.2.1 1: UNK Nebraskans\nEver year, the university keeps track of where students are from. The following are data on the umber of students admitted to UNK from the state of Nebraska:\n\n# create dataset in R\nnebraskans &lt;- c(5056,5061,5276,5244,5209,\n                5262,5466,5606,5508,5540,5614)\n\nyears &lt;- 2023:2013\n\nnebraskans_years &lt;- cbind(years,nebraskans) |&gt; \n  as.data.frame()\n\nnebraskans_years\n\n   years nebraskans\n1   2023       5056\n2   2022       5061\n3   2021       5276\n4   2020       5244\n5   2019       5209\n6   2018       5262\n7   2017       5466\n8   2016       5606\n9   2015       5508\n10  2014       5540\n11  2013       5614\n\n\nUsing these data, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers” for the number of UNK students from Nebraska.\nIn order to do this, we will need to first select the column that denotes the number of Nebraskans from the dataset. Remember, we need to save this as an object in R to do the analyses. Here is a demonstration of getting the column to look at the mean, so that you can repeat this for the other questions. This relies heavily on the use of $, used to get the data from a specific column.\n\n# $ method\nnebraskans &lt;- nebraskans_years$nebraskans\n\nnebraskans\n\n [1] 5056 5061 5276 5244 5209 5262 5466 5606 5508 5540 5614\n\n\nNow we can get the mean of this vector.\n\nmean(nebraskans) |&gt;\n  round(0) # don't forget to round!\n\n[1] 5349\n\n\nNow, calculate the following for nebraskans:\n\nmean - mean()\n25th and 50th percentiles - quantile()\nmedian - median()\nrange - max()-min()\ninterquartile range - IQR()\nvariance - var()\nstandard deviation - sd()\ncoefficient of variation - cv() function defined above\nstandard error - sd()\nwhether there are any “outliers” - Outliers()\n\nSynthesis question: Do you think that there are any really anomalous years? Do you feel data are similar between years? Note we are not looking at trends through time but whether any years are outliers.\n\n\n4.5.2.2 2: Piracy in the Gulf of Guinea\nThe following is a dataset looking at oceanic conditions and other variables associated with pirate attacks within the region between 2010 and 2021 (Moura et al. 2023). Using these data, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers” for distance from shore for each pirate attack (column Distance_from_Coast).\n\npirates &lt;- read_csv(\"https://figshare.com/ndownloader/files/42314604\")\n\nNew names:\nRows: 595 Columns: 40\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(18): Period, Season, African_Season, Coastal_State, Coastal_Zone, Navi... dbl\n(20): ...1, Unnamed: 0, Year, Month, Lat_D, Lon_D, Distance_from_Coast,... dttm\n(2): Date_Time, Date\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nNow, calculate the following for Distance_from_Coast:\n\nmean\nmedian\n25th and 75th percentiles\nrange\ninterquartile range\nvariance\nstandard deviation\ncoefficient of variation\nstandard error\nwhether there are any “outliers”\n\nSynthesis question: Do you notice any patterns in distance from shore? What may be responsible for these patterns? Hint: Think about what piracy entails and also what other columns are available as other variables in the above dataset.\n\n\n4.5.2.3 3: Patient ages at presentation\nThe following is a dataset on skin sores in different communities in Australia and Oceania, specifically looking at the amount of time that passes between skin infections (Lydeamore et al. 2020a). This file includes multiple different datasets, and focuses on data from children in the first five years of their life, on household visits, and on data collected during targeted studies (Lydeamore et al. 2020b).\n\nages &lt;- read_csv(\"https://doi.org/10.1371/journal.pcbi.1007838.s006\")\n\nRows: 17150 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): dataset\ndbl (1): time_difference\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s see what this file is like real fast. We can use the command dim to see the rows and columns.\n\ndim(ages)\n\n[1] 17150     2\n\n\nAs you can see, this file has only two columns but 17,150 rows! For the column time_difference, please calculate the mean, median, range, interquartile range, variance, standard deviation, coefficient of variation, standard error, and whether there are any “outliers”.\nCalculate the following for the time_difference:\n\nmean\nmedian\n25th and 75th percentiles\nrange\ninterquartile range\nvariance\nstandard deviation\ncoefficient of variation\nstandard error\nwhether there are any “outliers”\n\nSynthesis question: Folks will often think about probabilities of events being “low but never zero”. What does that mean in the context of secondary infection? What about these data make you think that a secondary infection is always possible?\n\n\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W. Cuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon, S. Y. C. Tong, and J. M. McCaw (2020b). Patient ages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W. Cuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon, S. Y. C. Tong, and J. M. McCaw (2020a). Estimation of the force of infection and infectious period of skin sores in remote Australian communities using interval-censored data. PLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset. https://doi.org/10.6084/m9.figshare.24119643.v1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "visual.html",
    "href": "visual.html",
    "title": "5  Diagnosing data visually",
    "section": "",
    "text": "5.1 The importance of visual inspection\nInspecting data visually can give us a lot of information about whether data are normally distributed and about whether there are any major errors or issues with our dataset. It can also help us determine if data meet model assumptions, or if we need to use different tests more appropriate for our datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#sample-data-and-preparation",
    "href": "visual.html#sample-data-and-preparation",
    "title": "5  Diagnosing data visually",
    "section": "5.2 Sample data and preparation",
    "text": "5.2 Sample data and preparation\nBefore we start, we must load our R libraries.\n\nlibrary(tidyverse)\n\nWe will need one additional R library this week as well - moments. You will need to install this library on your machine if you have not done so already.\n\nlibrary(moments)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#histograms",
    "href": "visual.html#histograms",
    "title": "5  Diagnosing data visually",
    "section": "5.3 Histograms",
    "text": "5.3 Histograms\nA histogram is a frequency diagram that we can use to visually diagnose data and their distributions. We are going to examine a histogram using a random string of data. R can generate random (though, actually pseudorandom) strings of data on command, pulling them from different distributions. These distributions are pseudorandom because we can’t actually program R to be random, so it starts from a wide variety of pseudorandom points.\n\n\n\n\n5.3.1 Histograms on numeric vectors\n\n\nClick to see how to make a default histogram\n\nThe following is how to create default histograms on data. If you need to create custom bin sizes, please see the notes under Cumulative frequency plot for data that are not already in frequency format.\n\n# create random string from normal distribution\n# this step is not necessary for data analysis in homework\nset.seed(8675309)\n\nx &lt;- rnorm(n = 1000, # 1000 values\n           mean = 0,\n           sd = 1)\n\n# make histogram\nhist(x)\n\n\n\n\n\n\n\n\nNOTE that a histogram can only be made on a vector of values. If you try to make a histogram on a data frame, you will get an error and it will not work. You have to specify which column you wish to use with the $ operator. (For example, for dataframe xy with columns x and y, you would use hist(xy$y)).\nWe can increase the number of bins to see this better.\n\nhist(x,breaks = 100)\n\n\n\n\n\n\n\n\nThe number of bins can be somewhat arbitrary, but a value should be chosen based off of what illustrates the data well. R will auto-select a number of bins in some cases, but you can also select a number of bins. Some assignments will ask you to choose a specific number of bins as well.\n\n\n\n\n\n\n5.3.2 Histograms on frequency counts\n\n\nClick to see how to make a histogram with frequency data\n\nSay, for example, that we have a dataset where everything is already shown as frequencies. We can create a frequency histogram using barplot.\n\ncount_table &lt;- matrix(nrow = 4, ncol = 2, byrow = T,\n                      data = c(\"Cat 1\", 4,\n                               \"Cat 2\", 8,\n                               \"Cat 3\", 7,\n                               \"Cat 4\", 3)) |&gt;\n  as.data.frame()\n\ncolnames(count_table) &lt;- c(\"Category\",\"Count\")\n\n# ensure counts are numeric data\ncount_table$Count &lt;- as.numeric(count_table$Count)\n\n# manually create histogram\nbarplot(count_table$Count, # response variable, counts for histogram\n        axisnames = T, # make names on plot\n        names.arg = count_table$Category) # make these the names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 ggplot histograms\n\n\nClick to see how to make fancy histograms (optional)\n\nThe following is an optional workthrough on how to make really fancy plots.\nWe can also use the program ggplot, part of the tidyverse, to create histograms.\n\n# ggplot requires data frames\nx2 &lt;- x |&gt; as.data.frame()\ncolnames(x2) &lt;- \"x\"\n\nggplot(data = x2, aes(x = x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot is nice because we can also clean up this graph a little.\n\nggplot(x2,aes(x=x)) + geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can also do a histogram of multiple values at once in R.\n\nx2$cat &lt;- \"x\"\n\ny &lt;- rnorm(n = 1000,\n           mean = 1,\n           sd = 1) |&gt;\n  as.data.frame()\n\ncolnames(y) &lt;- \"x\"\ny$cat &lt;- \"y\"\n\nxy &lt;- rbind(x2,y)\n\nhead(xy)\n\n           x cat\n1 -0.9965824   x\n2  0.7218241   x\n3 -0.6172088   x\n4  2.0293916   x\n5  1.0654161   x\n6  0.9872197   x\n\n\n\nggplot(xy,aes(x = x, fill = cat)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can also make this look a little nicer.\n\nggplot(xy, aes(x = x, colour = cat)) +\n  geom_histogram(fill = \"white\", alpha = 0.5, # transparency\n                 position = \"identity\") +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can show these a little differently as well.\n\nggplot(xy, aes(x = x, fill = cat))+\n  geom_histogram(position = \"identity\", alpha = 0.5) +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThere are lots of other commands you can incorporate as well if you so choose; I recommend checking sites like this one or using ChatGPT.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#boxplots",
    "href": "visual.html#boxplots",
    "title": "5  Diagnosing data visually",
    "section": "5.4 Boxplots",
    "text": "5.4 Boxplots\n\n\nClick to see how to make boxplots\n\nWe can also create boxplots to visualize the spread of the data. Boxplots include a bar for the median, a box representing the interquartile range between the 25th and 75th percentiles, and whiskers that extend \\(1.5 \\cdot IQR\\) beyond the 25th and 75th percentiles. We can create a boxplot using the command boxplot.\n\n# using pre-declared variable x\n\nboxplot(x)\n\n\n\n\n\n\n\n\nWe can set the axis limits manually as well.\n\nboxplot(x, # what to plot\n        ylim = c(-4, 4), # set y limits\n        pch = 19) # make dots solid\n\n\n\n\n\n\n\n\nOn the above plot, outliers for the dataset are shown as dots beyond the ends of the “whiskers”.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#skewness",
    "href": "visual.html#skewness",
    "title": "5  Diagnosing data visually",
    "section": "5.5 Skewness",
    "text": "5.5 Skewness\n\n\nClick to read about skewness\n\nSkew is a measure of how much a dataset “leans” to the positive or negative directions (i.e., to the “left” or to the “right”). This is a function from the moments library.\n\nskewness(x)\n\n[1] -0.07158066\n\n\nGenerally, a value between \\(-1\\) and \\(+1\\) for skewness is “acceptable” and not considered overly skewed. Positive values indicate “right” skew and negative values indicate a “left” skew. If something is too skewed, it may violate assumptions of normality and thus need non-parametric tests rather than our standard parametric tests - something we will cover later!\nThe following is not required for skewness and is an example to show greater skew.\nLet’s look at a skewed dataset. We are going to artificially create a skewed dataset from our x vector.\n\n# create more positive values\nx3 &lt;- c(x,\n        x[which(x &gt; 0)]*2,\n        x[which(x &gt; 0)]*4,\n        x[which(x &gt; 0)]*8)\n\nhist(x3)\n\n\n\n\n\n\n\n\n\nskewness(x3) |&gt; \n  round(2)\n\n[1] 2.18\n\n\nAs we can see, the above is a heavily skewed dataset with a positive (“right”) skew.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#kurtosis",
    "href": "visual.html#kurtosis",
    "title": "5  Diagnosing data visually",
    "section": "5.6 Kurtosis",
    "text": "5.6 Kurtosis\n\n\nClick to read about kurtosis\n\nKurtosis refers to how sharp or shallow the peak of the distribution is (platykurtic vs. leptokurtic). Remember - platykyrtic are plateaukurtic, wide and broad like a plateau, and leptokurtic distributions are sharp. Intermediate distributions that are roughly normal are mesokurtic.\nMuch like skewness, kurtosis values of \\(&gt; 2\\) and \\(&lt; -2\\) are generally considered extreme, and thus not mesokurtic. This threshold can vary a bit based on source, but for this class, we will use a threshold of \\(\\pm 2\\) for both skewness and kurtosis.\nLet’s see the kurtosis of x. Note that when doing the equation, a normal distribution actually has a kurtosis of \\(3\\); thus, we are doing kurtosis \\(-3\\) to “zero” the distribution and make it comparable to skewness.\nNote: kurtosis is a function from the moments library.\n\nhist(x)\n\n\n\n\n\n\n\n\n\n# zeroed\n# put whole thing to be rounded in brackets\n(kurtosis(x)-3) |&gt; round(2)\n\n[1] 0.05\n\n\nNote when rounding a calculation, the calculation must be in brackets or it will only round the last value.\nAs expected, out values drawn from a normal distribution are mesokurtic.\n\nThe following is not required for calculating kurtosis, and this is an example to show something more leptokurtic.\n\n\n\nClick here to see code for calculating on a more leptokurtic example\n\nLet’s compare these to a more leptokurtic distribution:\n\nxk &lt;- x^3\n\nkurtosis(xk)-3\n\n[1] 29.12246\n\n\nWhat does this dataset look like?\n\nhist(xk,breaks = 100)\n\n\n\n\n\n\n\n\nAs we can see, this is a very leptokurtic distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#cumulative-frequency-plot",
    "href": "visual.html#cumulative-frequency-plot",
    "title": "5  Diagnosing data visually",
    "section": "5.7 Cumulative frequency plot",
    "text": "5.7 Cumulative frequency plot\nA cumulative frequency plot shows the overall spread of the data as a cumulative line over the entire dataset. This is another way to see the spread of the data and is often complementary to a histogram.\n\n\n\nClick to see how to make a cumulative frequency plot if data are a numeric vector\n\nThe use of the Empirical Cumulative Distribution Function ecdf can turn a variable into what is needed to create a cumulative frequency plot.\n\nplot(ecdf(x)) #Creating a cumulative frequency plot\n\n\n\n\n\n\n\n\n\nplot(ecdf(x), \n     xlab = \"Data Values\", #Labeling the x-axis\n     ylab = \"Cumulative Probability\", #Labeling the y-axis\n     main = \"ECDF of X\") #Main title for the graph\n\n\n\n\n\n\n\n\n\n\n\n\nClick to see how to make a cumulative frequency plot if data are in count format\n\nIf you have a list of frequencies (say, for river discharge over several years), you only need to do the cumsum function. For example:\n\ny &lt;- c(1 ,2 ,4, 8, 16, 8, 4, 2, 1)\n\nsum_y &lt;- cumsum(y)\n\nprint(y)\n\n[1]  1  2  4  8 16  8  4  2  1\n\nprint(sum_y)\n\n[1]  1  3  7 15 31 39 43 45 46\n\n\nNow we can see we have out cumulative sums. Let’s plot these. NOTE that this method will not have the x variables match the dataset you started with, it will only plot the curve based on the number of values given.\n\nplot(x = 1:length(sum_y), # get length of sum_y, make x index\n     y = sum_y, # plot cumulative sums\n     type = \"l\") # make a line plot",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#homework-visualizing-data",
    "href": "visual.html#homework-visualizing-data",
    "title": "5  Diagnosing data visually",
    "section": "5.8 Homework: Visualizing data",
    "text": "5.8 Homework: Visualizing data\n\nDirections:\nPlease complete all computer portions in an rmarkdown document knitted as an html. Upload any “by hand” calculations as images in the HTML or separately on Canvas.\nRemember: you will need to load the tidyverse and moments libraries!\n\n\n\n\n5.8.1 Problem 1\nThe Research Institute Nature and Forest in Brussels, Belgium, conducted a survey of fish from the southern North Sea to help with the identification of fish from seabird diets (Verstraete et al. 2020). We will be looking at the lengths one specific taxon - the Atlantic Herring Clupea harengus.\nCopy and run the code below to load in your data. Use the object clupea.lengths for this problem.\n\n# download fish data\nfishes &lt;- read_csv(\"https://zenodo.org/records/4066594/files/reference_data.csv\")\n\n# isolate Clupea harengus\nclupea &lt;- fishes |&gt; \n  filter(scientific_name == \"Clupea harengus\")\n\n# get vector of lengths\nclupea.lengths &lt;- clupea$fish_length_mm |&gt; \n  # remove missing data\n  na.omit()\n\n\nCreate a histogram of these data.\nCreate a cumulative frequency plot of these data.\nCalculate the kurtosis of these data, and make a conclusion about the kurtosis of the dataset.\nCalculate the skewness of these data, and make a conclusion about the skewness of the dataset.\nCreate a boxplot of these data. Are there any outliers in this dataset?\nCompare the methods used to examine the data above. How do these methods help you understand the data? Do the numerical results from parts like skewness and kurtosis make sense with your plots?\n\n\n\n5.8.2 Problem 2\nThe Fremont Bridge in Seattle is one of the only bridges that connects the north and south sides of the city, crossing the Fremont Cut - a navigational canal between the Puget Sound and Lake Union & Lake Washington. As such, it is an important transportation corridor. We will be looking at counts of bicyclists, taken hourly for the east (predominately northbound) side over a seven year period (Weber 2019). This dataset has 56,904 records - thus, this is not something that would be easily done without the use of a program like R!\n\nbicycles &lt;- read_csv(\"https://zenodo.org/records/2648564/files/Fremont_Bridge_Hourly_Bicycle_Counts_by_Month_October_2012_to_present.csv\")\n\nbike.count &lt;- bicycles$`Fremont Bridge East Sidewalk` |&gt; \n  na.omit()\n\n\nCreate a histogram of these data.\nCreate a cumulative frequency plot of these data.\nCalculate the kurtosis of these data, and make a conclusion about the kurtosis of the dataset.\nCalculate the skewness of these data, and make a conclusion about the skewness of the dataset.\nCreate a boxplot of these data. Are there any outliers in this dataset?\nCompare the methods used to examine the data above. How do these methods help you understand the data? Do the numerical results from parts like skewness and kurtosis make sense with your plots?\n\n\n\n5.8.3 Problem 3\nTake the object bike.count from the previous problem and perform a log transformation of the data. To do this, we will take the natural log of the dataset plus one. We must add 1 to every value in the dataset, because the natural log of 0 is negative infinity (\\(-\\infty\\)); adding one makes every value in the dataset quantifiable as the minimum value for any count data will become 0.\nPerform the following action on your computer, replacing x with your bike.data object. Name your object log.bikes instead of log.x.\n\nlog.x &lt;- log(x + 1)\n\nAfter performing the log transformation, complete the following.\n\nCreate a histogram of these data.\nCreate a cumulative frequency plot of these data.\nCalculate the kurtosis of these data, and make a conclusion about the kurtosis of the dataset.\nCalculate the skewness of these data, and make a conclusion about the skewness of the dataset.\nCreate a boxplot of these data. Are there any outliers in this dataset?\nHow did the log transformation change these data?\n\n\n\n5.8.4 Problem 4\nIs there pseudoreplication in the bicycle dataset? Why or why not?\n\n\n5.8.5 Problem 5\nThis last problem will require you to practice Histograms on frequency counts and Cumulative frequency plot on count data. In this dataset, you will be looking at the counts of the response variable for each level of the treatment variable.\n\ntreatment &lt;- 1:10\nresponse &lt;- c(1,11,21,28,33,39,32,21,17,2)\n\nrandom_data &lt;- cbind(treatment, response) |&gt; \n  as.data.frame()\n\n\nCreate a histogram of these data.\nCreate a cumulative frequency plot of these data.\nCalculate the kurtosis of these data, and make a conclusion about the kurtosis.\nCalculate the skewness of these data, and make a conclusion about the skewness.\nCreate a boxplot of these data. Are there any outliers?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "visual.html#acknowledgment",
    "href": "visual.html#acknowledgment",
    "title": "5  Diagnosing data visually",
    "section": "5.9 Acknowledgment",
    "text": "5.9 Acknowledgment\nThanks to Hernan Vargas & Riley Grieser for help in formatting this page. Additional comments provided by BIOL 305 classes.\n\n\n\n\nVerstraete, H., W. Courtens, R. Daelemans, M. Van de walle, N. Vanermen, P. Desmet, and E. W. M. Stienen (2020). Photos and measurements of otoliths from fish caught in the southern part of the North Sea. [Online.] Available at https://zenodo.org/records/4066594.\n\n\nWeber, G. (2019). Fremont Bridge Hourly Bicycle Counts by Month October 2012 to present. [Online.] Available at https://zenodo.org/records/2648564.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  },
  {
    "objectID": "normality.html",
    "href": "normality.html",
    "title": "6  Normality & hypothesis testing",
    "section": "",
    "text": "6.1 Normal distributions\nA standard normal distribution is a mathematical model that describes a commonly observed phenomenon in nature. When measuring many different kinds of datasets, the data being measured often becomes something that resembles a standard normal distribution. This distribution is described by the following equation:\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^\\frac{(x-\\mu)^2}{2\\sigma^2}\\]\nThis equation is fairly well defined by the variance (\\(\\sigma^2\\)), the overall spread of the data, and by the standard deviation (\\(\\sigma\\)), which is defined by the square root of the variance.\nStandard normal distributions have a mean, median, and mode that are equal. The standard normal distribution is a density function, and we are interested in the “area under the curve” (AUC) to understand the relative probability of an event occurring. At the mean/median/mode, the probability on either side of the distribution is \\(50\\)%. When looking at a normal distribution distribution, it is impossible to say the probability of a specific event occurring, but it is possible to state the probability of an event as extreme or more extreme than the event observed occurring. This is known as the \\(p\\) value.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#normal-distributions",
    "href": "normality.html#normal-distributions",
    "title": "6  Normality & hypothesis testing",
    "section": "",
    "text": "A standard normal distribution, illustrating the percentage of area found within each standard deviation away from the mean. By Ainali on Wikipedia; CC-BY-SA 3.0.\n\n\n\n\n6.1.1 Example in nature\n\n\nAn example of naturally occurring normal distributions\n\nIn order to see an example of the normal distribution in nature, we are going to examine the BeeWalk survey database from the island of Great Britain (Comont 2020). We are not interested in the bee data at present, however, but in the climatic data from when the surveys were performed.\n\nbeewalk &lt;- read_csv(\"https://figshare.com/ndownloader/files/44726902\")\n\nRows: 306550 Columns: 49\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): Website.ID, Website.RecordKey, SiteName, Site.section, ViceCounty,...\ndbl (19): RecordKey, established, Precision, Transect.lat, Transect.long, tr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that this is another massive dataset - \\(306,550\\) rows of data!\nThe dataset has the following columns:\n\ncolnames(beewalk)\n\n [1] \"RecordKey\"            \"Website.ID\"           \"Website.RecordKey\"   \n [4] \"SiteName\"             \"Site.section\"         \"ViceCounty\"          \n [7] \"established\"          \"GridReference\"        \"Projection\"          \n[10] \"Precision\"            \"Transect.lat\"         \"Transect.long\"       \n[13] \"transect.OS1936.lat\"  \"Transect.OS1936.long\" \"transect_length\"     \n[16] \"section_length\"       \"section_grid_ref\"     \"H1\"                  \n[19] \"H2\"                   \"H3\"                   \"H4\"                  \n[22] \"habitat_description\"  \"L1\"                   \"L2\"                  \n[25] \"land_use_description\" \"start_time\"           \"end_time\"            \n[28] \"sunshine\"             \"wind_speed\"           \"temperature\"         \n[31] \"TaxonVersionKey\"      \"species\"              \"latin\"               \n[34] \"queens\"               \"workers\"              \"males\"               \n[37] \"unknown\"              \"Comment\"              \"transect_comment\"    \n[40] \"flower_visited\"       \"StartDate\"            \"EndDate\"             \n[43] \"DateType\"             \"Year\"                 \"Month\"               \n[46] \"Day\"                  \"Sensitive\"            \"Week\"                \n[49] \"TotalCount\"          \n\n\nWe are specifically interested in temperature to determine weather conditions. Let’s see what the mean of this variable is.\n\nmean(beewalk$temperature)\n\n[1] NA\n\n\nHmmm… we are getting an NA value, indicating that not every cell has data recorded. Let’s view summary.\n\nsummary(beewalk$temperature)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   16.00   19.00   18.65   21.00   35.00   16151 \n\n\nAs we can see, \\(16,151\\) rows do not have temperature recorded! We want to remove these NA rows, which we can do by using using na.omit.\n\nbeewalk$temperature |&gt;\n  na.omit() |&gt;\n  mean() |&gt;\n  round(2) # don't forget to round!\n\n[1] 18.65\n\n\nNow we can record the mean.\nLet’s visualize these data using a histogram. Note I do not use na.omit as the hist function automatically performs this data-cleaning step!\n\nhist(beewalk$temperature,breaks = 5)\n\n\n\n\n\n\n\n\nEven with only five breaks, we can see an interesting, normal-esque distribution in the data. Let’s refine the bin number.\n\nhist(beewalk$temperature,breaks = 40)\n\n\n\n\n\n\n\n\nWith forty breaks, the pattern becomes even more clear. Let’s see what a standard normal distribution around these data would look like.\n\n# save temperature vector without NA values\ntemps &lt;- beewalk$temperature |&gt; na.omit()\n\nmu &lt;- mean(temps)\nt.sd &lt;- sd(temps)\n\n# sample random values\nnormal.temps &lt;- rnorm(length(temps), # sample same size vector\n                      mean = mu,\n                      sd = t.sd)\n\nhist(normal.temps, breaks = 40)\n\n\n\n\n\n\n\n\nAs we can see, our normal approximation of temperatures is not too dissimilar from the distribution of temperatures we actually see!\nLet’s see what kind of data we have for temperatures:\n\n# load moments library\nlibrary(moments)\n\nskewness(temps)\n\n[1] 0.02393257\n\n\nData do not have any significant skew.\n\nkurtosis(temps)-3\n\n[1] 0.3179243\n\n\nData do not show any significant kurtosis.\n\n\n\n6.1.2 Effect of sampling\n\n\nHow sample size effects statistical calculations\n\nOftentimes, we will see things approach the normal distribution as we collect more samples. We can model this by subsampling our temperature vector.\n\n# make reproducible\nset.seed(1839)\n\nsub.temps &lt;- sample(temps,\n                    size = 10,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"10 samples\")\n\n\n\n\n\n\n\n\nWith only ten values sampled, we do not have much of a normal distribution. Let’s up this to \\(100\\) samples.\n\nsub.temps &lt;- sample(temps,\n                    size = 100,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"100 samples\",breaks = 10)\n\n\n\n\n\n\n\n\nNow we are starting to see more of a normal distribution! Let’s increase this to \\(1000\\) temperatures.\n\nsub.temps &lt;- sample(temps,\n                    size = 1000,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"1000 samples\", breaks = 40)\n\n\n\n\n\n\n\n\nNow the normal distribution is even more clear. As we can also see, the more we sample, the more we approach the true means and distribution of the actual dataset. Because of this, we can perform experiments and observations of small groups and subsamples and make inferences about the whole, given that most systems naturally approach statistical distributions like the normal!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#hypothesis-testing",
    "href": "normality.html#hypothesis-testing",
    "title": "6  Normality & hypothesis testing",
    "section": "6.2 Hypothesis testing",
    "text": "6.2 Hypothesis testing\nSince we can define specific areas under the curve within these distributions, we can look at the percentage of area within a certain bound to determine how likely a specific outcome would be. Thus, we can begin to test what the probability of observing an event is within a theoretical, probabilistic space. A couple of important conceptual ideas:\n\nWe may not be able to know the probability of a specific event, but we can figure out the probability of events more extreme or less extreme as that event.\nIf the most likely result is the mean, then the further we move away from the mean, the less likely an event becomes.\nIf we look away from the mean at a certain point, then the area represents the chances of getting a result as extreme or more extreme than what we observe. This probability is known as the \\(p\\) value.\n\nOnce we have a \\(p\\) value, we can make statements about the event that we’ve seen relative to the overall nature of the dataset, but we do not have sufficient information to declare if this result is statistically significant.\n\n6.2.1 Critical Values - \\(\\alpha\\)\nIn order to determine if something is significant, we compare things to a critical value, known as \\(\\alpha\\). This value is traditionally defined as \\(0.05\\), essentially stating that we deem an event as significant if \\(5\\)% or fewer of observed or predicted events are as extreme or more extreme than what we observe.\nYour value should always set your \\(\\alpha\\) critical value before you do your experiments and analyses.\nOur critical value of \\(\\alpha\\) represents our criterion for rejecting the null hypothesis. We set our \\(\\alpha\\) to try to minimize the chances of error.\nType I Error is also known as a false-positive, and is when we reject the null hypothesis when the null is true.\nType II Error is also known as a false-negative, and is when we support the null hypothesis when the null is false.\nBy setting an \\(\\alpha\\), we are creating a threshold of probability at which point we can say, with confidence, that results are different.\n\n\n6.2.2 Introduction to \\(p\\) values\nLet’s say that we are looking at a dataset defined by a standard normal distribution with \\(\\mu=0\\) and \\(\\sigma=1\\). We draw a random value, \\(x\\), with \\(x=1.6\\). What is the probability of drawing a number this extreme or more extreme from the dataset?\nFirst, let’s visualize this distribution:\n\n\nClick here to view plot creation code.\n\n\n###THIS WILL TAKE A WHILE TO RUN###\n\n# create gigantic normal distribution dataset\n# will be essentially normal for plotting\n# rnorm gets random values\nx &lt;- rnorm(100000000)\n\n# convert to data frame\nx &lt;- as.data.frame(x)\n# rename column\ncolnames(x) &lt;- c(\"values\")\n\n# thank you stack overflow for the following\n# Creating density plot\np = ggplot(x, \n           aes(x = values)\n          ) + \n  # generic density plot, no fill\n  geom_density(fill=\"lightblue\")\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # rename axes\n  geom_vline(xintercept = 1.6) +\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nAbove, the solid black line represents \\(x\\), with the illustrated standard normal distribution being filled in blue.\nLet’s see how much of the area represents values as extreme or more extreme as our value \\(x\\).\n\n\nClick here for code to create figure.\n\n\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  &lt;-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x &lt; 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nNow we can see that it is only a portion of the distribution as extreme or more extreme than the value we placed on the graph. The area of this region is our \\(p\\) value. This represents the probability of an event as extreme or more extreme occurring given the random variation observed in the dataset or in the distribution approximating the dataset. This is the value we compare to \\(\\alpha\\) - our threshold for rejecting the null hypothesis - to determine whether or not we are going to reject the null hypothesis.\nLet’s look at the above graph again, but let’s visualize a two-tailed \\(\\alpha\\) around the mean with a \\(95\\)% confidence interval. First, we need to get the \\(Z\\) scores for our \\(\\alpha=0.05\\), which we calculate by taking \\(\\frac{\\alpha}{2}\\) to account for the two tails. (Two tails essentially meaning we reject the null mean if we see things greater than or less than our expected value to a significant extent). We can calculate \\(Z\\) scores using qnorm.\n\nlow_alpha &lt;- qnorm(0.025) # looks left\nhi_alpha &lt;- qnorm(0.975) # looks left\n\nprint(paste0(\"Our Z scores are: \", \n             round(low_alpha,2), \n             \" & \", \n             round(hi_alpha,2)))\n\n[1] \"Our Z scores are: -1.96 & 1.96\"\n\n\nThe above values make sense, given the distribution is symmetrical. Our above dashed line is as \\(Z = 1.96\\), which means we should have a \\(p = 0.05\\), so the dashed line should be closer to the mean than our cutoffs.\n\n\nClick here for code to create figure.\n\n\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  &lt;-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  &lt;-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x &lt; 1.96), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.96, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  geom_vline(xintercept = low_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  geom_vline(xintercept = hi_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n\n\n\n\n\n\n\n\nExactly as we calculated, we see that our \\(p &lt; \\alpha\\) and thus we do not see an area (in blue) less than the area that would be further than the mean as defined by the red lines.\n\n\n6.2.3 Calculating a \\(Z\\) score\nWhen we are trying to compare our data to a normal distribution, we need to calculate a \\(Z\\) score for us to perform the comparison. A \\(Z\\) score is essentially a measurement of the number of standard deviations we are away from the mean on a standard normal distribution. The equation for a \\(Z\\) score is:\n\\[\nZ = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nWhere \\(\\bar{x}\\) is either a sample mean or a sample value, \\(\\mu\\) is the population mean, \\(\\sigma\\) is the population standard deviation and \\(n\\) is the number of individuals in your sample (\\(1\\) if comparing to a single value).\nWe can calculate this in R using the following function:\n\nzscore &lt;- function(xbar, mu, sd.x, n = 1){\n  z &lt;- (xbar - mu)/(sd.x/sqrt(n))\n  return(z)\n}\n\nNOTE that is the above isn’t working for you, you have a mistake somewhere in your code. Try comparing - character by character - what is listed above to what you have.\nLet’s work through an example, where we have a sample mean of \\(62\\) with \\(5\\) samples compared to a sample mean of \\(65\\) with a standard deviation of \\(3.5\\).\n\nZ &lt;- zscore(xbar = 62, # sample mean\n            mu = 65, # population mean\n            sd.x = 3.5, # population standard deviation\n            n = 5) # number in sample\n\nprint(Z)\n\n[1] -1.91663\n\n\nNow, we can calculate the \\(p\\) value for this \\(Z\\) score.\n\npnorm(Z)\n\n[1] 0.0276425\n\n\nAfter rounding, we get \\(p = 0.03\\), a \\(p\\) that is significant as it is less than our overall \\(\\alpha\\).\n\n\n6.2.4 Calculating the \\(p\\) value\nWe have two different methods for calculating a \\(p\\) value:\n\n6.2.4.1 Comparing to a \\(z\\) table\nWe can compare the \\(z\\) value we calculate to a \\(z\\) table, such as the one at ztable.net. On this webpage, you can scroll and find tables for positive and negative \\(z\\) scores. Note that normal distributions are symmetrical, so you can also transform from negative to positive to get an idea of the area as well. Given that a normal distribution is centered at \\(0\\), a \\(z\\) score of \\(0\\) will have a \\(p\\) value of \\(0.50\\).\nOn the \\(z\\) tables, you will find the tenths place for your decimal in the rows, and then go across to the columns for the hundredths place. For example, go to the website and find the \\(p\\) value for a \\(z\\) score of \\(-1.33\\). You should find the cell marked \\(0.09176\\). Note the website uses naked decimals, which we do not use in this class.\nFor values that aren’t on the \\(z\\) table, we can approximate its position between different points on the \\(z\\) table or, if it is extremely unlikely, denote that \\(p &lt; 0.0001\\).\n\n\n6.2.4.2 Using R\nIn R, we can calculate a \\(p\\) value using the function pnorm. This function uses the arguments of p for our \\(p\\) value, mean for the mean of our distribution, sd for the standard deviation of our distribution, and also information on whether we want to log-transform \\(p\\) or if we are testing a specific hypothesis (lower tail, upper tail, or two-tailed). The function pnorm defaults to a standard normal distribution, which would be a \\(z\\) score, but it can also perform the \\(z\\) transformations for us if we define the mean and standard deviation.\nFor example, if we have a \\(z\\) score of \\(-1.33\\):\n\npnorm(-1.33)\n\n[1] 0.09175914\n\n\nAs we can see, we get the same result as our \\(z\\) table, just with more precision!\nThere are other functions in this family as well in R, including dnorm for quantiles, qnorm for determining the \\(z\\) score for a specific \\(p\\) value, and rnorm for getting random values from a normal distribution with specific dimensions. For now, we will focus on pnorm.\n\n\n\n6.2.5 Workthrough Example\nWhen this class was being designed, Hurricane Milton was about to make contact with Florida. Hurricane Milton is considered one of the strongest hurricanes of all time, so we can look at historical hurricane data to determine just how powerful this storm really was. We can get information on maximum wind speeds of all recorded Atlantic hurricanes as of 2024 from Wikipedia.\n\n\nClick here for details on creating the file for this walkthrough.\n\n\n# https://stackoverflow.com/questions/59481608/how-to-remove-the-characters-from-a-string-and-leave-only-the-numbers-in-r\n\n# cocatenating wikipedia files.\nhurricanes1 &lt;- list.files(\"~/Downloads/\",pattern = \"List_of_Category_1_Atlantic*\")\n\nspeedlist_cat1 &lt;- NULL\nfor(j in 1:length(hurricanes1)){\n  test &lt;- read_csv(paste0(\"~/Downloads/\", hurricanes1[j]))\n  test &lt;- test[-1,] # remove extra header\n  for(i in 1:nrow(test)){\n    speed &lt;- test$`Peak intensity...3`[i]\n    speed2 &lt;- strsplit(speed,\" \")[[1]][1]\n    speed2 &lt;- gsub(\"\\\\D+\", \"\", speed2)\n    speedlist_cat1 &lt;- c(speedlist_cat1,speed2)\n  }\n}\n\nspeedlist_cat1 &lt;- as.numeric(speedlist_cat1)\n\n# cocatenating wikipedia files.\nhurricanes2 &lt;- list.files(\"~/Downloads/\",pattern = \"List_of_Category_2_Atlantic*\")\n\nspeedlist_cat2 &lt;- NULL\nfor(j in 1:length(hurricanes2)){\n  test &lt;- read_csv(paste0(\"~/Downloads/\", hurricanes2[j]))\n  test &lt;- test[-1,] # remove extra header\n  for(i in 1:nrow(test)){\n    speed &lt;- test$`Peak intensity...3`[i]\n    speed2 &lt;- strsplit(speed,\" \")[[1]][1]\n    speed2 &lt;- gsub(\"\\\\D+\", \"\", speed2)\n    speedlist_cat2 &lt;- c(speedlist_cat2,speed2)\n  }\n}\n\nspeedlist_cat2 &lt;- as.numeric(speedlist_cat2)\n\n# cocatenating wikipedia files.\nhurricanes3 &lt;- list.files(\"~/Downloads/\",pattern = \"List_of_Category_3_Atlantic*\")\n\nspeedlist_cat3 &lt;- NULL\nfor(j in 1:length(hurricanes3)){\n  test &lt;- read_csv(paste0(\"~/Downloads/\", hurricanes3[j]))\n  test &lt;- test[-1,] # remove extra header\n  for(i in 1:nrow(test)){\n    speed &lt;- test$`Peak intensity...3`[i]\n    speed2 &lt;- strsplit(speed,\" \")[[1]][1]\n    speed2 &lt;- gsub(\"\\\\D+\", \"\", speed2)\n    speedlist_cat3 &lt;- c(speedlist_cat3,speed2)\n  }\n}\n\nspeedlist_cat3 &lt;- as.numeric(speedlist_cat3)\nspeedlist_cat3 &lt;- na.omit(speedlist_cat3) |&gt;\n  as.numeric()\n\n# cocatenating wikipedia files.\nhurricanes4 &lt;- list.files(\"~/Downloads/\",pattern = \"List_of_Category_4_Atlantic*\")\n\nspeedlist_cat4 &lt;- NULL\nfor(j in 1:1){\n  test &lt;- read_csv(paste0(\"~/Downloads/\", hurricanes4[j]))\n  test &lt;- test[-1,] # remove extra header\n  for(i in 1:nrow(test)){\n    speed &lt;- test$`Peak intensity...3`[i]\n    speed2 &lt;- strsplit(speed,\" \")[[1]][1]\n    speed2 &lt;- gsub(\"\\\\D+\", \"\", speed2)\n    speedlist_cat4 &lt;- c(speedlist_cat4,speed2)\n  }\n}\n\nspeedlist_cat4 &lt;- as.numeric(speedlist_cat4)\n\n# cocatenating wikipedia files.\nhurricanes5 &lt;- list.files(\"~/Downloads/\",pattern = \"List_of_Category_5_Atlantic*\")\n\nspeedlist_cat5 &lt;- NULL\nfor(j in 1:1){\n  test &lt;- read_csv(paste0(\"~/Downloads/\", hurricanes5[j]))\n  test &lt;- test[-1,] # remove extra header\n  for(i in 1:nrow(test)){\n    speed &lt;- test$`Peak intensity...4`[i]\n    speed2 &lt;- strsplit(speed,\" \")[[1]][1]\n    speed2 &lt;- gsub(\"\\\\D+\", \"\", speed2)\n    speedlist_cat5 &lt;- c(speedlist_cat5,speed2)\n  }\n}\n\nspeedlist_cat5 &lt;- as.numeric(speedlist_cat5)\n\nspeedlist &lt;- c(speedlist_cat1,speedlist_cat2,speedlist_cat3,\n               speedlist_cat4,speedlist_cat5)\n\nspeedlist &lt;- as.data.frame(speedlist)\n\ncolnames(speedlist) &lt;- \"Hurricane_Windspeed\"\n\nwrite_csv(speedlist,path = \"~/Documents/GitHub/biol105_unk/assignments/hurricane_speeds.csv\")\n\n\n\nhurricanes &lt;- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/assignments/hurricane_speeds.csv\")\n\nRows: 889 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): Hurricane_Windspeed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAbove, we have loaded a .csv of one column that has all the hurricane speeds up to 2024. Hurricane Milton is the last row - the most recent hurricane. Let’s separate this one out. We will use [ , ], which defines [rows,columns] to subset data.\n\nmilton &lt;- hurricanes$Hurricane_Windspeed[nrow(hurricanes)]\n\nother_hurricanes &lt;- hurricanes[-nrow(hurricanes),]\n\nWe want to compare the windspeed of Milton (180 mph) to the overall distribution of hurricane speeds. We can visualize this at first.\n\n# all hurricanes\nhist(hurricanes$Hurricane_Windspeed)\n\n\n\n\n\n\n\n\nWindspeeds are more towards the lower end of the distribution, with strong storms being rarer.\nFor the sake of this class, we will assume we can use a normal distribution for these data, but if we were doing an official study we would likely need to use a non-parametric test (we will cover these later, but they cover non-normal data).\n\nmu &lt;- mean(other_hurricanes$Hurricane_Windspeed)\n\nmu\n\n[1] NA\n\n\nHmmm… we need to use na.omit to be sure we do this properly.\n\nother_hurricanes_windspeed &lt;- na.omit(other_hurricanes$Hurricane_Windspeed)\n\nmu &lt;- mean(other_hurricanes_windspeed)\nmean(other_hurricanes_windspeed)\n\n[1] 102.5254\n\n\nNext, we need the standard deviation.\n\nsd.hurricane &lt;- sd(other_hurricanes_windspeed)\n\nsd.hurricane\n\n[1] 23.08814\n\n\nNow, we can calculate our \\(Z\\) value.\n\nZ &lt;- (milton - mu)/sd.hurricane\n\nZ\n\n[1] 3.355603\n\n\nHow significant is this?\n\npnorm(Z)\n\n[1] 0.999604\n\n\nThis is greater than \\(0.5\\), so we need to do \\(1-p\\) to figure things out.\n\n1 - pnorm(Z)\n\n[1] 0.000395961\n\n\nThis rounds to \\(0.0004\\), which means that this is an extremely strong hurricane.\n\n6.2.5.1 Non-normality, for those curious\nWe can do a Shapiro-Wilk test of normality to see if this dataset is normal.\n\nshapiro.test(other_hurricanes_windspeed)\n\n\n    Shapiro-Wilk normality test\n\ndata:  other_hurricanes_windspeed\nW = 0.88947, p-value &lt; 2.2e-16\n\n\nA \\(p &lt; 0.05\\) indicates that these data are non-normal.\nWe can do a Wilcoxon-Test since these data are extremely non-normal.\n\nwilcox.test(other_hurricanes_windspeed,\n            milton)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  other_hurricanes_windspeed and milton\nW = 6.5, p-value = 0.08678\nalternative hypothesis: true location shift is not equal to 0\n\n\nUsing non-normal corrections, we find that this is not an extremely strong hurricane, but it is near the upper end of what we would consider “normal” under historical conditions. Still an extremely bad hurricane!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#confidence-intervals",
    "href": "normality.html#confidence-intervals",
    "title": "6  Normality & hypothesis testing",
    "section": "6.3 Confidence Intervals",
    "text": "6.3 Confidence Intervals\nBecause we can figure out the probability of an event occurring, we can also calculate confidence intervals. A confidence interval provides a range of numbers around a value of interest that indicates where we believe the mean of a population lies and our confidence that it lies within that range. Note that nothing is ever 100% certain, but this helps us determine where a mean is and demonstrates our confidence in our results.\nSpecifically, if the tails of the distribution, our \\(\\alpha\\), are \\(0.05\\), then we have an area of \\(0.95\\) around the mean where we do not reject results. Another perspective on this area is that we can say with \\(95\\)% certainty that a mean is within a certain area, and that if values fall within that confidence area then we do not reject the null hypothesis that the means are equal.\nWe will cover several different ways to calculate confidence intervals, but for normal distributions, we use the following equation, with the \\(0.95\\) confidence interval shown as an example:\n\\[\nCI=\\bar{x} \\pm Z_{1-\\frac{\\alpha}{2}}\\cdot \\frac{\\sigma}{\\sqrt{n}}\n\\]\nThis interval gives us an idea of where the mean should lie. For example, if we are looking at the aforementioned beewalk temperature data, we can calulate a \\(0.95\\) confidence interval around the mean.\n\ntemps &lt;- na.omit(beewalk$temperature)\n\nxbar &lt;- mean(temps)\nn &lt;- length(temps)\nsdtemp &lt;- sd(temps)\n# Z for 0.95 as P, so for 0.975, 0.025\n# get value from P\nZ &lt;- qnorm(0.975)\n\nCI &lt;- Z*(sdtemp/sqrt(n))\n\nCI\n\n[1] 0.01458932\n\n\nWe have a very narrow confidence zone, because we have so many measurements. Let’s round everything and present it in a good way.\nIf I want numbers to show up in text in RMarkdown, I can add code to a line of plain text using the following syntax:\n\n# DO NOT RUN\n# Format in plaintext\n`r xbar`\n\n\n\n\nThis is the “coded” version of the text below. Compare the above window to the text below this image.\n\n\nTyping that into the plain text should render as the following: 18.645963. Then I can also type my answer as follows:\nThe \\(95\\)% Confidence Interval for the mean for this temperature dataset is:\n\nprint(paste(round(xbar, 2),\n      \"+/-\",\n      round(CI, 2)))\n\n[1] \"18.65 +/- 0.01\"\n\n\nNote that the above is rounded to two decimal places to illustrative purposes ONLY, and should be rounded to one decimal place if it was a homework assignment because the original data has only one decimal place.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "normality.html#homework-normal-distributions",
    "href": "normality.html#homework-normal-distributions",
    "title": "6  Normality & hypothesis testing",
    "section": "6.4 Homework: Normal distributions",
    "text": "6.4 Homework: Normal distributions\nSubmit one html file, as created in RStudio, with you answers to the following questions. For maximum clarity, create headings to separate your problems. (Remember, a header can be invoked by placing ‘#’ in front of a line of text. For example: the header here is written as # Homework: Chapter 8).\n\n6.4.1 \\(Z\\) to \\(p\\)\nFor each of the following, report the corresponding \\(p\\) value for the \\(z\\) score. List exactly as displayed, keeping in mind that it defaults to “looking left” (lower.tail = TRUE).\n\n2.3\n6.4\n-1.5\n-9.2\n0.3\n\n\n\n6.4.2 \\(P\\) to \\(z\\)\nFor the following, report the corresponding \\(z\\) score for the stated \\(p\\) value. For this question, assume every value is “looking left” (lower.tail = T).\n\n0.96\n0.23\n0.14\n0.0036\n0.47\n0.62\n\n\n\n6.4.3 Confidence intervals\nFor the following scenarios, calculate the confidence interval for the mean.\n\n\\(\\bar{x} = 7.6\\), \\(\\sigma = 1.3\\), \\(n = 5\\), \\(CI = 99\\%\\)\n\\(\\bar{x} = -4.9\\), \\(\\sigma = 0.7\\), \\(n = 6\\), \\(CI = 95\\%\\)\n\\(\\bar{x} = 2.1\\), \\(\\sigma = 0.4\\), \\(n = 10\\), \\(CI = 90\\%\\)\n\\(\\bar{x} = 0.1\\), \\(\\sigma = 3\\), \\(n = 12\\), \\(CI = 95\\%\\)\n\n\n\n6.4.4 Italian climate - spring\nAs part of a study on how climate affects the weights of Alpine Ibex, researchers recorded the maximum temperature in the spring at their study site (Brambilla et al. 2024). The researchers find another site with an average spring maximum temperature of 6.2ºC, and claim that it is significantly different than their study site. Are they correct? Assume that \\(\\alpha=0.05\\) and determine whether this is a one-tailed (unidirectional) or two-tailed (bidirectional) test.\nThe data for this problem will be in the object spring_temps below:\n\n# load weather data\nweather &lt;- read_csv(\"https://zenodo.org/records/12749273/files/20240505_env_var_dataset.csv\")\n\n# isolate spring temperatures\nspring_temps &lt;- weather$SpringTmax |&gt;\n  # remove NA values\n  na.omit() |&gt; \n  # ensure data are in correct numeric format\n  as.numeric()\n\nHINT: \\(x = 6.2\\), \\(\\bar{x}\\) is the mean of the spring_temps above. We are comparing a single value to the dataset, so \\(n=1\\).\nEvaluate their claims by:\n\nCalculating a \\(z\\)-score for their temperature (6.2ºC).\nCalculating a \\(p\\)-value for their temperature (6.2ºC).\nReporting a 95% confidence interval for the mean for the temperatures reported in their dataset.\nIn a single sentence, state whether you find this temperature to be significantly warmer than the mean or not, stating what evidence you are using to make this conclusion.\n\n\n\n6.4.5 Italian climate - Summer\nUsing the same dataset (weather) as the previous question, isolate the data for the summer (column SummTmax). The researchers find the average temperature of another site is 13.6ºC, and claim this is warmer than their research site. Are they correct? Assume that \\(\\alpha=0.05\\) and determine whether this is a one-tailed (unidirectional) or two-tailed (bidirectional) test.\nNote for this question, you will have to isolate the column for summer temperatures from the weather dataframe and perform all of the analysis steps yourself.\nAs with the previous question, complete the following:\n\nCalculating a \\(z\\)-score for their temperature (13.6ºC).\nCalculating a \\(p\\)-value for their temperature (13.6ºC).\nReporting a 95% confidence interval for the mean for the temperatures reported in their dataset.\nIn a single sentence, state whether you find this temperature to be significantly warmer than the mean or not, stating what evidence you are using to make this conclusion.\n\n\n\n\n\nBrambilla, A., A. von Hardenberg, B. Bassano, L. Ranghetti, L. Keller, and M. Festa-Bianchet (2024). Data from: Climate drives body mass changes in a mountain ungulate: Shorter winters lead to heavier alpine ibex. [Online.] Available at https://doi.org/10.5061/dryad.w9ghx3fz6.\n\n\nComont, R. (2020). BeeWalk dataset 2008-23. https://doi.org/10.6084/m9.figshare.12280547.v4",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Normality & hypothesis testing</span>"
    ]
  },
  {
    "objectID": "transformations.html",
    "href": "transformations.html",
    "title": "7  Transforming data",
    "section": "",
    "text": "7.1 Importance of normality Data\nFor this walkthrough, we will use two datasets - one with normal data, and one with non-normal data.\nOur normal dataset is drawn from measurements of birds in western Ecuador, collected by several friends of Dr. Cooper, where we will be specifically looking at measurements of the Plain Antvireo Dysithamnus mentalis (Lele et al. 2022).\nOur non-normal dataset counts the number of job vacancies offering an hourly wage by region in the United Kingdom for different time periods (Urban Big Data Centre 2025).\nlibrary(tidyverse)\n\n## Bird data\n\nbirds &lt;- read_csv(\"https://zenodo.org/records/6511860/files/Lele_et_al._2022_BITR-21-373_raw_data.csv\")\n\nantvireo &lt;- birds |&gt; \n  filter(SPECIES == \"PLAIN ANTVIREO\") |&gt; \n  # remove abnormal birds\n  filter(BILL.LENGTH &gt; 5)\n\nantvireo.bill &lt;- antvireo$BILL.LENGTH |&gt; \n  na.omit()\n\n## Job advert data\n\njobs &lt;- read_csv(\"https://zenodo.org/records/14771706/files/adzuna_month_ttwa_vacancies_with_hourly_wage_panel.csv\")\n\n# take a subset\n# whole dataset is too large!\nhourly.jobs &lt;- jobs$vacancies_offering_an_hourly_wage[1:100]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transforming data</span>"
    ]
  },
  {
    "objectID": "transformations.html#testing-if-data-are-normal",
    "href": "transformations.html#testing-if-data-are-normal",
    "title": "7  Transforming data",
    "section": "7.2 Testing if data are normal",
    "text": "7.2 Testing if data are normal\nThere are two major methods we can use to see if data are normally distributed.\n\n7.2.1 Histograms\nOne of the first things that you should do is look at a histogram of your data. Histograms will help you spot any large data irregularities, and can help you get an idea of whether you should expect you data to be non-normally distributed.\nFirst, let’s look at our antvireo.bill data:\n\nhist(antvireo.bill)\n\n\n\n\n\n\n\n\nThese data appear relatively normal.\nNext, let’s look at the hourly.jobs data.\n\nhist(hourly.jobs)\n\n\n\n\n\n\n\n\nThese data appear highly non-normal; most values are low, and we have an extreme right skew.\n\n\n7.2.2 QQ Plots\n\n\nVisual method for assessing normality\n\nOne way to see if data are normal is to use a QQ plot. These plot data quantiles to theoretical quantiles to see how well they align, with a perfectly normal distribution having a completely linear QQ plot. Let’s look at these with our antvireo.bill data.\n\nqqnorm(antvireo.bill)\n\n\n\n\n\n\n\n\nAs we can see above, the data are roughly linear, which means are data appear normal. The “stairsteps” are from the accuracy in measuring temperature, which was likely rounded and thus created a distribution that is not completely continuous.\nNow, lets check how our hourly.jobs data look:\n\nqqnorm(hourly.jobs)\n\n\n\n\n\n\n\n\nAs we can see, these data are not very linear, suggesting that the data are highly non-normal.\n\n\n\n7.2.3 Shapiro-Wilk test\n\n\nStatistical method for assessing normality\n\nAnother way to test for normality is to use a Shapiro-Wilk test of normality. We will not get into the specifics of this distribution, but this tests the null hypothesis that data originated in a normal distribution, with the alternative hypothesis that the data originated in a non-normal distribution.\nNOTE that the Shapiro-Wilk test does not perform well with extremely large datasets, which may be the result of model overfitting.\nThis test uses an \\(\\alpha = 0.05\\), and we reject the null hypothesis if our \\(p &lt; \\alpha\\), with \\(p\\) representing the probability of observing something as extreme or more extreme than the result we observe. If we reject the null hypothesis, our data are non-normal and require transformation. If we accept the null hypothesis, we can proceed with treating our data as normally distributed.\nLet’s look at the antvireo.bill data:\n\nshapiro.test(antvireo.bill)\n\n\n    Shapiro-Wilk normality test\n\ndata:  antvireo.bill\nW = 0.97513, p-value = 0.2984\n\n\nAs we expected from our qqnorm plot, our data for the Plain Antvireo bills are normally distributed, and we accept the null hypothesis as \\(p&gt;\\alpha\\) with \\(0.30 &gt; 0.05\\).\nNext, let’s do a Shapiro-Wilks test on our hourly.jobs data.\n\nshapiro.test(hourly.jobs)\n\n\n    Shapiro-Wilk normality test\n\ndata:  hourly.jobs\nW = 0.39393, p-value &lt; 2.2e-16\n\n\nAs we can see, our \\(p &lt; 0.001\\), therefore we reject the null hypothesis and conclude the hourly.jobs data are non-normal.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transforming data</span>"
    ]
  },
  {
    "objectID": "transformations.html#transforming-data",
    "href": "transformations.html#transforming-data",
    "title": "7  Transforming data",
    "section": "7.3 Transforming data",
    "text": "7.3 Transforming data\nThere are multiply different transformations that can be performed on different datasets. In this class, we will focus on three transformations:\n\nLog transformations, some of the most common transformations\nSquare transformations\nSquare-root transformations\n\nNote: there are many other types of transformations, we just don’t go in depth with them here.\nWhen transforming a dataset, you must perform the mathematical function across all values within the dataset. You can then assess whether this dataset is normally distributed and determine whether you can proceed with statistical analyses that you would perform on normal distributions.\nThe process for all transformations is the same, so this walkthrough will only perform one: the log transformation.\n\n7.3.1 Log transformation example\n\n\nWalkthrough of a transformation for normality\n\nWe know that our hourly.jobs dataset is non-normally distributed, so we need to perform a transformation to see if we can achieve normality. Here, we will perform a log transformation. In R, the function log performs a natural log (\\(ln\\)) by default. Because \\(ln(0)\\) is \\(-Infinity\\), folks will often add \\(+1\\) to their entire dataset to avoid zero values. This is not necessary if your entire dataset is non-zero and positive.\n\n# check if any values are below zero\n# function will return # vals at or below 0\nsum(hourly.jobs &lt;= 0)\n\n[1] 0\n\n\nThere are no values less than or equal to zero in this dataset. However, if we wanted to add one to them, we could do so by just taking hourly.jobs + 1 and creating a new object.\nNext, let’s perform the log transformation:\n\nlog.hourly.jobs &lt;- hourly.jobs |&gt; \n  log()\n\nVery straightforward! How do these data look?\n\nhist(log.hourly.jobs)\n\n\n\n\n\n\n\n\nThese data appear far more normal. What about on a qqplot?\n\nqqnorm(log.hourly.jobs)\n\n\n\n\n\n\n\n\nThe line is much straighter, as is expected for a normal distribution.\nNow, for the Shapiro-Wilk test:\n\nshapiro.test(log.hourly.jobs)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log.hourly.jobs\nW = 0.98204, p-value = 0.1911\n\n\nWe now have a \\(p&gt;\\alpha\\) with \\(0.19&gt;0.05\\), indicating that these data are now normally distributed. We can now proceed with our analyses!\nNote, however, that you must back transform your data after a transformation!\nA back transformation of \\(ln(x)\\) is \\(e^x\\). In R, \\(e\\) is found by entering exp(1). Thus we would do the following if looking at the mean from the logs:\n\nlog.mean &lt;- mean(log.hourly.jobs)\n\nlog.mean |&gt; round(2)\n\n[1] 5.05\n\n\nAnd back transformed:\n\ntransformed.log.mean &lt;- exp(1)^log.mean\ntransformed.log.mean |&gt; round(0)\n\n[1] 156",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transforming data</span>"
    ]
  },
  {
    "objectID": "transformations.html#homework-transforming-data",
    "href": "transformations.html#homework-transforming-data",
    "title": "7  Transforming data",
    "section": "7.4 Homework: Transforming data",
    "text": "7.4 Homework: Transforming data\nThis homework assignment is designed to help you become more familiar with manipulating data and with transforming data.\n\n7.4.1 Analyzing data\nUsing the bird dataset above, pick two other species of birds and do the following:\n\nProvide information on the mean, median, mode, kurtosis, and skewness\nCreate a histogram plot\nDetermine if the data are normally distributed\nIf data are not normally distributed, see if a square, log, or square root transformation will make the data normal.\nProvide an overall assessment of the data’s normality.\n\n\n\n7.4.2 Transforming Data\nUsing the below newly created data object, perform all the same steps as you did for the bird data above.\n\nhourly.jobs.homework &lt;- jobs$vacancies_offering_an_hourly_wage[200:300]\n\n\n\n7.4.3 Submitting assignment\nSubmit your homework assignment as an html file on Canvas.\n\n\n\n\nLele, A., H. Garrod, E. Ferguson, C. Azahara Prieto Gil, and M. Ellis (2022). Morphological measurements in a coastal Ecuadorian avifauna. [Online.] Available at https://zenodo.org/records/6511860.\n\n\nUrban Big Data Centre (2025). Counts hourly pay Adzuna jobs. [Online.] Available at https://zenodo.org/records/14771706.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transforming data</span>"
    ]
  },
  {
    "objectID": "exam2practice.html",
    "href": "exam2practice.html",
    "title": "8  Exam 2 Practice",
    "section": "",
    "text": "8.1 Cyclones\nConsider the following dataset collected in the same year:\n# note - you can combine numbers into a vector like so\n\nx &lt;- c(1,2,3,4,5,6)\n\nx\n\n[1] 1 2 3 4 5 6",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exam 2 Practice</span>"
    ]
  },
  {
    "objectID": "exam2practice.html#cyclones",
    "href": "exam2practice.html#cyclones",
    "title": "8  Exam 2 Practice",
    "section": "",
    "text": "Counts of cyclones per latitudinal band.\n\n\nLatitudinal Band\nSeason\nNumber of cyclones\n\n\n\n\n40 - 49º S\nFall\n370\n\n\n40 - 49º S\nWinter\n452\n\n\n40 - 49º S\nSpring\n273\n\n\n40 - 49º S\nSummer\n422\n\n\n50 - 59º S\nFall\n526\n\n\n50 - 59º S\nWinter\n624\n\n\n50 - 59º S\nSpring\n513\n\n\n50 - 59º S\nSummer\n1059\n\n\n60 - 69º S\nFall\n980\n\n\n60 - 69º S\nWinter\n1200\n\n\n60 - 69º S\nSpring\n995\n\n\n60 - 69º S\nSummer\n1751\n\n\n\n\nClassify each of the three variables as either an explanatory or a response variable. Justify your answer.\nClassify each of the three variables as nominal, ordinal, interval or ratio data. Justify your answer.\nState the null and alternative hypotheses for this scenario. Note: there are two sets of null/alternative hypotheses.\nCalculate the mean, median, and mode of your response variable. Based on your results, would you expect your data to be normally distributed or not? Justify your answer.\n\n\n\nLet’s say now that you’re testing the null hypothesis that the mean number of cyclones in this one year is similar to the average year. The mean number of cyclones per year across all years of data collected is 650 cyclones with a standard deviation of 425. Calculate the \\(z\\)-score and make a decision regarding the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exam 2 Practice</span>"
    ]
  },
  {
    "objectID": "exam2practice.html#minnows",
    "href": "exam2practice.html#minnows",
    "title": "8  Exam 2 Practice",
    "section": "8.2 Minnows",
    "text": "8.2 Minnows\nConsider this scenario: You have discovered a never-before-documented population of minnow in the Kearney Canal near campus. During your first sampling trip, you notice that the total length (i.e., measured from the tip of the snout to the very tip of the tail) of the fish you measure appear to be smaller than the average total length of the species as recorded among all known individuals across their range. The mean total length noted in one publication is 85.00 mm with a standard deviation of 4.50. Below is your data the data from 20 minnows that you captured during your first sampling trip to the Kearney Canal:\n\nFish data for this problem.\n\n\nFish ID\nLength (mm)\n\n\n\n\n1\n89.58\n\n\n2\n75.44\n\n\n3\n86.86\n\n\n4\n74.71\n\n\n5\n69.70\n\n\n6\n100.34\n\n\n7\n73.70\n\n\n8\n69.56\n\n\n9\n96.24\n\n\n10\n79.35\n\n\n11\n61.37\n\n\n12\n62.82\n\n\n13\n95.45\n\n\n14\n98.71\n\n\n15\n100.34\n\n\n16\n57.57\n\n\n17\n70.54\n\n\n18\n78.65\n\n\n19\n65.39\n\n\n20\n65.57\n\n\n\n\nState the null and alternative hypotheses for this study.\nCalculate the first quartile, median, third quartile, and interquartile range of your response variable. Create a boxplot. Based on your results, are there any outliers in your data? Explain.\nCalculate the \\(z\\)-score for this scenario.\nWhat is the probability that, by random chance alone, you would find your observed mean or something more extreme?\nAssume that you set your \\(\\alpha\\) for your study prior to your data collection to 0.05. Based on this information and the \\(p\\) value you obtained, is your null hypothesis supported or rejected?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exam 2 Practice</span>"
    ]
  },
  {
    "objectID": "probdist.html",
    "href": "probdist.html",
    "title": "9  Probability distributions",
    "section": "",
    "text": "9.1 Probability distributions\nWe rely on multiple different probability distributions to help us understand what probable outcomes are for a specific scenario. All of the tests that we are performing are comparing our results to what we would expect under perfectly random scenarios. For example, if we are flipping a coin, we are interested in whether the the observation we have of the flips on our coin matches our expectation given the probability of getting heads or tails on a perfectly fair coin. While it is possible to get all heads or all tails on a coin flip, it is highly unlikely and may lead us to believe we have an unfair coin. The more trails we perform, the more confident we can be that out coin is atypical.\nWe perform similar comparisons for other distributions. If we are comparing sets of events, we can look at the probability of those events occurring if events are occurring randomly. If we are comparing counts, we can compare our counts to our expectation of counts if events or subjects are distributed randomly throughout the matrix or whether two sets of counts are likely under the same sets of assumptions.\nRemember, for our specific tests, we are setting an \\(\\alpha\\) value in advance (traditionally \\(0.05\\), or \\(5\\)%) against which we compare our \\(p\\) value, with \\(p\\) representing the probability of observing an event as extreme or more extreme than the event we observe given a specific probability distribution.\nPreviously, we talked about the normal distribution, which is used to approximate a lot of datasets in nature. However, several other probability distributions are also useful for biological systems, which are outlined here.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#review-basic-statistics",
    "href": "probdist.html#review-basic-statistics",
    "title": "9  Probability distributions",
    "section": "9.2 Review: Basic statistics",
    "text": "9.2 Review: Basic statistics\nIf we are looking at things that could be one or the other thing, we add the probabilities. For example, the odds of getting a four or a six on a 20 sided die are \\(\\frac{1}{20}+\\frac{1}{20}=\\frac{2}{20}=\\frac{1}{10}\\).\nIf we are looking at things that could be one thing and another thing, we multiply the probabilities together. For example, if we roll two six-sided die and we are looking at the odds of a 2 or a 4 on both die, the odds are \\((\\frac{1}{6}+\\frac{1}{6})\\cdot(\\frac{1}{6}+\\frac{1}{6})=\\frac{1}{3}\\cdot\\frac{1}{3}=\\frac{1}{9}\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#binomial-distribution",
    "href": "probdist.html#binomial-distribution",
    "title": "9  Probability distributions",
    "section": "9.3 Binomial distribution",
    "text": "9.3 Binomial distribution\nA binomial distribution is one in which only two outcomes are possible - often coded as \\(0\\) and \\(1\\) and usually representing failure and success, respectively. The binomial is described by the following function:\n\\[\np(x)=\\binom{n}{x}p^{x}(1-p)^{n-x}\n\\]\nwhere \\(n =\\) number of trials, \\(x =\\) the number of successes, and \\(p =\\) the probability of a success under random conditions.\nIn R, the binomial distribution is represented by the following functions:\n\ndbinom: the density of a binomial distribution\npbinom: the distribution function, or the probability of a specific observation\nqbinom: the value at which a specific probability is found (the quantile function)\nrbinom: generates random values according to a binomial.\n\n\n9.3.1 Binomial examples\n\n\nBinomial example walkthrough\n\nLet’s see what this looks like. Let’s consider a scenario where we flip a coin 10 times and get 9 heads. How likely is this outcome?\n\nx &lt;- pbinom(q = 9, # number successes, 9 heads\n            size = 10, # number of trials, 10 flips\n            prob = 0.5) # probability with a fair coin\n\nround(x,4)\n\n[1] 0.999\n\n\nNOTE that the trailing \\(0\\) is dropped, such that the real answer is \\(0.9990\\). However, we mentioned before that the \\(p\\) value should be the probability of a result as extreme or more extreme, meaning that it should always be less than \\(0.5\\). If we are reporting a value of greater than \\(0.5\\), then we are comparing to the upper tail of the distribution. For a one-tailed \\(\\alpha\\) of \\(0.05\\), this would mean that we are looking for a value greater than \\(0.95\\) (\\(1-\\alpha\\)).\nSo, our real \\(p\\) is:\n\n1 - round(x,4)\n\n[1] 0.001\n\n\nAgain, the trailing zero is missing. Given that \\(p &lt; \\alpha\\), we reject the null hypothesis that this is a fair coin.\nHow does this distribution look?\n\n# number of successes\n# start at 0 for no heads\nx &lt;- 0:10\n# cumulative probability to left of outcome\ny &lt;- pbinom(x,\n            size = 10, \n            prob = 0.5, \n            lower.tail = T)\n\n# cumulative probability of results to the left\nplot(x,\n     y,\n     type=\"l\") # line plot\n\n\n\n\n\n\n\n\nWhat about if we always have \\(p\\) less than \\(0.5\\) to reflect two tails?\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,\n     y,\n     type=\"l\")\n\n\n\n\n\n\n\n\nWhat if we do this with a bigger dataset, like for \\(50\\) flips?\n\n# number of successes\n# start at 0 for no heads\nx &lt;- 0:50\n# cumulative probability to left of outcome\ny &lt;- pbinom(x,\n            size = length(x), \n            prob = 0.5, \n            lower.tail = T)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,\n     y,\n     type=\"l\")\n\n\n\n\n\n\n\n\nAs we increase the number of flips, we can see that the probability of success forms a normal distribution centered on the outcome given the default probability. Thus, as we deviate from our expected outcome (initial probability multiple by the number of trials), then our results become less likely.\n\n\n\n9.3.2 Binomial exact tests\n\n\nBinomial exact test walkthrough\n\nWe can perform exact binomial tests by using the R function binom.test. This is a built in function within R. This test requires the following arguments:\n\nx: number of successes (success = outcome of interest)\nn: number of trials (number of events)\np: probability of success in a typical situation (i.e., for a fair coin, this is \\(50\\)%)\nalternative: the hypothesis to be tested, whether two.sided, greater, or less.\nconf.level is the confidence level to be returned; default is \\(95\\)%.\n\nLet’s say you flip a coin ten times, randomly assigning one side as a “success” and one side as a “failure”. We do ten flips, and get 3 “successes”. How likely is this outcome?\n\nbinom.test(x = 3,   # three successes\n           n = 10,  # ten flips\n           p = 0.5) # 50% chance on fair coin\n\n\n    Exact binomial test\n\ndata:  3 and 10\nnumber of successes = 3, number of trials = 10, p-value = 0.3438\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.06673951 0.65245285\nsample estimates:\nprobability of success \n                   0.3 \n\n\nNow let’s say we do 1000 flips, and we get 300 successes.\n\nbinom.test(x = 300,\n           n = 1000,\n           p = 0.5)\n\n\n    Exact binomial test\n\ndata:  300 and 1000\nnumber of successes = 300, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2717211 0.3294617\nsample estimates:\nprobability of success \n                   0.3 \n\n\nAs we can see, both of these return a confidence interval among other things. If we save the object, we can access these “slots” of data using the $ character.\n\nbinom_result &lt;- binom.test(x = 3,\n                           n = 10,\n                           p = 0.5)\n\nbinom_result$p.value |&gt; round(2)\n\n[1] 0.34\n\n\n\nbinom_result$conf.int\n\n[1] 0.06673951 0.65245285\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThis test is easily implemented, but always double check and make sure you are setting it up correctly.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#poisson-distribution",
    "href": "probdist.html#poisson-distribution",
    "title": "9  Probability distributions",
    "section": "9.4 Poisson distribution",
    "text": "9.4 Poisson distribution\nThe Poisson distribution is used to reflect random count data. Specifically, the Poisson is used to determine if success events are overdispersed (i.e., regularly spaced), random, or underdispersed (i.e., clustered). The Poisson introduces the variable lambda (\\(\\lambda\\)) which represents the mean (\\(\\mu\\)) and the variance (\\(\\sigma^2\\)), which are equal in a Poisson distribution. A Poisson distribution is described by the following function:\n\\[\np(x)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\]\nThe Poisson is represented by the following functions in R which closely resemble the functions for the normal and binomial distributions:\n\ndpois: the log density function\nppois: log distribution (probability) function\nqpois: quantile function\nrpois: random values from a Poisson.\n\n\n9.4.1 Poisson example\nLet’s look at the probability of \\(0\\) to \\(10\\) successes when we have our \\(\\lambda=1\\).\n\nx &lt;- 0:10\ny &lt;- ppois(x,lambda = 1)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")\n\n\n\n\n\n\n\n\nAs we can see, the probability of rare events is high, whereas the probability quickly decreases as the number of successes increases.\n\n\n9.4.2 Poisson test\nMuch like the Binomial Distribution and its binom.test, we can use poisson.test to analyze data via a Poisson Distribution. This command uses the arguments:\n\nx: number of events of interest\nT: time base (if for an event count)\nr: hypothesized rate or ratio\nalternative and conf.level are the same as for binom.test\n\nWe will not often use the poisson.test in this class, but it is good to be aware of.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#cumulative-probabilities",
    "href": "probdist.html#cumulative-probabilities",
    "title": "9  Probability distributions",
    "section": "9.5 Cumulative Probabilities",
    "text": "9.5 Cumulative Probabilities\nWith both the Binomial and the Poisson, we can calculate cumulative probabilities. Both of these require the density (d) versions of the arguments.\n\n9.5.1 Binomial cumulative\n\n\nObtaining cumulative probability in a binomial distribution\n\nLet’s say we have ten trials with three successes and a base probability of \\(0.5\\). We can calculate the probability to the left by using the following:\n\npbinom(q = 3,\n       size = 10,\n       prob = 0.5)\n\n[1] 0.171875\n\n\nAs we can see, this is ~17.19%. Now let’s try using dbinom. This command gives us the value at an individual bin, given that it is a more discrete distribution for these smaller sample sizes.\n\ndbinom(x = 0:3, \n       size = 10, \n       prob = 0.5)\n\n[1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000\n\n\nAbove, we can see the probability of each number of successes three and fewer, for 0, 1, 2, and 3. Let’s sum these probabilities.\n\ndbinom(x = 0:3, \n       size = 10, \n       prob = 0.5) |&gt;\n  sum()\n\n[1] 0.171875\n\n\nAs we can see, we get the same value as for pbinom! We can use this method for finding very specific answers, like what is the probability of getting between 3 and 6 successes in ten trials?\n\ndbinom(x = 3:6,\n       size = 10,\n       prob = 0.5) |&gt; \n  sum() |&gt; \n  round(4)\n\n[1] 0.7734\n\n\nThe probability of getting one of these outcomes is 77.34%.\n\n\n\n9.5.2 Poisson cumulative probability\n\n\nCumulative probability for Poisson distributions\n\nLikewise, we can use ppois to get the \\(p\\) value and dpois to get the distribution function of specific outcomes. So, let’s say we have a scenario with a \\(\\lambda = 0.5\\) and we are looking at the probability of 2 successes or greater. In this case, we have an infinite series, which we can’t calculate. However, we can calculate the probability of what it isn’t and then subtract from 1. In this case, we are looking for the probability of not having 0 or 1 successes.\n\ndpois(x = 0:1,\n      lambda = 0.5)\n\n[1] 0.6065307 0.3032653\n\n\nNow, let’s sum this and subtract it from 1.\n\n1 - dpois(x = 0:1, lambda = 0.5) |&gt; \n  sum()\n\n[1] 0.09020401\n\n\nThe probability is only about 9%.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "probdist.html#homework",
    "href": "probdist.html#homework",
    "title": "9  Probability distributions",
    "section": "9.6 Homework",
    "text": "9.6 Homework\nRemember - your homework must be completed in an rmarkdown document submitted as an html file.\nFor each question, you must determine whether a binomial or a Poisson distribution is most appropriate. (Provide justification on your document for why you chose the distribution you did). After picking the correct distribution, answer the statistical question posed.\nSome of the inspiration for these problems is drawn from Statology.\n\n9.6.1 Problem 1\nThere is a claw machine at the local fair that is notoriously difficult to use. In fact, it is estimated that only 1 in 50 attempts are successful at getting a toy out of the machine. What is the probability that you would try it 50 times and win thrice?\n\n\n9.6.2 Problem 2\nWhooping Cranes Grus americanus are some of the most endangered birds in North America, with only 557 birds in the population that migrates through Nebraska. Conversely, approximately 736,000 Sandhill Cranes Antigone canadensis migrate through Nebraska. Assuming that census counts of flocks of 5000 cranes have the expected average ratio of cranes, what are the odds of finding a crane flock with two or more Whooping Cranes?\n\n\n9.6.3 Problem 3\nYour friend has a twenty-sided dice that you suspect to be unfair. They have rolled a 20 four times in the past thirty rolls. Is the dice unfair?\n\n\n9.6.4 Problem 4\nYou roll three die at a single time. What are the odds of getting exactly two fours?\n\n\n9.6.5 Problem 5\nYou notice that in a class of 200 people, the odds of any given student being absent is approximately 3%. What are the odds that 5 people miss a given class?\n\n\n9.6.6 Problem 6\nThere are approximately 3 severe storm warnings per 16 week semester at UNK. What are the odds of having a semester with no severe storm warnings?\n\n\n9.6.7 Problem 7\nYou are giving mice an experimental treatment that is designed to change their fur color. In a trial of 500 mice, 15 had their fur change color. Using this precedent, are the odds of having 6 mice have their fur change color in another trial of 500 individuals?\n\n\n9.6.8 Problem 8\nWhat are the odds of rolling no fours on two twenty-sided die?\n\n\n9.6.9 Problem 9\nDr. Cooper once caught his mom shaking sodas and putting them back at the shelves at a gas station while they were traveling. Assuming his mom was able to shake 10% of the sodas before getting caught, what are the odds of getting three or more shaken sodas if you buy 20 sodas?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "chi.html",
    "href": "chi.html",
    "title": "10  χ2 (Chi-squared) tests",
    "section": "",
    "text": "10.1 \\(\\chi^2\\)-squared distribution\n\\(\\chi^2\\)-squared (pronounced “kai”, and spelled “chi”) is a distribution used to understand if count data between different categories matches our expectation. For example, if we are looking at students in the class and comparing major vs. number of books read, we would expect no association, however we may find an association for a major such as English which required reading more literature. The \\(\\chi^2\\) introduces a new term degrees of freedom (\\(df\\)) which reflects the number of individuals in the study. For many tests, \\(df\\) are needed to reflect how a distribution changes with respect the number of individuals (and amount of variation possible) within a dataset. The equation for the \\(\\chi^2\\) is as follows, with the \\(\\chi^2\\) being a special case of the gamma (\\(\\gamma\\) or \\(\\Gamma\\)) distribution that is affected by the \\(df\\), which is defined as the number of rows minus one multiplied by the number of columns minus one \\(df = (rows-1)(cols-1)\\):\n\\[\nf_n(x)=\\frac{1}{2^{\\frac{n}{2}}\\Gamma(\\frac{n}{2})}x^\\frac{n}{2-1}e^\\frac{-x}{2}\n\\]\nThe \\(\\chi^2\\)-squared distribution is also represented by the following functions, which perform the same things as the previous outlined equivalents for Poisson and binomial:\nWe can view these probabilities as well:\nx &lt;- 0:10\n\ny &lt;- pchisq(x, df = 9)\n\n# any value greater than 0.5 is subtracted from 1\ny[y &gt; 0.5] &lt;- 1 - y[y &gt; 0.5]\n\nplot(x,y,type=\"l\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#chi2-squared-distribution",
    "href": "chi.html#chi2-squared-distribution",
    "title": "10  χ2 (Chi-squared) tests",
    "section": "",
    "text": "dchisq\npchisq\nqchisq\nrchisq\n\n\n\n\n10.1.1 Calculating the test statistic\nWe evaluate \\(\\chi^2\\) tests by calculating a \\(\\chi^2\\) value based on our data and comparing it to an expected \\(\\chi^2\\) distribution. This test statistic can be evaluated by looking at a \\(\\chi^2\\) table or by using R. Note that you need to know the degrees of freedom in order to properly evaluate a \\(\\chi^2\\) test. We calculate our test statistic as follows:\n\\[\n\\chi^2=\\Sigma\\frac{(o-e)^2}{e}\n\\]\nwhere \\(e =\\) the number of expected individuals and \\(o =\\) the number of observed individuals in each category. Since we are squaring these values, we will only have positive values, and thus this will always be a one-tailed test.\nThere are multiple types of \\(\\chi^2\\) test, including the following we will cover here:\n\n\\(\\chi^2\\) Goodness-of-fit test\n\\(\\chi^2\\) test of independence\n\n\n\n10.1.2 \\(\\chi^2\\) goodness-of-fit test\nA \\(\\chi^2\\) goodness-of-fit test looks at a vector of data, or counts in different categories, and asks if the observed frequencies vary from the expected frequencies.\n\n10.1.2.1 \\(\\chi^2\\) estimate by hand\nLet’s say, for example, we have the following dataset:\n\n\n\nHour\nNo. Drinks Sold\n\n\n\n\n6-7\n3\n\n\n7-8\n8\n\n\n8-9\n15\n\n\n9-10\n7\n\n\n10-12\n5\n\n\n12-13\n20\n\n\n13-14\n18\n\n\n14-15\n8\n\n\n15-16\n10\n\n\n16-17\n12\n\n\n\nNow, we can ask if the probability of selling drinks is the same across all time periods.\n\ndrinks &lt;- c(3, 8, 15, 7, 5, 20, 18, 8, 10, 12)\n\nWe can get the expected counts by assuming an equal probability for each time period; thus, \\(Exp(x)=\\frac{N}{categories}\\).\n\n# sum all values for total number\nN &lt;- sum(drinks)\n\n# get length of vector for categories\ncats &lt;- length(drinks)\n\n# repeat calculation same number of times as length\nexp_drinks &lt;- rep(N/cats, cats)\n\nexp_drinks\n\n [1] 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6\n\n\nNow, we can do out \\(\\chi^2\\) calculation.\n\nchi_vals &lt;- ((drinks - exp_drinks)^2)/exp_drinks\n\nchi_vals\n\n [1] 5.44905660 0.63773585 1.82641509 1.22264151 2.95849057 8.33584906\n [7] 5.16603774 0.63773585 0.03396226 0.18490566\n\n\n\nsum(chi_vals)\n\n[1] 26.45283\n\n\nAnd now, to get the probability.\n\nchi_vals |&gt; \n  # get chi statistic\n  sum() |&gt; \n  # get p value\n  pchisq(df = length(chi_vals) - 1,\n         # looking RIGHT\n         lower.tail = F)\n\n[1] 0.001721825\n\n\nHere, we get \\(p = 0.002\\), indicating that there is not an equal probability for selling drinks at different times of day.\n\n\n10.1.2.2 \\(\\chi^2\\) estimation by code\nWe can use the test chisq.test to perform this analysis as well.\n\nchisq.test(drinks)\n\n\n    Chi-squared test for given probabilities\n\ndata:  drinks\nX-squared = 26.453, df = 9, p-value = 0.001722\n\n\nAs we can see, these values are exactly the same as we just calculated by hand! Note that we can define the probability p if we want, otherwise it defaults to p = rep(1/length(x), length(x)).\n\n\n\n10.1.3 \\(\\chi^2\\) test of independence\nUsually when we use a \\(\\chi^2\\), we are looking at count data. Let’s consider the following hypothetical scenario, comparing experience with R between non-biology majors (who, in this theoretical scenario, do not regularly use R) and Biology majors who are required to take R for this class:\n\nThe above table of counts is also known as a contingency table. Intuitively, we can see a difference, but we want to perform a statistical test to see just how likely these counts would be if both groups were equally likely. We can calculate this both “by hand” and using built in R functions.\n\n\nMajor\nR experience\nNo R experience\n\n\n\n\nNon-biology\n3\n10\n\n\nBiology\n9\n2\n\n\n\n\n10.1.3.1 \\(\\chi^2\\) estimations by hand\nFirst, we can enter the data into R.\n\ndata &lt;- matrix(data = c(3,10,9,2), nrow = 2, ncol = 2, byrow = T)\n\ncolnames(data) &lt;- c(\"R\", \"No R\")\nrownames(data) &lt;- c(\"Non-biology\", \"Biology\")\n\ndata\n\n            R No R\nNon-biology 3   10\nBiology     9    2\n\n\nNext, we need the observed - expected values. We determine expected values either through probability (\\(0.5 \\cdot n\\) for equal probability for two categories) or via calculating the the expected values (see later section on expected counts). In this case, since we are looking at equally likely in each cell, we have an expected matrix as follows:\n\n# total datapoints\nN &lt;- sum(data)\n\nexpected &lt;- matrix(data = c(0.25*N,0.25*N,0.25*N,0.25*N), nrow = 2, ncol = 2, byrow = T)\n\ncolnames(expected) &lt;- c(\"R\", \"No R\")\nrownames(expected) &lt;- c(\"Non-biology\", \"Biology\")\n\nexpected\n\n            R No R\nNon-biology 6    6\nBiology     6    6\n\n\nNow we need to find our observed - expected.\n\no_e &lt;- data - expected\n\no_e\n\n             R No R\nNon-biology -3    4\nBiology      3   -4\n\n\nNote that in R we can add and subtract matrices, so there’s no reason to reformat these data!\nNow, we can square these data.\n\no_e2 &lt;- o_e^2\n\no_e2\n\n            R No R\nNon-biology 9   16\nBiology     9   16\n\n\nNext, we take these and divide them by the expected values and then sum those values.\n\nchi_matrix &lt;- o_e2/expected\n\nchi_matrix\n\n              R     No R\nNon-biology 1.5 2.666667\nBiology     1.5 2.666667\n\n\n\nsum(chi_matrix)\n\n[1] 8.333333\n\n\nHere, we get a \\(\\chi^2\\) value of 8.3333333. We can use our handy family functions to determine the probability of this event:\n\nchi_matrix |&gt; \n  # get chi statistic\n  sum() |&gt; \n  pchisq(df = 1,\n         # looking RIGHT\n         lower.tail = F)\n\n[1] 0.003892417\n\n\nHere, we get a \\(p\\) value of:\n\nchi_matrix |&gt; \n  sum() |&gt; \n  pchisq(df = 1, lower.tail = F) |&gt; \n  round(3)\n\n[1] 0.004\n\n\nAlternatively, we can calculate this using expected counts. For many situations, we don’t know what the baseline probability should be, so we calculate the expected counts based on what we do know. Expected counts are calculated as follows:\n\\[\nExp(x)=\\frac{\\Sigma(row_x)\\cdot\\Sigma(col_x)}{N}\n\\]\nwhere \\(N\\) is the sum of all individuals in the table. For the above example, this would look like this:\n\ndata_colsums &lt;- colSums(data)\ndata_rowsums &lt;- rowSums(data)\nN &lt;- sum(data)\n\nexpected &lt;- matrix(data = c(data_colsums[1]*data_rowsums[1],\n                            data_colsums[2]*data_rowsums[1],\n                            data_colsums[1]*data_rowsums[2],\n                            data_colsums[2]*data_rowsums[2]),\n                   nrow = 2, ncol = 2, byrow = T)\n\n# divide by total number\nexpected &lt;- expected/N\n\ncolnames(expected) &lt;- colnames(data)\nrownames(expected) &lt;- rownames(data)\n\nexpected\n\n              R No R\nNon-biology 6.5  6.5\nBiology     5.5  5.5\n\n\nHere, we can see our expected number are not quite 50/50! this will give us a different result than our previous iteration.\n\ne_o2 &lt;- ((data - expected)^2)/expected\n\nsum(e_o2)\n\n[1] 8.223776\n\n\nNow we have a \\(\\chi^2\\) of:\n\ne_o2 |&gt; \n  sum() |&gt; \n  round(2)\n\n[1] 8.22\n\n\nAs we will see below, is the exact same value as we get for an uncorrected chisq.test from R’s default output.\n\n\n10.1.3.2 \\(\\chi^2\\) estimations in R\nWe can calculate this in R by entering in the entire table and using chisq.test.\n\ndata\n\n            R No R\nNon-biology 3   10\nBiology     9    2\n\n\n\nchi_data &lt;- chisq.test(data, correct = F)\n\nchi_data\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 8.2238, df = 1, p-value = 0.004135\n\n\nHere, we get the following for your \\(p\\) value:\n\nchi_data$p.value |&gt; \n  round(3)\n\n[1] 0.004\n\n\nThis is significant with \\(\\alpha = 0.05\\).\nNote that these values are slightly different. they will be even more different if correct is set to TRUE. By default, R, uses a Yate’s correction for continuity. This accounts for error introduced by comparing the discrete values to a continuous distribution.\n\nchisq.test(data)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  data\nX-squared = 6.042, df = 1, p-value = 0.01397\n\n\nApplying this correction lowers the degrees of freedom, and increases the \\(p\\) value, thus making it harder to get \\(p &lt; \\alpha\\).\nNote that the Yate’s correction is only applied for 2 x 2 contingency tables.\nGiven the slight differences in calculation between by hand and what the functions of R are performing, it’s important to always show your work.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#fishers-exact-test",
    "href": "chi.html#fishers-exact-test",
    "title": "10  χ2 (Chi-squared) tests",
    "section": "10.2 Fisher’s exact test",
    "text": "10.2 Fisher’s exact test\n\\(\\chi^2\\) tests don’t work in scenarios where we have very small count sizes, such as a count size of 1. For these situations with small sample sizes and very small count sizes, we use Fisher’s exact test. This test gives us the \\(p\\) value directly - no need to use a table of any kind! Let’s say we have a \\(2x2\\) contingency table, as follows:\n\n\n\n\\(a\\)\n\\(b\\)\n\n\n\\(c\\)\n\\(d\\)\n\n\n\nWhere row totals are \\(a+b\\) and \\(c + d\\) and column totals are \\(a + c\\) and \\(b + d\\), and \\(n = a + b + c + d\\). We can calculate \\(p\\) as follows:\n\\[\np = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!n!}\n\\]\nIn R, we can use the command fisher.test to perform these calculations. For example, we have the following contingency table, looking at the number of undergrads and graduate students in introductory and graduate level statistics courses:\n\n\n\n\nUndergrad\nGraduate\n\n\n\n\nIntro Stats\n8\n1\n\n\nGrad Stats\n3\n5\n\n\n\n\nstats_students = matrix(data = c(8,1,3,5),\n                        byrow = T, ncol = 2, nrow = 2)\n\nstats_students\n\n     [,1] [,2]\n[1,]    8    1\n[2,]    3    5\n\n\n\nfisher.test(stats_students)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  stats_students\np-value = 0.04977\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.7934527 703.0167380\nsample estimates:\nodds ratio \n  11.10917 \n\n\nIn this situation, \\(p = 0.05\\) when rounded, so we would fail to reject but note that this is borderline.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "chi.html#homework-chi2-tests",
    "href": "chi.html#homework-chi2-tests",
    "title": "10  χ2 (Chi-squared) tests",
    "section": "10.3 Homework: \\(\\chi^2\\) tests",
    "text": "10.3 Homework: \\(\\chi^2\\) tests\n\n10.3.1 La Selva Bird Counts\nThe La Selva station in Costa Rica performed Christmas Bird Counts for 23 years between 1989-2011 and used these data to analyze the status and trends of different birds within the region (Boyle and Sigel 2016). Here, we will look at their data for some of the “big groundbirds” - namely, members of the families Tinamidae and Cracidae.\n\n# download La Selva data\nlaselva &lt;- read_csv(\"https://zenodo.org/records/4979259/files/LaSelvaBirdTrendsDatatable.csv\")\n\nRows: 202 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): ScientificName, CommonName, QualChg, Prob&gt;ChiSq, Diet_AB, Diet_BS,...\ndbl (10): MaxN, MeanAbund, NYears, TrendEstimate, StdError, L-RChiSquare, Me...\nlgl  (2): Extirpated?, Invading?\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# isolate the big groundbirds\nbig_groundbirds &lt;- laselva[1:6,]\n\nThe researchers are curious about the following questions; use \\(\\chi^2\\) tests to answer them appropriately. For each, don’t forget to also state your null and alternative hypotheses, as well as your conclusion about whether you support or reject the null.\n\nIs each species equally likely to be encountered across years (NYears)?\nDoes each species have an equal high count (MaxN)?\n\n\n\n10.3.2 Bicycle counts: Time of day\nWe are going to revisit the bicycle data of Weber (2019) to examine two different temporal patterns in bridge use.\nOne question that the authors had was whether equal numbers of people use the bridge at different times of day between 8:00 and 17:00 on March 31st.\nFirst, run the following to load the whole dataset.\n\nbicycles &lt;- read_csv(\"https://zenodo.org/records/2648564/files/Fremont_Bridge_Hourly_Bicycle_Counts_by_Month_October_2012_to_present.csv\")\n\nRows: 56904 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date\ndbl (2): Fremont Bridge East Sidewalk, Fremont Bridge West Sidewalk\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbicycles &lt;- bicycles |&gt; \n  # convert to data frame\n  as.data.frame() |&gt; \n  # convert to date format\n  mutate(Date = as.POSIXct(Date, format = \"%m/%d/%Y %I:%M:%S %p\")) |&gt; \n  #subset to March 30-31, 2019\n  filter(year(Date) == 2019) |&gt; \n  filter(month(Date) == 3) |&gt; \n  filter(day(Date) == 30 | day(Date) == 31)\n\nNext, we will isolate the data of interest for 8:00 and 17:00 on March 31st.\n\nmarch_31 &lt;- bicycles |&gt; \n  filter(day(Date) == 31) |&gt; \n  filter(hour(Date) &gt; 7 & hour(Date) &lt; 18)\n\nUsing a \\(\\chi^2\\) test, determine if the bicycle counts are the same for each hour in the march_31 object for Eastbound traffic, for Westbound traffic, and for comparing Eastbound and Westbound traffic. Please list your null hypothesis, alternative hypothesis, and your conclusions.\n\n\n10.3.3 Bicycles: Date\nThe researchers are also curious if the rate of bicyclists crossing the bridge were the same on March 30 and March 31 in 2019 between 8:00-17:00 for eastbound traffic. First, we will create a new comparative data frame. Note how to do this, as you may have to do something similar on an exam.\n\nmarch_30 &lt;- bicycles |&gt; \n  filter(day(Date) == 30) |&gt; \n  filter(hour(Date) &gt; 7 & hour(Date) &lt; 18)\n\n# combine data frame by hour\n\n# convert dates to hours\nmarch_30 &lt;- march_30 |&gt; \n  select(-`Fremont Bridge West Sidewalk`) |&gt; \n  mutate(Date = hour(Date)) |&gt; \n  rename(\"March 30\" = `Fremont Bridge East Sidewalk`)\n\ndate_compare_data &lt;- march_31 |&gt;  \n  select(-`Fremont Bridge West Sidewalk`) |&gt; \n  mutate(Date = hour(Date)) |&gt; \n  rename(\"March 31\" = `Fremont Bridge East Sidewalk`) |&gt; \n  # combine data frames\n  full_join(march_30, by = \"Date\") |&gt; \n  rename(\"Hour\" = Date)\n\nPerform your \\(\\chi^2\\) test to compare these dates. Don’t forget your null hypothesis, alternative hypothesis, and conclusion.\n\n\n\n\nBoyle, W. A., and B. J. Sigel (2016). Data from: Ongoing changes in the avifauna of la selva biological station, costa rica: Twenty-three years of christmas bird counts. [Online.] Available at https://doi.org/10.5061/dryad.65v10.\n\n\nWeber, G. (2019). Fremont Bridge Hourly Bicycle Counts by Month October 2012 to present. [Online.] Available at https://zenodo.org/records/2648564.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>χ2 (Chi-squared) tests</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "11  Means testing",
    "section": "",
    "text": "11.1 Introduction\nPreviously, we talked about normal distributions as a method for comparing samples to overall populations or comparing individuals to overall populations. However, sample sizes can introduce some error, and oftentimes we may not have access to an entire population. In these situations, we need a better test that can account for this changing error and the effect of different sample sizes. This is especially important when comparing two samples to each other. We may find a small sample from one population and a small sample for another, and we want to determine if these came from the same overall population as effectively as possible.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#introduction",
    "href": "ttest.html#introduction",
    "title": "11  Means testing",
    "section": "",
    "text": "11.1.1 Parametric and Non-parametric tests\nWe have divided tests into parametric and non-parametric tests below. Parametric tests are those that follow a normal distribution; non-parametric tests violate this expectation.\nRemember, parametric tests are more powerful and preferred in all circumstances. If your data are not parametric, you will first have to see if the data can be transformed. If the data cannot be transformed, then you can proceed with a non-parametric test.\n\n\n11.1.2 A little history\n\n\nWhy is it called “Student’s t test”?\n\nThe distribution that we commonly refer to as a \\(t\\)-distribution is also sometimes known as a “Student’s \\(t\\)-distribution” as it was first published by a man with the pseudonym of “Student”. Student was in fact William Sealy Gossett, an employee of the Guinness corporation who was barred from publishing things by his employer to ensure that trade secrets were not made known to their competitors. Knowing that his work regarding statistics was important, Gossett opted to publish his research anyway under his pseudonym.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#dataset",
    "href": "ttest.html#dataset",
    "title": "11  Means testing",
    "section": "11.2 Dataset",
    "text": "11.2 Dataset\nFor all of the examples on this page, we will be using a dataset on the morphology of canine teeth for identification of predators killing livestock (Courtenay 2019).\n\ncanines &lt;- read_csv(\"https://figshare.com/ndownloader/files/15070175\")\n\nWe want to set up some of these columns as “factors” to make it easier to process and parse in R. We will look at the column OA for these examples. Unfortunately, it is unclear what exactly OA stands for since this paper is not published at the present time.\n\ncanines$Sample &lt;- as.factor(canines$Sample)\n\n# we will be examining the column \"OA\"\n\ncanines$OA &lt;- as.numeric(canines$OA)\n\nsummary(canines)\n\n  Sample        WIS              WIM              WIB         \n Dog :34   Min.   :0.1323   Min.   :0.1020   Min.   :0.03402  \n Fox :41   1st Qu.:0.5274   1st Qu.:0.3184   1st Qu.:0.11271  \n Wolf:28   Median :1.1759   Median :0.6678   Median :0.25861  \n           Mean   :1.6292   Mean   :1.0233   Mean   :0.44871  \n           3rd Qu.:2.4822   3rd Qu.:1.5194   3rd Qu.:0.74075  \n           Max.   :4.8575   Max.   :3.2423   Max.   :1.51721  \n       D                 RDC               LDC                OA       \n Min.   :0.005485   Min.   :0.05739   Min.   :0.02905   Min.   :100.7  \n 1st Qu.:0.034092   1st Qu.:0.28896   1st Qu.:0.22290   1st Qu.:139.2  \n Median :0.182371   Median :0.61777   Median :0.55985   Median :149.9  \n Mean   :0.250188   Mean   :0.88071   Mean   :0.84615   Mean   :148.4  \n 3rd Qu.:0.361658   3rd Qu.:1.26417   3rd Qu.:1.26754   3rd Qu.:158.0  \n Max.   :1.697461   Max.   :3.02282   Max.   :3.20533   Max.   :171.5",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#parametric-tests",
    "href": "ttest.html#parametric-tests",
    "title": "11  Means testing",
    "section": "11.3 Parametric tests",
    "text": "11.3 Parametric tests\n\n11.3.1 \\(t\\)-distribution\nFor these scenarios where we are testing a single sample mean from one or more samples we use a \\(t\\)-distributions. A \\(t\\)-distribution is a specially altered normal distribution that has been adjusted to account for the number of individuals being sampled. Specifically, a \\(t\\)-distributions with infinite degrees of freedom is the same as a normal distribution, and our degrees of freedom help create a more platykurtic distribution to account for error and uncertainty. The distribution can be calculated as follows:\n\\[\nt = \\frac{\\Gamma(\\frac{v+1}{2})}{\\sqrt{\\pi \\nu}\\Gamma(\\frac{\\nu}{2})}(1+\\frac{t^2}{\\nu})^{-\\frac{(v+1)}{2}}\n\\]\nThese \\(t\\)-distributions can be visualized as follows:\n\n\n\nIkamusumeFan - Wikipedia\n\n\nFor all \\(t\\)-tests, we calculate the degrees of freedom based on the number of samples. If comparing values to a single sample, we use \\(df = n -1\\). If we are comparing two sample means, then we have \\(df = n_1 + n_2 -2\\).\nImportantly, we are testing to see if the means of the two distributions are equal in a \\(t\\)-test. Thus, our hypotheses are as follows:\n\\(H_0: \\mu_1 = \\mu_2\\) or \\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\\(H_A: \\mu_1 \\ne \\mu_2\\) or \\(H_A: \\mu_1 - \\mu_2 \\ne 0\\)\nWhen asked about hypotheses, remember the above as the statistical hypotheses that are being directly tested.\nIn R, we have the following functions to help with \\(t\\) distributions:\n\ndt: density function of a \\(t\\)-distribution\npt: finding our \\(p\\) value from a specific \\(t\\) in a \\(t\\)-distribution\nqt: finding a particular \\(t\\) from a specific \\(p\\) in a \\(t\\)-distribution\nrt: random values from a \\(t\\)-distribution\n\nAll of the above arguments required the degrees of freedom to be declared. Unlike the normal distribution functions, these can not be adjusted for your data; tests must be performed using t.test.\n\n\n11.3.2 \\(t\\)-tests\nWe have three major types of \\(t\\)-tests:\n\nOne-sample \\(t\\)-tests: a single sample is being compared to a value, or vice versa.\nTwo-sample \\(t\\)-tests: two samples are being compared to one another to see if they come from the same population.\nPaired \\(t\\)-tests: before-and-after measurements of the same individuals are being compared. This is necessary to account for a repeat in the individuals being measured, and different potential baselines at initiation. In this case, we are looking to see if the difference between before and after is equal to zero.\n\nWe also have what we call a “true” \\(t\\)-test and “Welch’s” \\(t\\)-test. The formula for a “true” \\(t\\) is as follows:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n\\]\nWhere \\(s_p\\) is based on the “pooled variance” between the samples. This can be calculated as follows:\n\\[\ns_p = \\sqrt{\\frac{(n_1-1)(s_1^2)+(n_2-1)(s_2^2)}{n_1+n_2 -2}}\n\\]\nWhereas the equation for a “Welch’s” \\(t\\) is:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\n\\]\nWelch’s \\(t\\) also varies with respect to the degrees of freedom, calculated by:\n\\[\ndf = \\frac{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1-1}+\\frac{(\\frac{s_2^2}{n_2})^2}{n_2-1}}\n\\]\nOK, so why the difference?\nA \\(t\\)-test works well under a certain set of assumptions, include equal variance between samples and roughly equal sample sizes. A Welch’s \\(t\\)-test is better for scenarios with unequal variance and small sample sizes. If sample sizes and variances are equal, the two \\(t\\)-tests should perform the same.\nBecause of this, some argue that “Welch’s” should be the default \\(t\\)-test, and in R, Welch’s is the default \\(t\\)-test. If you want to specify a “regular” \\(t\\)-value, you will have to set the option var.equal = TRUE. (The default is var.equal = FALSE).\nIn this class, we will default to a Welch’s test in all instances.\nIf you choose to do a Student’s t-test, you must do the following:\n\nDownload the car library\nUse the leveneTest function to see if variances are equal between populations\n\nWe do not cover this in depth here, but be aware of this difference. For more information, see Ruxton (2006).\n\n\n11.3.3 One-sample \\(t\\)-tests\nLet’s look at the values of all of the dog samples in our canines dataset.\n\ndogs &lt;- canines |&gt;\n  filter(Sample == \"Dog\") |&gt;\n  select(Sample, OA)\n\nxbar &lt;- mean(dogs$OA)\nsd_dog &lt;- sd(dogs$OA)\nn &lt;- nrow(dogs)\n\nNow we have stored all of our information on our dog dataset. Let’s say that the overall populations of dogs a mean OA score of \\(143\\) with a \\(\\sigma = 1.5\\). Is our sample different than the overall population?\n\nt.test(x = dogs$OA,\n       alternative = \"two.sided\",\n       mu = 143)\n\n\n    One Sample t-test\n\ndata:  dogs$OA\nt = -0.74339, df = 33, p-value = 0.4625\nalternative hypothesis: true mean is not equal to 143\n95 percent confidence interval:\n 138.4667 145.1070\nsample estimates:\nmean of x \n 141.7869 \n\n\nAs we can see above, we fail to reject the null hypothesis that our sample is different than the overall mean for dogs.\n\n\n11.3.4 Two-sample \\(t\\)-tests\nNow let’s say we want to compare foxes and dogs to each other. Since we have all of our data in the same data frame, we will have to subset our data to ensure we are doing this properly.\n\n# already got dogs\ndog_oa &lt;- dogs$OA\n\nfoxes &lt;- canines |&gt;\n  filter(Sample == \"Fox\") |&gt;\n  select(Sample, OA)\n\nfox_oa &lt;- foxes$OA\n\nNow, we are ready for the test!\n\nt.test(dog_oa, fox_oa)\n\n\n    Welch Two Sample t-test\n\ndata:  dog_oa and fox_oa\nt = -6.3399, df = 72.766, p-value = 1.717e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -19.62289 -10.23599\nsample estimates:\nmean of x mean of y \n 141.7869  156.7163 \n\n\nAs we can see, the dogs and the foxes significantly differ in their OA measurement, so we reject the null hypothesis that \\(\\mu_{dog} = \\mu_{fox}\\).\n\n\n11.3.5 Paired \\(t\\)-tests\nI will do a highly simplified version of a paired \\(t\\)-test here just for demonstrations sake. Remember that you want to used paired tests when we are looking at the same individuals at different points in time.\n\n# create two random distributions\n# DEMONSTRATION ONLY\n\n# make repeatable\nset.seed(867)\n\nt1 &lt;- rnorm(20,0,1)\nt2 &lt;- rnorm(20,2,1)\n\nNow we can compare these using paired = TRUE.\n\nt.test(t1, t2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  t1 and t2\nt = -7.5663, df = 19, p-value = 3.796e-07\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.107787 -1.760973\nsample estimates:\nmean difference \n       -2.43438 \n\n\nAs we can see, we reject the null hypothesis that these distributions are equal in this case. Let’s see how this changes though if we set paired = FALSE.\n\nt.test(t1, t2)\n\n\n    Welch Two Sample t-test\n\ndata:  t1 and t2\nt = -8.1501, df = 37.48, p-value = 8.03e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.039333 -1.829428\nsample estimates:\n  mean of x   mean of y \n-0.07258938  2.36179080 \n\n\nThis value differs because, in a paired test, we are looking to see if the difference between the distributions is \\(0\\), while in the independent (standard) test we are comparing the overall distributions of the samples.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#non-parametric-tests",
    "href": "ttest.html#non-parametric-tests",
    "title": "11  Means testing",
    "section": "11.4 Non-parametric tests",
    "text": "11.4 Non-parametric tests\nThe following tests should be used when no data transformations have been successful with your dataset.\n\n11.4.1 Wilcoxon tests\nWhen data (and the differences among data) are non-normal, they violate the assumptions of a \\(t\\)-test. In these cases, we have to do a Wilcoxon test (also called a Wilcoxon signed rank test). In R, the command wilcox.test also includes the Mann-Whitney \\(U\\) test for unpaired data and the standard Wilcoxon test \\(W\\) for paired data.\n\n\n11.4.2 Mann-Whitney \\(U\\)\nFor this test, we would perform the following procedures to figure out our statistics:\n\nRank the pooled dataset from smallest to largest, and number all numbers by their ranks\nSum the ranks for the first column and the second column\nCompute \\(U_1\\) and \\(U_2\\), comparing the smallest value to a Mann-Whitney \\(U\\) table.\n\nThe equations for these statistics are as follows, where \\(R\\) represents the sum of the ranks for that sample:\n\\[\nU_1 = n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\n\\]\n\\[\nU_2 = n_1n_2 + \\frac{n_2(n_2+1)}{2} - R_2\n\\]\nIn R, this looks like so:\n\nwilcox.test(t1, t2, paired = FALSE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  t1 and t2\nW = 11, p-value = 2.829e-09\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n11.4.3 Wilcoxon signed rank test\nFor paired samples, we want to do the Wilcoxon signed rank test. This is performed by:\n\nFinding the difference between sampling events for each sampling unit.\nOrder the differences based on their absolute value\nFind the sum of the positive ranks and the negative ranks\nThe smaller of the values is your \\(W\\) statistic.\n\nIn R, this test looks as follows:\n\nwilcox.test(t1, t2, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  t1 and t2\nV = 0, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#confidence-intervals",
    "href": "ttest.html#confidence-intervals",
    "title": "11  Means testing",
    "section": "11.5 Confidence intervals",
    "text": "11.5 Confidence intervals\nIn \\(t\\) tests, we are looking at the difference between the means. Oftentimes, we are looking at a confidence interval for the difference between these means. This can be determined by:\n\\[\n(\\bar{x}_1-\\bar{x}_2) \\pm t_{crit}\\sqrt{\\frac{s_p^2}{n_1}+\\frac{s_p^2}{n_2}}\n\\]\nThis is very similar to the CI we calculated with the \\(Z\\) statistic. Remember that we can use the following function to find our desired \\(t\\), which requires degrees of freedom to work:\n\nqt(0.975, df = 10)\n\n[1] 2.228139",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#homework-chapter-9",
    "href": "ttest.html#homework-chapter-9",
    "title": "11  Means testing",
    "section": "11.6 Homework: Chapter 9",
    "text": "11.6 Homework: Chapter 9\nFor Chapter 9, complete problems 9.1, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9, and 9.10.  For problems 9.3 - 9.8, be sure to state the null and alternative hypotheses and whether the test is one- or two-tailed.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "ttest.html#homework-chapter-10",
    "href": "ttest.html#homework-chapter-10",
    "title": "11  Means testing",
    "section": "11.7 Homework: Chapter 10",
    "text": "11.7 Homework: Chapter 10\nTwo-sample means are practiced in Chapter 10. Please see Canvas for more information.\n\n\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth Scores. https://doi.org/10.6084/m9.figshare.8081108.v1\n\n\nRuxton, G. D. (2006). The unequal variance t-test is an underused alternative to Student’s t-test and the Mann–Whitney u test. Behavioral Ecology 17:688–690.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "anova1.html",
    "href": "anova1.html",
    "title": "12  ANOVA: Part 1",
    "section": "",
    "text": "12.1 Introduction\nWhen we are comparing multiple (2+) populations, we perform what is called an analysis of variance - or an ANOVA. We opt for this different method because we are trying to minimize error. As you’ll recall, we use \\(\\alpha\\) to minimize our chances of making an error and coming to an incorrect conclusion regarding our data. In our previous tests (\\(t\\)-tests) we are comparing the means between two different populations, such that \\(H_0: \\mu_1 = \\mu_2\\). When comparing multiple populations, comparing the means in this direct fashion can increase the probability of introducing error into a system. Consider the following:\nlibrary(tidyverse)\n\n# needed for summarizing data\nlibrary(plyr)\n\n# needed for better Tukey tests\nlibrary(agricolae)\n# This creates a reproducible example\n# rnorm creates random datasets\n\nset.seed(8675309)\n\nfor(i in 1:100){\n  x &lt;- rnorm(10)\n  if(i == 1){\n    data &lt;- x |&gt; as.data.frame()\n    colnames(data) &lt;- \"Response\"\n    data$Explanatory &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x |&gt; as.data.frame()\n    colnames(newdat) &lt;- \"Response\"\n    newdat$Explanatory &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# summarize by group\nsummary_data &lt;- ddply(data, \"Explanatory\", summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n\nggplot(summary_data, aes(x = Explanatory, y = mean, group = Explanatory)) +\n  geom_point() +\n  geom_errorbar(data = summary_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Explanatory), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-1.5,1.5)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\nAs we can see above, with just ten random samples and 100 sampling events, we get some datasets that do not have the mean included within the interquartile range, and thus have means that would be statistically different than what we draw. As we increase the number of draws, we get closer to the mean:\nset.seed(8675309)\n\nfor(i in 1:100){\n  x &lt;- rnorm(100)\n  if(i == 1){\n    data &lt;- x |&gt; as.data.frame()\n    colnames(data) &lt;- \"Response\"\n    data$Explanatory &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x |&gt; as.data.frame()\n    colnames(newdat) &lt;- \"Response\"\n    newdat$Explanatory &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# summarize by group\nsummary_data &lt;- ddply(data, \"Explanatory\", summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n\nggplot(summary_data, aes(x = Explanatory, y = mean, group = Explanatory)) +\n  geom_point() +\n  geom_errorbar(data = summary_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Explanatory), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-1.5,1.5)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\nAs we can see, even with 100 sample, we still have some chances of having groups that are different! When we do pairwise comparisons, we are compounding the error and the possibility of coming to an incorrect conclusion. Thus, when comparing multiple groups, we use the variances to see if groups come from the same distribution rather than the mean.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#anova-by-hand",
    "href": "anova1.html#anova-by-hand",
    "title": "12  ANOVA: Part 1",
    "section": "12.2 ANOVA: By hand",
    "text": "12.2 ANOVA: By hand\nWe are predominately going to be using the default function aov to perform ANOVAs in this course. However, you can expand the following workthrough if you would like to see step-by-step instructions on calculating a one-way ANOVA by hand.\n\n\nClick here to see the manual method.\n\nFor this workthrough, we will use the following psuedorandom dataset:\n\nset.seed(8675309)\n\nfor(i in 1:4){\n  x &lt;- rnorm(10)\n  if(i == 1){\n    x &lt;- rnorm(10, mean = 2)\n    data &lt;- x |&gt; as.data.frame()\n    colnames(data) &lt;- \"Response\"\n    data$Explanatory &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x |&gt; as.data.frame()\n    colnames(newdat) &lt;- \"Response\"\n    newdat$Explanatory &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# split into \"typical\" table\nexpanded_data &lt;- NULL\nexpanded_data$x1 &lt;- data$Response[which(data$Explanatory==\"x1\")]\nexpanded_data$x2 &lt;- data$Response[which(data$Explanatory==\"x2\")]\nexpanded_data$x3 &lt;- data$Response[which(data$Explanatory==\"x3\")]\nexpanded_data$x4 &lt;- data$Response[which(data$Explanatory==\"x4\")]\n\nexpanded_data &lt;- expanded_data |&gt;\n  as.data.frame()\n\n# summarize by group\nsummary_data &lt;- ddply(data, \"Explanatory\", summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n\nggplot(summary_data, aes(x = Explanatory, y = mean, group = Explanatory)) +\n  geom_point() +\n  geom_errorbar(data = summary_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Explanatory), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-3,3)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\n\n\n\n\n\n\n\n\n\nexpanded_data |&gt; round(2)\n\n     x1    x2    x3    x4\n1  0.45  1.99  0.38 -0.64\n2  3.02  0.04  1.15  0.00\n3  2.15 -0.40  1.57  0.05\n4  1.34 -0.47  0.59  0.68\n5  1.01 -0.41 -0.62 -0.25\n6  3.97  0.68 -0.23 -0.15\n7  1.56  0.69  0.06 -0.87\n8  1.10  0.53 -0.31 -1.98\n9  1.85 -0.19 -0.25  0.24\n10 1.17  0.38 -0.15  0.04\n\n\nAbove, we can see the made-up dataset where it appears as though one population differs from the other populations in our measurements. Let’s calculate an ANOVA and find out if this is the case!\nNOTE throughout this process that I am trying to name variables in a straightforward fashion so as not to lose my way.\n\n12.2.1 Calculate group means and Grand Mean\nLet us assume we have a dataset, \\(x\\), that is \\(k\\) columns and \\(n\\) rows, with \\(N\\) data points in the entire data frame. We first want to take the column means for each group \\(k\\), such that we have \\(\\bar{x}_k\\). We also need to find the mean of the entire dataset, \\(\\bar{x}_n\\). We can calculate this by taking \\(\\frac{\\Sigma x}{n}\\), which we have to calculate by column as follows.\n\n# calculate the mean of each group\n# each group is in a single column\ngroup_means &lt;- colMeans(expanded_data)\n\n# rounding to two decimal places\ngroup_means |&gt; round(2)\n\n   x1    x2    x3    x4 \n 1.76  0.28  0.22 -0.29 \n\n\nNext, we need to calculate the number of total entries in the dataset. We have written a function to accomplish this incase some rows have different numbers of entries from others.\n\nn &lt;- 0\n\nfor(i in 1:ncol(expanded_data)){\n  # account for unequal row length, if exists\n  sample &lt;- expanded_data[,i] |&gt; \n    as.numeric() |&gt;\n    na.omit()\n  n &lt;- n + length(sample)\n}\n\nn\n\n[1] 40\n\n\nNext, we can calculate the grand_mean of all of the data.\n\n# sum up all the data\ndataset_sum &lt;- colSums(expanded_data) |&gt;\n  sum()\n\n# divide by the number of data points\ngrand_mean &lt;- dataset_sum/n\n\n# display mean\ngrand_mean |&gt; round(2)\n\n[1] 0.49\n\n\n\n\n12.2.2 Total sum of squares\nTo calculate the total sum of squares (TSS), we need to take the deviations (differences) of each point from the grand mean \\(\\bar{x}_n\\), square them, and them take the sum of them.\n\n# calculate deviates\n# can calculate across all table at once\ngrand_deviates_squared &lt;- (expanded_data - grand_mean)^2\n\n# round output for here\ngrand_deviates_squared |&gt; round(2)\n\n      x1   x2   x3   x4\n1   0.00 2.22 0.01 1.28\n2   6.39 0.20 0.43 0.25\n3   2.74 0.81 1.17 0.20\n4   0.72 0.94 0.01 0.04\n5   0.26 0.83 1.23 0.56\n6  12.10 0.04 0.52 0.42\n7   1.13 0.04 0.19 1.87\n8   0.37 0.00 0.65 6.11\n9   1.84 0.46 0.55 0.07\n10  0.46 0.01 0.42 0.21\n\n\n\n# calculate the sum of all the deviates\nss_total &lt;- rowSums(grand_deviates_squared) |&gt;\n  sum()\n\nss_total |&gt; round(2)\n\n[1] 47.73\n\n\n\n\n12.2.3 Within-group sum of squares\nFor each data point, we need to calculate its deviation from its own group mean, squaring these deviations and then summing them together. We can’t calcualte this quite as elegantly as the aforementioned data, but we can write a function that will operate across the table and create a new dataset on our behalf.\n\n# replicate dataset\n# replace columns with deviate data\ngroup_deviates &lt;- expanded_data\n\n# loop through each column\nfor(i in 1:ncol(group_deviates)){\n  # get the data in each group\n  dat &lt;- group_deviates[,i]\n  # calculate the group mean\n  mu &lt;- mean(dat)\n  # calculate the group deviates\n  dev.dat &lt;- (dat - mu)^2\n  # save into table\n  group_deviates[,i] &lt;- dev.dat\n}\n\ngroup_deviates |&gt; round(2)\n\n     x1   x2   x3   x4\n1  1.72 2.90 0.02 0.12\n2  1.59 0.06 0.87 0.08\n3  0.15 0.47 1.84 0.11\n4  0.18 0.57 0.14 0.95\n5  0.57 0.49 0.70 0.00\n6  4.89 0.16 0.20 0.02\n7  0.04 0.16 0.02 0.34\n8  0.44 0.06 0.28 2.85\n9  0.01 0.22 0.22 0.28\n10 0.35 0.01 0.14 0.11\n\n\n\n# calculate sum of data table\nss_within &lt;- colSums(group_deviates) |&gt; \n  sum()\n\nss_within |&gt; round(2)\n\n[1] 24.33\n\n\n\n\n12.2.4 Among-group sum of squares\nThe total sum of squares is equal to the among groups sum of squares and the within groups sum of squares added together; thus, we can solve this part with some easy arithmetic.\n\nss_among &lt;- ss_total - ss_within\n\nss_among |&gt; round(2)\n\n[1] 23.4\n\n\n\n\n12.2.5 Calculate degrees of freedom\nOur degrees of freedom for the “between” group is the number of categories minus one (\\(K-1\\)).\n\nss_among_df &lt;- ncol(expanded_data) - 1\n\nss_among_df\n\n[1] 3\n\n\nOur degrees of freedom for the within group are the number of total samples minus the number of categories (\\(N - K\\)).\n\nss_within_df &lt;- n - ncol(expanded_data)\n\nss_within_df\n\n[1] 36\n\n\nOur degrees of freedom for the total sum of squares is the number of samples minus one (\\(N-1\\)).\n\nss_total_df &lt;- n - 1\n\nss_total_df\n\n[1] 39\n\n\n\n\n12.2.6 Calculate mean squares\nFor each category (among and within), the mean square is equal to the sum of squares divided by the degrees of freedom.\n\nms_among &lt;- ss_among/ss_among_df\n\nms_among |&gt; round(2)\n\n[1] 7.8\n\n\n\nms_within &lt;- ss_within/ss_within_df\n\nms_within |&gt; round(2)\n\n[1] 0.68\n\n\n\n\n12.2.7 Get \\(F\\) statistic\nWe divide the sum of squares among data point by the sum of squares within data points to obtain our \\(F\\) statistic.\n\nf_stat &lt;- ms_among/ms_within\n\nf_stat |&gt; round(2)\n\n[1] 11.54\n\n\n\n\n12.2.8 Get \\(p\\) value\nWe can use the function pf to calculate the \\(p\\) value for any given \\(F\\). Note that this function requires two different degrees of freedom to work correctly, and we are always looking right since this is a unidirectional distribution.\n\npf(f_stat, \n   df1 = ss_among_df, \n   df2 = ss_within_df, \n   lower.tail = F)\n\n[1] 1.894073e-05\n\n\nGiven how small our \\(p\\) value is, we want to round this to \\(p&lt;0.0001\\). As we can see, it is very unlikely that these are the same population.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#anova-by-r",
    "href": "anova1.html#anova-by-r",
    "title": "12  ANOVA: Part 1",
    "section": "12.3 ANOVA: By R",
    "text": "12.3 ANOVA: By R\nFor this, we need to use the dataframe where we have all data in a single column and all ID’s in the other columns. We then show how we want the ANOVA to operate across the data using the ~ symbol.\n\ndata_aov &lt;- aov(Response ~ Explanatory, data = data)\n\nsummary(data_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nExplanatory  3  23.40   7.801   11.54 1.89e-05 ***\nResiduals   36  24.33   0.676                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThese numbers match the by hand methodology from the previous section, showing that the math is the same.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#post-hoc-tukey-test",
    "href": "anova1.html#post-hoc-tukey-test",
    "title": "12  ANOVA: Part 1",
    "section": "12.4 Post-hoc Tukey test",
    "text": "12.4 Post-hoc Tukey test\nANOVA tells us if a test is different, but it doesn’t tell us which test is different. To do this, we have to perform a Tukey test.\n\n12.4.1 Tukey test by hand\nTo do this by hand, we will need a lot of data from our aforementioned ANOVA test.\n\n\nClick here to see the by-hand method.\n\nWe need to calculate pairwise differences between each set of means.\n\nx1_mean &lt;- mean(data$Response[data$Explanatory == \"x1\"])\nx2_mean &lt;- mean(data$Response[data$Explanatory == \"x2\"])\nx3_mean &lt;- mean(data$Response[data$Explanatory == \"x3\"])\nx4_mean &lt;- mean(data$Response[data$Explanatory == \"x4\"])\n\ngroup_means &lt;- c(x1_mean, x2_mean, x3_mean, x4_mean)\n\n# calculate all pairwise differences\npairwise_mean_diffs &lt;- dist(group_means)\n\npairwise_mean_diffs |&gt; round(2)\n\n     1    2    3\n2 1.48          \n3 1.54 0.06     \n4 2.05 0.57 0.51\n\n\nNext, we need a critical \\(Q\\) value against which we can compare. For Tukey, our degrees of freedom are the same as the degrees of freedom for \\(SS_{within}\\): \\(N - K\\).\n\n# set p value\ntukey_q &lt;- qtukey(p = 0.95,\n                  # get length of categories / columns\n                  nmeans = ncol(expanded_data),\n                  df = ss_within_df)\n\ntukey_q |&gt; round(2)\n\n[1] 3.81\n\n\nWe need to multiply \\(Q\\) by the pooled variance.This is the same as the average of the variances for each group.\n\nvar_data &lt;- 0\n\n# calculate pooled variance\nfor(i in 1:ncol(expanded_data)){\n  var_data &lt;- var_data + var(expanded_data[,i])\n}\n\npooled_var_dat &lt;- sqrt(var_data/n)\n\npooled_var_dat |&gt; round(2)\n\n[1] 0.26\n\n\nWe can calculate the Tukey critical value by multiplying the pooled variance by \\(Q\\).\n\ntukey_critical &lt;- tukey_q*pooled_var_dat\n\ntukey_critical |&gt; round(2)\n\n[1] 0.99\n\n\nRemember, we are comparing to the actual value, not the rounded value.\nWhich mean differences are difference compared to our critical value?\n\npairwise_mean_diffs[pairwise_mean_diffs &lt; tukey_critical] &lt;- 0\n\npairwise_mean_diffs\n\n         1        2        3\n2 1.477866                  \n3 1.542282 0.000000         \n4 2.051068 0.000000 0.000000\n\n\nAs we can see above, three differences cross our threshold - all associated with x1.\nWhen we graph things, we want to label this group as different. We will cover this a little later in the tutorial.\n\n\n\n12.4.2 Tukey test in R\nTukey tests in the R program are a bit more straightforward.\n\nTukeyHSD(data_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Response ~ Explanatory, data = data)\n\n$Explanatory\n             diff       lwr        upr     p adj\nx2-x1 -1.47786579 -2.468021 -0.4877106 0.0015554\nx3-x1 -1.54228164 -2.532437 -0.5521265 0.0009388\nx4-x1 -2.05106768 -3.041223 -1.0609125 0.0000147\nx3-x2 -0.06441585 -1.054571  0.9257393 0.9980525\nx4-x2 -0.57320189 -1.563357  0.4169533 0.4141599\nx4-x3 -0.50878604 -1.498941  0.4813691 0.5173399\n\n\nAs we can see above, only three comparisons have a p value of \\(&lt; 0.05\\), and thus only those three are significantly different. All involve x1.\nWe can also use HSD.test to get more specific results:\n\ntukey_data_aov &lt;- HSD.test(data_aov,\n                           # what to group by?\n                           \"Explanatory\",\n                           # significance level?\n                           alpha = 0.05, \n                           # are data unbalanced\n                           unbalanced = FALSE,\n                           # show answer?\n                           console = TRUE)\n\n\nStudy: data_aov ~ \"Explanatory\"\n\nHSD Test for Response \n\nMean Square Error:  0.6758192 \n\nExplanatory,  means\n\n     Response       std  r        se        Min      Max        Q25         Q50\nx1  1.7620153 1.0505466 10 0.2599652  0.4504476 3.972459  1.1175485  1.44911720\nx2  0.2841495 0.7532422 10 0.2599652 -0.4729986 1.985826 -0.3497379  0.21347543\nx3  0.2197337 0.7019368 10 0.2599652 -0.6150452 1.574903 -0.2436023 -0.04493909\nx4 -0.2890524 0.7345336 10 0.2599652 -1.9769014 0.684072 -0.5394534 -0.07741642\n          Q75\nx1 2.07491533\nx2 0.64579865\nx3 0.53544151\nx4 0.04323417\n\nAlpha: 0.05 ; DF Error: 36 \nCritical Value of Studentized Range: 3.808798 \n\nMinimun Significant Difference: 0.9901551 \n\nTreatments with the same letter are not significantly different.\n\n     Response groups\nx1  1.7620153      a\nx2  0.2841495      b\nx3  0.2197337      b\nx4 -0.2890524      b\n\nprint(tukey_data_aov)\n\n$statistics\n    MSerror Df      Mean       CV       MSD\n  0.6758192 36 0.4942115 166.3422 0.9901551\n\n$parameters\n   test      name.t ntr StudentizedRange alpha\n  Tukey Explanatory   4         3.808798  0.05\n\n$means\n     Response       std  r        se        Min      Max        Q25         Q50\nx1  1.7620153 1.0505466 10 0.2599652  0.4504476 3.972459  1.1175485  1.44911720\nx2  0.2841495 0.7532422 10 0.2599652 -0.4729986 1.985826 -0.3497379  0.21347543\nx3  0.2197337 0.7019368 10 0.2599652 -0.6150452 1.574903 -0.2436023 -0.04493909\nx4 -0.2890524 0.7345336 10 0.2599652 -1.9769014 0.684072 -0.5394534 -0.07741642\n          Q75\nx1 2.07491533\nx2 0.64579865\nx3 0.53544151\nx4 0.04323417\n\n$comparison\nNULL\n\n$groups\n     Response groups\nx1  1.7620153      a\nx2  0.2841495      b\nx3  0.2197337      b\nx4 -0.2890524      b\n\nattr(,\"class\")\n[1] \"group\"\n\n\nThis output is nice because it labels groups based on which groups belong together! This will be important for plotting, and is cumbersome if you have a lot of means.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#plotting-our-anova-results",
    "href": "anova1.html#plotting-our-anova-results",
    "title": "12  ANOVA: Part 1",
    "section": "12.5 Plotting our ANOVA results",
    "text": "12.5 Plotting our ANOVA results\nWhen we plot our ANOVAs, we want to show which mean is different from all of the others. Below, I will show the full pipeline for performing an ANOVA on a dataset and plotting the data at the end using out data object.\n\ndata_aov &lt;- aov(Response ~ Explanatory,data)\n\nsummary(data_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nExplanatory  3  23.40   7.801   11.54 1.89e-05 ***\nResiduals   36  24.33   0.676                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we need to summarize things based on our plyr function.\n\n# summarize by group\nsummary_data &lt;- ddply(data, \"Explanatory\", summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n\nNow, we can use the aforementioned Tukey results to assign to groups. Because agricolae functions define the groups in a $groups slot, we can pull out these data and perform some minimal transformations to make them ready to plot. This will save us a lot of time and hassle.\n\n# note first group must be EXACT MATCH to your summary_data object\n# groups are saved in the Tukey object\n# this is true for Tukey later as well\n\n# the following is a function that will make the significant label table\nsig.label.maker &lt;- function(tukey_test, group_name){\n  sig.labels &lt;- tukey_test$groups |&gt; \n    # convert to a data.frame\n    as.data.frame() |&gt;\n    # create a new column - place rownames into the column\n    # converts to a format better for ggplot\n    mutate(Explanatorys = rownames(tukey_test$groups)) |&gt;\n    # rename column to prevent confusion\n    # specify dplyr; default function may be from plyr and not work\n    dplyr::rename(significance = groups)\n  colnames(sig.labels)[which(colnames(sig.labels) == \"Explanatorys\")] &lt;- group_name\n  return(sig.labels)\n}\n\n# pull out the groups slot from tukey\n# same for Kruskal later on!\nsig.labels &lt;- sig.label.maker(tukey_data_aov, \"Explanatory\")\n\nsig.labels\n\n     Response significance Explanatory\nx1  1.7620153            a          x1\nx2  0.2841495            b          x2\nx3  0.2197337            b          x3\nx4 -0.2890524            b          x4\n\n\nNote that in the above, the row labels and the column Explanatory are identical. We moved these data into a column to make it easier to use ggplot. If you want letters to be different - capitalized or something else - you will have to do this yourself. Now we can plot our data and add the labels!\n\nggplot(summary_data, # plot summary data\n       # Define plotting - x by group, y is mean, grouping by group\n       aes(x = Explanatory, y = mean, group = Explanatory)) +\n  # add points to plot for y values\n  geom_point() +\n  # add error bars around points\n  geom_errorbar(data = summary_data, \n                # define error bars\n                aes(ymin = mean - 2*se, ymax = mean+2*se,\n                    # define color, size\n                                    color = Explanatory), width = 0.1) +\n  # add line at average for the main group\n  # this is not always known - nor requires!\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  # set vertical limits for plot\n  ylim(c(-3,3)) +\n  # make it a classic theme - more legible\n  theme_classic() +\n  # add text to plot\n  geom_text(data = sig.labels,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = Explanatory, \n                # define height of label\n                y = -2, \n                # what are the labels?\n                label = paste0(significance))) +\n  xlab(\"Explanatory Name\") +\n  ylab(\"Mean\") +\n  # remove legend - not needed here\n  theme(legend.position = \"none\",\n        # make label text vertical, easier to read\n        axis.text.x = element_text(angle = 90, \n                                   # vertical offset of text\n                                   vjust = 0.5, \n                                   # text size\n                                   size = 12))\n\n\n\n\n\n\n\n\nNote that I place the labels below here, but they could also be placed above. You have to position them based on your own judgment.\nAddendum: if you see a mean labeled a and b, it would be statistically indistinguishable from means labeled a and means labeled b, despite means with only an a or a b being different from each other.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#kruskal-wallis-tests",
    "href": "anova1.html#kruskal-wallis-tests",
    "title": "12  ANOVA: Part 1",
    "section": "12.6 Kruskal-Wallis tests",
    "text": "12.6 Kruskal-Wallis tests\nThe Kruskal-Wallis test is the non-parametric version of an ANOVA. To demonstrate this, we will be creating a non-normal distribution by pulling random values from a uniform distribution, using the random uniform function runif. Note we are rounding the data here to make it more similar to non-normal datasets you may encounter, and to increase the probability of ties.\n\n## COPY THIS WHOLE CHUNK ##\n# will load example data\n\nset.seed(8675309)\n\nfor(i in 1:4){\n  x &lt;- runif(10, min = -1, max = 1) |&gt;\n    round(2)\n  if(i == 1){\n    x &lt;- runif(10, min = 1, max = 2) |&gt;\n      round(2)\n    data &lt;- x |&gt; as.data.frame()\n    colnames(data) &lt;- \"Response\"\n    data$Explanatory &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x |&gt; as.data.frame()\n    colnames(newdat) &lt;- \"Response\"\n    newdat$Explanatory &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# split into \"typical\" table\nexpanded_data &lt;- NULL\nexpanded_data$x1 &lt;- data$Response[which(data$Explanatory==\"x1\")]\nexpanded_data$x2 &lt;- data$Response[which(data$Explanatory==\"x2\")]\nexpanded_data$x3 &lt;- data$Response[which(data$Explanatory==\"x3\")]\nexpanded_data$x4 &lt;- data$Response[which(data$Explanatory==\"x4\")]\n\nexpanded_data &lt;- expanded_data |&gt;\n  as.data.frame()\n\n# summarize by group\nsummary_data &lt;- ddply(data, \"Explanatory\", summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n\nggplot(summary_data, aes(x = Explanatory, y = mean, group = Explanatory)) +\n  geom_point() +\n  geom_errorbar(data = summary_data, aes(ymin = mean - 2*se, ymax = mean+2*se,\n                                    color = Explanatory), width = 0.1) +\n  geom_hline(yintercept = 0, col = \"black\", linewidth = 0.5) +\n  ylim(c(-3,3)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))\n\n\n\n\n\n\n\n\nWe can confirm these data are non-normal with Shapiro-Wilk tests and histograms. Demonstrating with the first column only:\n\nhist(data[,1])\n\n\n\n\n\n\n\n\n\nshapiro.test(data[,1])\n\n\n    Shapiro-Wilk normality test\n\ndata:  data[, 1]\nW = 0.93404, p-value = 0.02187\n\n\nThe above histogram appears non-normal, and the Shapiro-Wilk also indicates this is non-normal.\n\n12.6.1 By hand\n\n\nClick here to see the Kruskal-Wallis by hand.\n\nFirst, we need to order all the data in the entire dataset. This is easiest to do if we use the dataset with all data in a single column.\n\ndata$ranks &lt;- rank(data$Response, ties.method = \"average\")\n\n# view first couple of rows\nhead(data)\n\n  Response Explanatory ranks\n1     1.84          x1    39\n2     1.58          x1    34\n3     1.51          x1    33\n4     1.26          x1    32\n5     1.75          x1    37\n6     1.92          x1    40\n\n\nNow, we need to calculate the sum of the ranks for each category. The below function will take the sum of the ranks for the rows which match the condition of having the group be x1.\n\nx1_sum &lt;- sum(data$ranks[which(data$Explanatory==\"x1\")])\nx2_sum &lt;- sum(data$ranks[which(data$Explanatory==\"x2\")])\nx3_sum &lt;- sum(data$ranks[which(data$Explanatory==\"x3\")])\nx4_sum &lt;- sum(data$ranks[which(data$Explanatory==\"x4\")])\n\nNow, we need to calculate our test statistic. The test statistic is \\(H\\), with: \\[H = \\frac{12}{N(N+1)} \\cdot \\Sigma \\frac{R_j^2}{n_j}-3(N+1)\\]In this equation, \\(R\\) is the sum of the ranks for a given category. This follows a \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom, with \\(k\\) referring to categories.\nFor these, we need to know what \\(n\\) is for each category.\n\nn1 &lt;- length(data$ranks[which(data$Explanatory==\"x1\")])\nn2 &lt;- length(data$ranks[which(data$Explanatory==\"x2\")])\nn3 &lt;- length(data$ranks[which(data$Explanatory==\"x3\")])\nn4 &lt;- length(data$ranks[which(data$Explanatory==\"x4\")])\n\nNext, we can calculate the sums of the \\(\\frac{R^2}{n}\\) term.\n\nr2_sum &lt;- sum(x1_sum^2/n,\n              x2_sum^2,n,\n              x3_sum^2/n,\n              x4_sum^2/n)\n\nNow, we can calculate \\(H\\).\n\nN &lt;- sum(n1, n2, n3, n4)\n\nH &lt;- ((12/(N*(N+1)))*r2_sum)-(3*(N+1))\n\nH |&gt; round(2)\n\n[1] 57.43\n\n\nNow, we can evaluate this with a \\(\\chi^2\\) \\(p\\) value.\n\npchisq(q = H, \n       df = ncol(data)-1, \n       # remember, looking right!\n       lower.tail = FALSE)\n\n[1] 3.382831e-13\n\n\nAs we can see, the probability is extremely low with \\(p &lt; 0.0001\\). One distribution is different, and we can proceed with Tukey tests.\n\n\n\n12.6.2 Using R\nWe can also use R for this test.\n\nkruskal_data &lt;- kruskal.test(Response ~ Explanatory, data)\n\nprint(kruskal_data)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Response by Explanatory\nKruskal-Wallis chi-squared = 22.149, df = 3, p-value = 6.073e-05\n\n\nThis test is similar, but not quite the same, as what we calculated above. The package agricolae provides a more in-depth Kruskal-Wallis test that gives us more metadata. We have to use the wrapper with to get this to work properly, per the help page.\n\n# use \"with\", defining your dataset first\ndata_kruskal &lt;- with(data,\n                     # define parameters for Kruskal-Wallis test\n                     # First, variable of interest\n                     kruskal(Response,\n                             # second, grouping variable\n                             Explanatory,\n                             # do you want group designations returned?\n                             group = TRUE,\n                             # what is this being done on?\n                             # must match dataset name in \"with\"!!!\n                             main = \"data\"))\n\n# display results\n# summary WILL NOT show the right thing\nprint(data_kruskal)\n\n$statistics\n     Chisq Df     p.chisq  t.value      MSD\n  22.14917  3 6.07312e-05 2.028094 7.252228\n\n$parameters\n            test p.ajusted      name.t ntr alpha\n  Kruskal-Wallis      none Explanatory   4  0.05\n\n$means\n   Response  rank       std  r   Min  Max     Q25    Q50    Q75\nx1    1.633 35.50 0.2415252 10  1.21 1.92  1.5275  1.720 1.8025\nx2   -0.075 14.20 0.6424130 10 -0.88 0.69 -0.6325 -0.105 0.5575\nx3    0.020 16.15 0.5327080 10 -0.63 0.95 -0.3250 -0.110 0.3775\nx4    0.045 16.15 0.6774011 10 -0.92 0.95 -0.3500 -0.135 0.6475\n\n$comparison\nNULL\n\n$groups\n   Response groups\nx1    35.50      a\nx3    16.15      b\nx4    16.15      b\nx2    14.20      b\n\nattr(,\"class\")\n[1] \"group\"\n\n\nAs we can see, the above gives us our group designations under the $group section. This is what we need to be able to plot things, as with ANOVA above. I do not repeat those steps here.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova1.html#homework-chapter-11",
    "href": "anova1.html#homework-chapter-11",
    "title": "12  ANOVA: Part 1",
    "section": "12.7 Homework: Chapter 11",
    "text": "12.7 Homework: Chapter 11\nYour homework is to complete problems 11.1, 11.2, 11.3, and 11.4. The first two will require ANOVA, and the last two will require Kruskal-Wallis tests. For each pair of tests, you must also complete the problem by hand in addition to using the default R methods. For all problems:\n\nState your hypothesis in words\nState your hypothesis mathematically (hint: \\(\\mu\\))\nAnswer in complete sentences and don’t forget to round your answers.\nIf you reject the null hypothesis, you must plot the results and label them to show which means are different.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA: Part 1</span>"
    ]
  },
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "13  ANOVA: Part 2",
    "section": "",
    "text": "13.1 Two-way ANOVA\nPreviously, we discussed one-way ANOVAs, where we are looking at a single factor split across three or more groups and trying to determine if the means of these groups are equal (i.e., \\(H_0: \\mu_1=\\mu_2=...\\mu_i\\)). ANOVA specifically allows us to analyze the variance of these different groups to ascertain which factors are most responsible for the variation we observe in the data. Because of the way ANOVA operates, we can actually test multiple different combinations of variables simultaneously in what we call a two-way ANOVA.\nDon’t forget to load your required packages - some we have used before, like agricolae, plyr, and tidyverse, but others are new for this section: multcomp and nlme! As a reminder, these packages are designed for the following:\nlibrary(agricolae)\nlibrary(plyr)\nlibrary(tidyverse)\n\n# NEW PACKAGES NEEDED\n# Don't forget to install these on your machine\nlibrary(multcomp)\nlibrary(nlme)\nlibrary(PMCMRplus)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#two-way-anova",
    "href": "anova2.html#two-way-anova",
    "title": "13  ANOVA: Part 2",
    "section": "",
    "text": "agricolae: originally written as a Master’s thesis at the Universidad Nacional de Ingeniería (Lima, Perú), this package is designed to help with agricultural research.\nplyr: tools for common problems, including splitting data, applying functions across data, and combining datasets together.\ntidyverse: one we are already familiar with; a wrapper for installing ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats.\nmultcomp: more in depth and better MULTiple COMParisons via linear models and related models.\nnlme: a package for fitting Gaussian and non-linear mixed-effect models.\nPMCMRplus: a math package with post-hoc tests for Friedman’s test",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#designs",
    "href": "anova2.html#designs",
    "title": "13  ANOVA: Part 2",
    "section": "13.2 Designs",
    "text": "13.2 Designs\nThere are several different designs for two-way ANOVAs, and we will cover some of the most common designed here.\nFor these examples, we are going to randomly generated examples. I will refer to the variables as Response and Explanatory for simplicity’s sake.\n\n13.2.1 Randomized block design\nRandomized block designs look at combinations of variables that could be affecting the results. More specifically, we are looking at two strata or factors and their effects on a continuous response variable.\n\nset.seed(8675309)\n\n# random example\n\n# Blocking variable\nBlocking_Variable &lt;- c(\"Group 1\", \"Group 2\", \"Group 3\")\n\n# explanatory variables\n# these are your columns\n# these are your primary hypothesis\nExplanatory_1 &lt;- c(10.1, 9.4, 11.1)\nExplanatory_2 &lt;- c(12, 13.0, 15.4)\nExplanatory_3 &lt;- c(11.2, 10.1, 11.9)\n\n# create \"data table\" as we normally see it\n# combine all columns\ndata_expanded &lt;- cbind(Blocking_Variable,\n                       Explanatory_1,\n                       Explanatory_2,\n                       Explanatory_3) |&gt; \n  as.data.frame() # create data frame\n\ndata_expanded\n\n  Blocking_Variable Explanatory_1 Explanatory_2 Explanatory_3\n1           Group 1          10.1            12          11.2\n2           Group 2           9.4            13          10.1\n3           Group 3          11.1          15.4          11.9\n\n\nNote that this table is in the format that we most often see, but we need to reshape these data to make it easier for us to perform our analyses. I created the data here as a matrix with named columns and rows; the following code may need to be adjusted if you do things differently.\n\n# expand to \"long\" format\n# if not done earlier, convert to data frame\n\ndata &lt;- data_expanded |&gt;\n  # !by column for aggregating\n  # names_to = what to name column aggregation\n  # values_to = what the measurements should be called\n  pivot_longer(!Blocking_Variable, names_to = \"Explanatory_Variables\", values_to = \"Measurements\")\n\ndata\n\n# A tibble: 9 × 3\n  Blocking_Variable Explanatory_Variables Measurements\n  &lt;chr&gt;             &lt;chr&gt;                 &lt;chr&gt;       \n1 Group 1           Explanatory_1         10.1        \n2 Group 1           Explanatory_2         12          \n3 Group 1           Explanatory_3         11.2        \n4 Group 2           Explanatory_1         9.4         \n5 Group 2           Explanatory_2         13          \n6 Group 2           Explanatory_3         10.1        \n7 Group 3           Explanatory_1         11.1        \n8 Group 3           Explanatory_2         15.4        \n9 Group 3           Explanatory_3         11.9        \n\n\nNow we can do our ANOVA. Note that I put factor around the blocking variable.\n\n# mark block by factor\n# best to always use\ndata_aov &lt;- aov(Measurements ~ Explanatory_Variables + factor(Blocking_Variable), data)\n\nsummary(data_aov)\n\n                          Df Sum Sq Mean Sq F value Pr(&gt;F)  \nExplanatory_Variables      2 17.182   8.591  14.412 0.0149 *\nfactor(Blocking_Variable)  2  6.829   3.414   5.728 0.0670 .\nResiduals                  4  2.384   0.596                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this particular example, the blocking variable does not significantly differ, however the explanatory variable does differ.\nRemember, the columns represent your primary hypothesis. You will only plot your results if your primary hypothesis is significant!\nGiven that our primary null hypothesis is rejected (that is to say, not all means are equal), we need to plot our results.\nTo determine which mean(s) differ, we will use a Tukey Test. Unfortunately, the agricolae function HSD.test does not work as well for these multi-directional ANOVAs, so we need to use TukeyHSD.\n\ntukey_data_aov &lt;- TukeyHSD(data_aov)\n\ntukey_data_aov\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Measurements ~ Explanatory_Variables + factor(Blocking_Variable), data = data)\n\n$Explanatory_Variables\n                                  diff       lwr        upr     p adj\nExplanatory_2-Explanatory_1  3.2666667  1.019919  5.5134144 0.0144034\nExplanatory_3-Explanatory_1  0.8666667 -1.380081  3.1134144 0.4333834\nExplanatory_3-Explanatory_2 -2.4000000 -4.646748 -0.1532523 0.0406301\n\n$`factor(Blocking_Variable)`\n                      diff        lwr      upr     p adj\nGroup 2-Group 1 -0.2666667 -2.5134144 1.980081 0.9082398\nGroup 3-Group 1  1.7000000 -0.5467477 3.946748 0.1118461\nGroup 3-Group 2  1.9666667 -0.2800810 4.213414 0.0745795\n\n\nAs we can see above, each pairwise comparison is given a \\(p\\) value for the level of difference. We need to manually label these groups based on these \\(p\\) values, with groups being considered different if \\(p &lt; 0.05\\). We can do this as follows, but unfortunately, we need to do it by hand since we don’t have a short-form code (yet) for this conversion.\n\n# change Explanatory_Variables to your data\nTreatments &lt;- unique(data$Explanatory_Variables)\n\nsig_labels &lt;- Treatments |&gt; \n  as.data.frame() |&gt; \n  mutate(Significance = rep(NA, length(Treatments)))\n\n# Change Explanatory_Variables to your data\ncolnames(sig_labels) &lt;- c(\"Explanatory_Variables\", # MUST BE SAME AS DATA\n                          \"Significance\")\n\ntukey_data_aov$Explanatory_Variables\n\n                                  diff       lwr        upr      p adj\nExplanatory_2-Explanatory_1  3.2666667  1.019919  5.5134144 0.01440339\nExplanatory_3-Explanatory_1  0.8666667 -1.380081  3.1134144 0.43338343\nExplanatory_3-Explanatory_2 -2.4000000 -4.646748 -0.1532523 0.04063012\n\n\nNOTE that I am going to have to adjust column names and variable names a few times. Based on the above, we can see that Explanatory_3 and Explanatory_1 are not different from each other, but everything else is in a different group relative to each other. We can label these by hand.\n\nsig_labels$Significance &lt;- c(\"A\", \"B\", \"A\")\n\nsig_labels\n\n  Explanatory_Variables Significance\n1         Explanatory_1            A\n2         Explanatory_2            B\n3         Explanatory_3            A\n\n\nAs we can see above, now only Explanatory_2 is given a different letter category.\nNow, we can plot these different factors.\n\n# summarize by group\n# slight adjustment from previous\nsummary_data &lt;- ddply(data, \"Explanatory_Variables\", summarise,\n                          N = length(as.numeric(Measurements)),\n                          mean = mean(as.numeric(Measurements)),\n                          sd = sd(as.numeric(Measurements)),\n                          se = sd / sqrt(N))\n\nsummary_data\n\n  Explanatory_Variables N     mean        sd        se\n1         Explanatory_1 3 10.20000 0.8544004 0.4932883\n2         Explanatory_2 3 13.46667 1.7473790 1.0088497\n3         Explanatory_3 3 11.06667 0.9073772 0.5238745\n\n\n\n# SET Y LIMITS\n# change based on observed data\nylims &lt;- c(0, 20)\n\n# set label height, can change before plotting function\nlabel_height &lt;- 4\n\nggplot(summary_data, # plot summary data\n       # Define plotting - x by group, y is mean, grouping by group\n       aes(x = Explanatory_Variables, y = mean)) +\n  # add points to plot for y values\n  geom_point() +\n  # add error bars around points\n  geom_errorbar(data = summary_data, \n                # define error bars\n                aes(ymin = mean - 2*se, ymax = mean+2*se), \n                # width of bar\n                width = 0.1) +\n  # set vertical limits for plot\n  ylim(ylims) +\n  # make it a classic theme - more legible\n  theme_classic() +\n  # add text to plot\n  geom_text(data = sig_labels,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = Explanatory_Variables, \n                # define height of label\n                y = 4, \n                # what are the labels?\n                label = paste0(Significance))) +\n  xlab(\"Treatment\") +\n  ylab(\"Mean\") +\n  # remove legend - not needed here\n  theme(legend.position = \"none\",\n        # make label text vertical, easier to read\n        axis.text.x = element_text(angle = 90, \n                                   # vertical offset of text\n                                   vjust = 0.5, \n                                   # text size\n                                   size = 12))\n\n\n\n\n\n\n\n\n\n\n13.2.2 Repeated measures\nNow, we are going to do a repeated measures ANOVA, where we have the same individuals being measured multiple times. Consider the following imaginary dataset:\n\nVisit_1 &lt;- c(5.5,6.2,5.8)\nVisit_2 &lt;- c(4.6,5.4,5.2)\nVisit_3 &lt;- c(3.8,4.0,3.9)\n\nIndividuals &lt;- c(paste0(\"Individual\",\" \",1:3))\n\ndata &lt;- cbind(Individuals, \n                   Visit_1,\n                   Visit_2,\n                   Visit_3) |&gt;\n  as.data.frame() |&gt; \n  pivot_longer(!Individuals,\n               names_to = \"Visits\",\n               values_to = \"Measurements\")\n\ndata\n\n# A tibble: 9 × 3\n  Individuals  Visits  Measurements\n  &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;       \n1 Individual 1 Visit_1 5.5         \n2 Individual 1 Visit_2 4.6         \n3 Individual 1 Visit_3 3.8         \n4 Individual 2 Visit_1 6.2         \n5 Individual 2 Visit_2 5.4         \n6 Individual 2 Visit_3 4           \n7 Individual 3 Visit_1 5.8         \n8 Individual 3 Visit_2 5.2         \n9 Individual 3 Visit_3 3.9         \n\n\nWe need to perform the ANOVA again, but we need to account for the factor of which locations are repeated.\n\nrepeated_aov &lt;- aov(Measurements ~ factor(Visits) + Error(factor(Individuals)), data)\n\nsummary(repeated_aov)\n\n\nError: factor(Individuals)\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  2 0.4867  0.2433               \n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(Visits)  2  5.687  2.8433   89.79 0.000475 ***\nResiduals       4  0.127  0.0317                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnfortunately, because of the model this is, we cannot perform a Tukey Test on the “object” that is created from this ANOVA analysis. We can, however, approach this from a different direction and get our Tukey results (thanks to Henrik on StackOverflow!). For this to work, we need to install the packages nlme and multcomp.\n\n# ensure data is proper format\ndata$Individuals &lt;- as.factor(data$Individuals)\ndata$Visits &lt;- as.factor(data$Visits)\ndata$Measurements &lt;- as.numeric(data$Measurements)\n\nThe next part of the code fits a linear model to the data. A linear model, which we will cover later in the class, is mathematically very similar to an ANOVa. However, we can data from this model and extract the ANOVA data to understand more about the interactions. We need to use a linear model for this to account with the relationships between the two.\n\n# fit a linear mixed-effects model\n# similar to ANOVA\n\nlme_data &lt;- lme(Measurements ~ Visits, \n                         data = data, \n                         # define repeated section\n                         random = ~1|Individuals)\n\n# perform ANOVA on model\nanova(lme_data)\n\n            numDF denDF  F-value p-value\n(Intercept)     1     4 900.1654  &lt;.0001\nVisits          2     4  89.7894   5e-04\n\n\nAs we can see above, we can get the ANOVA results from this linear mixed-effects model fit to the dataset. Now, we need to know post-hoc which sets are different:\n\nlme_data |&gt;\n  # \"general linear hypothesis\"\n  # define a comparison to make\n  # can add corrections like test = adjusted (type = \"bonferroni\")\n  glht(linfct = mcp(Visits = \"Tukey\")) |&gt;\n  # return a summary of the above\n  summary()\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lme.formula(fixed = Measurements ~ Visits, data = data, random = ~1 | \n    Individuals)\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \nVisit_2 - Visit_1 == 0  -0.7667     0.1453  -5.277   &lt;1e-06 ***\nVisit_3 - Visit_1 == 0  -1.9333     0.1453 -13.306   &lt;1e-06 ***\nVisit_3 - Visit_2 == 0  -1.1667     0.1453  -8.030   &lt;1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe can see that every visit is different.\n\n# manually labeling\n\nsig_levels_repeated &lt;- matrix(data = c(\"Visit_1\", \"A\",\n                                       \"Visit_2\", \"B\",\n                                       \"Visit_3\", \"C\"), \n                              byrow = T, ncol = 2) |&gt; as.data.frame()\n\n# make labels match!\ncolnames(sig_levels_repeated) &lt;- c(\"Visits\", \"Significance\")\n\nLet’s plot these. Note that we are not summarizing these the same way, since things are varying based on individual as well.\nNote: For reasons I am not certain, you need to put the locations and then ggplot uses these colors to define everything. I really don’t know why this is happening, so if you have a solution, let me know.\n\nsummary_data &lt;- ddply(data, \"Visits\", summarise,\n                          N = length(as.numeric(Measurements)),\n                          mean = mean(as.numeric(Measurements)),\n                          sd = sd(as.numeric(Measurements)),\n                          se = sd / sqrt(N))\n\n\nggplot(summary_data,\n       aes(x = Visits, y = mean)) +\n  geom_point() +\n  geom_errorbar(data = summary_data, \n                # define error bars\n                aes(ymin = mean - 2*se, ymax = mean+2*se), \n                # width of bar\n                width = 0.1) +\n  # set vertical limits for plot\n  ylim(c(0,10)) +\n  # make it a classic theme - more legible\n  theme_classic() +\n  # add text to plot\n  geom_text(data = sig_levels_repeated,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = Visits, \n                # define height of label\n                y = 2, \n                # what are the labels?\n                label = paste0(Significance))) +\n  xlab(\"Visits\") +\n  ylab(\"Measurement\") +\n  # remove legend - not needed here\n  theme(legend.position = \"none\",\n        # make label text vertical, easier to read\n        axis.text.x = element_text(angle = 90, \n                                   # vertical offset of text\n                                   vjust = 0.5, \n                                   # text size\n                                   size = 12))\n\n\n\n\n\n\n\n\n\n\n13.2.3 Factorial ANOVA\nMathematically, a factorial ANOVA is the same as a randomized block ANOVA; please see that section for information on how to run this test.\n\n\n13.2.4 ANOVA with interaction\nSometimes when we running a model, we want to look for interactive effects. Interactive effects are situations where one (or both) variables on their own do not effect the data, but there is a cumulative effect between variables that effects things. Let’s look at an example, based on our initial example but with the data altered.\n\nset.seed(8675309)\n\n# we are using data from the randomized black ANOVA again\n\ndata_expanded\n\n  Blocking_Variable Explanatory_1 Explanatory_2 Explanatory_3\n1           Group 1          10.1            12          11.2\n2           Group 2           9.4            13          10.1\n3           Group 3          11.1          15.4          11.9\n\n\n\n### YOU DO NOT NEED TO DO THIS\n### CREATING DATA FOR EXAMPLE\n\ndata_expanded$Explanatory_1 &lt;- as.numeric(data_expanded$Explanatory_1)\ndata_expanded$Explanatory_2 &lt;- as.numeric(data_expanded$Explanatory_2)\ndata_expanded$Explanatory_3 &lt;- as.numeric(data_expanded$Explanatory_3)\n\n# create some pseudorandom data\n# [,-1] excludes first column - group data\ndata_expanded2 &lt;- cbind(Blocking_Variable,\n                        data_expanded[,-1] - 0.75)\ndata_expanded3 &lt;- cbind(Blocking_Variable,\n                        data_expanded[,-1]*1.05)\n\ndata_expanded &lt;- rbind(data_expanded,\n                       data_expanded2,\n                       data_expanded3)\n\n# expand to \"long\" format\ndata &lt;- data_expanded |&gt;\n  # convert to data frame\n  as.data.frame() |&gt;\n  # !by column for aggregating\n  # names_to = what to name column aggregation\n  # values_to = what the measurements should be called\n  pivot_longer(!Blocking_Variable, names_to = \"Treatments\", values_to = \"Measurements\")\n\n# specifying factor to be safe\n\ninteractive_aov &lt;- aov(Measurements ~ factor(Treatments) + \n                         factor(Blocking_Variable) + \n                         factor(Treatments)*factor(Blocking_Variable), \n                       data)\n\nsummary(interactive_aov)\n\n                                             Df Sum Sq Mean Sq F value   Pr(&gt;F)\nfactor(Treatments)                            2  53.28  26.640  59.680 1.14e-08\nfactor(Blocking_Variable)                     2  21.18  10.588  23.719 9.01e-06\nfactor(Treatments):factor(Blocking_Variable)  4   7.39   1.848   4.141    0.015\nResiduals                                    18   8.03   0.446                 \n                                                \nfactor(Treatments)                           ***\nfactor(Blocking_Variable)                    ***\nfactor(Treatments):factor(Blocking_Variable) *  \nResiduals                                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs we can see above, we have very significant effects for Treatment and Blocking_Variable, but a less significant effect for the interaction between the two. Remember - we only need to plot our primary hypothesis. Note however, that Tukey gives us our differences and \\(p\\) values for each set of tests and comparisons:\n\nTukeyHSD(interactive_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Measurements ~ factor(Treatments) + factor(Blocking_Variable) + factor(Treatments) * factor(Blocking_Variable), data = data)\n\n$`factor(Treatments)`\n                                  diff         lwr       upr     p adj\nExplanatory_2-Explanatory_1  3.3211111  2.51730768  4.124915 0.0000000\nExplanatory_3-Explanatory_1  0.8811111  0.07730768  1.684915 0.0304521\nExplanatory_3-Explanatory_2 -2.4400000 -3.24380343 -1.636197 0.0000011\n\n$`factor(Blocking_Variable)`\n                      diff        lwr       upr     p adj\nGroup 2-Group 1 -0.2711111 -1.0749145 0.5326923 0.6710682\nGroup 3-Group 1  1.7283333  0.9245299 2.5321368 0.0000928\nGroup 3-Group 2  1.9994444  1.1956410 2.8032479 0.0000159\n\n$`factor(Treatments):factor(Blocking_Variable)`\n                                                     diff         lwr\nExplanatory_2:Group 1-Explanatory_1:Group 1  1.931667e+00  0.02027781\nExplanatory_3:Group 1-Explanatory_1:Group 1  1.118333e+00 -0.79305553\nExplanatory_1:Group 2-Explanatory_1:Group 1 -7.116667e-01 -2.62305553\nExplanatory_2:Group 2-Explanatory_1:Group 1  2.948333e+00  1.03694447\nExplanatory_3:Group 2-Explanatory_1:Group 1 -1.776357e-15 -1.91138886\nExplanatory_1:Group 3-Explanatory_1:Group 1  1.016667e+00 -0.89472219\nExplanatory_2:Group 3-Explanatory_1:Group 1  5.388333e+00  3.47694447\nExplanatory_3:Group 3-Explanatory_1:Group 1  1.830000e+00 -0.08138886\nExplanatory_3:Group 1-Explanatory_2:Group 1 -8.133333e-01 -2.72472219\nExplanatory_1:Group 2-Explanatory_2:Group 1 -2.643333e+00 -4.55472219\nExplanatory_2:Group 2-Explanatory_2:Group 1  1.016667e+00 -0.89472219\nExplanatory_3:Group 2-Explanatory_2:Group 1 -1.931667e+00 -3.84305553\nExplanatory_1:Group 3-Explanatory_2:Group 1 -9.150000e-01 -2.82638886\nExplanatory_2:Group 3-Explanatory_2:Group 1  3.456667e+00  1.54527781\nExplanatory_3:Group 3-Explanatory_2:Group 1 -1.016667e-01 -2.01305553\nExplanatory_1:Group 2-Explanatory_3:Group 1 -1.830000e+00 -3.74138886\nExplanatory_2:Group 2-Explanatory_3:Group 1  1.830000e+00 -0.08138886\nExplanatory_3:Group 2-Explanatory_3:Group 1 -1.118333e+00 -3.02972219\nExplanatory_1:Group 3-Explanatory_3:Group 1 -1.016667e-01 -2.01305553\nExplanatory_2:Group 3-Explanatory_3:Group 1  4.270000e+00  2.35861114\nExplanatory_3:Group 3-Explanatory_3:Group 1  7.116667e-01 -1.19972219\nExplanatory_2:Group 2-Explanatory_1:Group 2  3.660000e+00  1.74861114\nExplanatory_3:Group 2-Explanatory_1:Group 2  7.116667e-01 -1.19972219\nExplanatory_1:Group 3-Explanatory_1:Group 2  1.728333e+00 -0.18305553\nExplanatory_2:Group 3-Explanatory_1:Group 2  6.100000e+00  4.18861114\nExplanatory_3:Group 3-Explanatory_1:Group 2  2.541667e+00  0.63027781\nExplanatory_3:Group 2-Explanatory_2:Group 2 -2.948333e+00 -4.85972219\nExplanatory_1:Group 3-Explanatory_2:Group 2 -1.931667e+00 -3.84305553\nExplanatory_2:Group 3-Explanatory_2:Group 2  2.440000e+00  0.52861114\nExplanatory_3:Group 3-Explanatory_2:Group 2 -1.118333e+00 -3.02972219\nExplanatory_1:Group 3-Explanatory_3:Group 2  1.016667e+00 -0.89472219\nExplanatory_2:Group 3-Explanatory_3:Group 2  5.388333e+00  3.47694447\nExplanatory_3:Group 3-Explanatory_3:Group 2  1.830000e+00 -0.08138886\nExplanatory_2:Group 3-Explanatory_1:Group 3  4.371667e+00  2.46027781\nExplanatory_3:Group 3-Explanatory_1:Group 3  8.133333e-01 -1.09805553\nExplanatory_3:Group 3-Explanatory_2:Group 3 -3.558333e+00 -5.46972219\n                                                    upr     p adj\nExplanatory_2:Group 1-Explanatory_1:Group 1  3.84305553 0.0464895\nExplanatory_3:Group 1-Explanatory_1:Group 1  3.02972219 0.5317567\nExplanatory_1:Group 2-Explanatory_1:Group 1  1.19972219 0.9173224\nExplanatory_2:Group 2-Explanatory_1:Group 1  4.85972219 0.0010200\nExplanatory_3:Group 2-Explanatory_1:Group 1  1.91138886 1.0000000\nExplanatory_1:Group 3-Explanatory_1:Group 1  2.92805553 0.6439270\nExplanatory_2:Group 3-Explanatory_1:Group 1  7.29972219 0.0000003\nExplanatory_3:Group 3-Explanatory_1:Group 1  3.74138886 0.0667267\nExplanatory_3:Group 1-Explanatory_2:Group 1  1.09805553 0.8457970\nExplanatory_1:Group 2-Explanatory_2:Group 1 -0.73194447 0.0032271\nExplanatory_2:Group 2-Explanatory_2:Group 1  2.92805553 0.6439270\nExplanatory_3:Group 2-Explanatory_2:Group 1 -0.02027781 0.0464895\nExplanatory_1:Group 3-Explanatory_2:Group 1  0.99638886 0.7519790\nExplanatory_2:Group 3-Explanatory_2:Group 1  5.36805553 0.0001577\nExplanatory_3:Group 3-Explanatory_2:Group 1  1.80972219 0.9999999\nExplanatory_1:Group 2-Explanatory_3:Group 1  0.08138886 0.0667267\nExplanatory_2:Group 2-Explanatory_3:Group 1  3.74138886 0.0667267\nExplanatory_3:Group 2-Explanatory_3:Group 1  0.79305553 0.5317567\nExplanatory_1:Group 3-Explanatory_3:Group 1  1.80972219 0.9999999\nExplanatory_2:Group 3-Explanatory_3:Group 1  6.18138886 0.0000097\nExplanatory_3:Group 3-Explanatory_3:Group 1  2.62305553 0.9173224\nExplanatory_2:Group 2-Explanatory_1:Group 2  5.57138886 0.0000766\nExplanatory_3:Group 2-Explanatory_1:Group 2  2.62305553 0.9173224\nExplanatory_1:Group 3-Explanatory_1:Group 2  3.63972219 0.0947841\nExplanatory_2:Group 3-Explanatory_1:Group 2  8.01138886 0.0000000\nExplanatory_3:Group 3-Explanatory_1:Group 2  4.45305553 0.0047491\nExplanatory_3:Group 2-Explanatory_2:Group 2 -1.03694447 0.0010200\nExplanatory_1:Group 3-Explanatory_2:Group 2 -0.02027781 0.0464895\nExplanatory_2:Group 3-Explanatory_2:Group 2  4.35138886 0.0069891\nExplanatory_3:Group 3-Explanatory_2:Group 2  0.79305553 0.5317567\nExplanatory_1:Group 3-Explanatory_3:Group 2  2.92805553 0.6439270\nExplanatory_2:Group 3-Explanatory_3:Group 2  7.29972219 0.0000003\nExplanatory_3:Group 3-Explanatory_3:Group 2  3.74138886 0.0667267\nExplanatory_2:Group 3-Explanatory_1:Group 3  6.28305553 0.0000070\nExplanatory_3:Group 3-Explanatory_1:Group 3  2.72472219 0.8457970\nExplanatory_3:Group 3-Explanatory_2:Group 3 -1.64694447 0.0001097\n\n\nI do not plot this here, but it would be similar to the other parts of this test.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#friedmans-test",
    "href": "anova2.html#friedmans-test",
    "title": "13  ANOVA: Part 2",
    "section": "13.3 Friedman’s test",
    "text": "13.3 Friedman’s test\n\n13.3.1 Using R\nFriedman’s test is a non-parametric alternative to a two-way ANOVA, so as you would guess, it can be painful to implement. We will use an altered version of the same test we’ve used before:\n\n# set seed - make reproducible\nset.seed(8675309)\n\n### DO NOT NEED TO REPEAT THIS\n### CREATING DATA FOR EXAMPLE \n\n# new set of foods - this time, ten of them\nTreatments &lt;- c(paste(\"Treatment\",1:10)) |&gt;\n  as.factor()\n\n# pre-created data frame of locations from earlier\nBlocking_Factor &lt;- c(paste(\"Block\", 1:10)) |&gt;\n  as.factor()\n\nlong_data &lt;- crossing(Blocking_Factor, Treatments)\n\nlong_data$Measurements &lt;- NA\n\nfor(i in 1:length(unique(long_data$Treatments))){\n  subset_rows &lt;- which(long_data$Treatments==long_data$Treatments[i])\n  long_data$Measurements[subset_rows] &lt;- runif(n = length(subset_rows),\n                                               min = i-2, max = i+2) |&gt; \n    round(1)\n}\n\nlong_data\n\n# A tibble: 100 × 3\n   Blocking_Factor Treatments   Measurements\n   &lt;fct&gt;           &lt;fct&gt;               &lt;dbl&gt;\n 1 Block 1         Treatment 1          -0.4\n 2 Block 1         Treatment 10          3.4\n 3 Block 1         Treatment 2           1.2\n 4 Block 1         Treatment 3           5.9\n 5 Block 1         Treatment 4           6.9\n 6 Block 1         Treatment 5           7  \n 7 Block 1         Treatment 6           7.6\n 8 Block 1         Treatment 7           7.6\n 9 Block 1         Treatment 8           8.1\n10 Block 1         Treatment 9           9.8\n# ℹ 90 more rows\n\n\nNow that we have our expanded and randomized table, we can get started with our test.\nOur calculation for the Friedman’s test statistic \\(Q\\) (not to be confused with Tukey’s \\(q\\)!) is: \\[Q = \\frac{12}{nk(k+1)} \\cdot \\Sigma R_j^2 - 3n(k+1)\\]\nwhere \\(n\\) is the total number of individuals in each sample in the dataset, \\(k\\) is the number of groups, and \\(R_j^2\\) is the sum of the ranks.\nIn this class, we will do this in R.\n\nfriedman_long_data &lt;- friedman.test(y = long_data$Measurements,\n                                    groups = long_data$Treatments,\n                                    blocks = long_data$Blocking_Factor)\n\nprint(friedman_long_data)\n\n\n    Friedman rank sum test\n\ndata:  long_data$Measurements, long_data$Treatments and long_data$Blocking_Factor\nFriedman chi-squared = 79.983, df = 9, p-value = 1.629e-13\n\n\nNote you will get a different answer if you are switching the blocks and the groups.\nWe can use the following, from package PMCMRplus, to find the adjacent and non-adjacent groups.\n\n# find differences\nfrdAllPairsConoverTest(y = long_data$Measurements,\n                       groups = long_data$Treatments,\n                       blocks = long_data$Blocking_Factor, \n                       p.adjust.method = \"bonf\")\n\n\n    Pairwise comparisons using Conover's all-pairs test for a two-way balanced complete block design\n\n\ndata: y, groups and blocks\n\n\n             Treatment 1 Treatment 10 Treatment 2 Treatment 3 Treatment 4\nTreatment 10 1.00000     -            -           -           -          \nTreatment 2  1.00000     1.00000      -           -           -          \nTreatment 3  0.00094     0.10167      0.80301     -           -          \nTreatment 4  3.8e-09     1.6e-06      3.4e-05     0.19031     -          \nTreatment 5  3.2e-11     1.6e-08      4.0e-07     0.00637     1.00000    \nTreatment 6  &lt; 2e-16     9.2e-16      2.5e-14     1.5e-09     0.00042    \nTreatment 7  &lt; 2e-16     &lt; 2e-16      3.6e-16     2.0e-11     9.3e-06    \nTreatment 8  &lt; 2e-16     &lt; 2e-16      &lt; 2e-16     6.1e-15     3.8e-09    \nTreatment 9  &lt; 2e-16     &lt; 2e-16      &lt; 2e-16     &lt; 2e-16     1.1e-12    \n             Treatment 5 Treatment 6 Treatment 7 Treatment 8\nTreatment 10 -           -           -           -          \nTreatment 2  -           -           -           -          \nTreatment 3  -           -           -           -          \nTreatment 4  -           -           -           -          \nTreatment 5  -           -           -           -          \nTreatment 6  0.01886     -           -           -          \nTreatment 7  0.00063     1.00000     -           -          \nTreatment 8  4.0e-07     0.34617     1.00000     -          \nTreatment 9  1.4e-10     0.00094     0.02676     1.00000    \n\n\n\nP value adjustment method: bonferroni\n\n\nAs we can see, some pairs are inseparable and others are separable. We can now plot as for the other problems.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "anova2.html#homework-chapter-12",
    "href": "anova2.html#homework-chapter-12",
    "title": "13  ANOVA: Part 2",
    "section": "13.4 Homework: Chapter 12",
    "text": "13.4 Homework: Chapter 12\nFor problems 12.1, 12.2, 12.3, 12.4, and 12.5, state your hypotheses in sentence form and mathematically. Then, identify the appropriate ANOVA and perform the analysis. If you reject the null, complete a Tukey test and plot your results, showing letters denoting each group. Note that 12.4 requires a Friedman’s test, but all other problems require some form of ANOVA.\nNext, for problems 12.7, 12.8, and 12.9, identify the appropriate test and justify your reasoning. State the null and alternative hypothesis in word form and mathematically, and perform your analysis. If you perform an ANOVA and you reject the null hypothesis, plot your results and label the groups by letter.\nRemember, only plot the results if you reject your primary hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ANOVA: Part 2</span>"
    ]
  },
  {
    "objectID": "cor_reg.html",
    "href": "cor_reg.html",
    "title": "14  Correlation & regression",
    "section": "",
    "text": "14.1 Introduction\nWhen we are comparing two continuous variables, we use two forms of tests: correlation to understand if there is a relationship between two variables, and linear regression to determine what that relationships is.\nRemember - “if” is always correlation, and “what is it” is always linear regression when choosing a test for an exam.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#correlation",
    "href": "cor_reg.html#correlation",
    "title": "14  Correlation & regression",
    "section": "14.2 Correlation",
    "text": "14.2 Correlation\nCorrelation - denoted by \\(\\rho\\) (“rho”) and not to be confused with \\(p\\) - is a measure of how closely related to continuous variables appear to be. This can vary from a purely negative relationship to a purely positive relationship, such that \\(-1 \\le \\rho \\le 1\\) with \\(\\rho = 0\\) indicating a random relationship between data.\nWe can visualize these as follows:\n\n### ILLUSTRATIVE PURPOSES ONLY\n\n# create two random uniform distributions\ny &lt;- runif(10)\nx &lt;- runif(10)\n\nFor example, two things compared to themselves have a \\(\\rho = 1\\).\n\nplot(y, y, pch = 19)\n\n\n\n\n\n\n\n\n\ncor(y, y)\n\n[1] 1\n\n\nAs we can see above, the correlation is 1.\nPlotting by the negative will be a correlation of -1.\n\nplot(y, -y, pch = 19)\n\n\n\n\n\n\n\n\n\ncor(y, -y)\n\n[1] -1\n\n\nLastly, two random variables plotted against each other should have \\(\\rho \\approx 0\\).\n\nplot(y, x, \n     pch = 19, \n     asp = 1) # aspect ratio\n\n\n\n\n\n\n\n\n\ncor(y, x) |&gt; \n  round(2)\n\n[1] 0.09\n\n\n\n14.2.1 Pearson’s\nPearson’s correlation coefficient is our value for parametric tests. We often denote our correlation coefficient as \\(r\\) and not \\(\\rho\\) for this particular test. It is calculated as follows:\n\\[\nr = \\frac{\\Sigma xy - (\\frac{\\Sigma x \\Sigma y}{n})}{\\sqrt{(\\Sigma x^2 - \\frac{(\\Sigma x)^2}{n}})(\\Sigma y^2 - \\frac{(\\Sigma y)^2}{n})}\n\\]\nwhere \\(x\\) is variable 1, \\(y\\) is variable 2, and \\(n\\) is the total number of data point pairs.\nIn this class, we will be using R to calculate \\(r\\), which is done using the command cor. To ensure we are using the correct method, we need to set method = \"pearson\".\n\nset.seed(8675309)\n\n### EXAMPLE DATA\nx &lt;- c(1,2,5,3,4,5,8,7,9,6,10,12,15,20,25)\ny &lt;- c(2,5,4,3,8,6,4,2,8,9,15,13,10,18,19)\n\nplot(x, y, pch = 19)\n\n\n\n\n\n\n\n\n\ncor(x, y, method = \"pearson\") |&gt; \n  round(2)\n\n[1] 0.86\n\n\nAs we can see, these data are fairly positively correlated. As x increases, so does y. But how significant is this relationship?\nWell, we can calculate two things - the amount of variation explained, which is \\(r^2\\), and the significance of the relationships, which is determined via a \\(t\\) test and the equation \\(t=r \\sqrt{\\frac{n-2}{1-r^2}}\\). This is a two-tailed distribution, with \\(df = n-2\\).\nWe can write a function to perform all of these options:\n\nbiol305_cor &lt;- function(x=NA, y=NA, method = \"pearson\"){\n  if(is.data.frame(x)==T){\n    if(ncol(x)==2){\n      r &lt;- cor(x[,1], x[,2], method = method)\n    }else{\n      r &lt;- cor(x, method = method)\n    }\n\n    r2 &lt;- r\n    r[r==1|r==-1] &lt;- 0\n    \n    n &lt;- 2*nrow(x)\n  }else{\n    r &lt;- cor(x, y, method = method)\n    \n    n &lt;- 2*length(x)\n  }\n  \n  t_val &lt;- r*sqrt((n-2)/(1-r^2))\n  \n  p &lt;- pt(t_val, df = n - 2)\n  \n  p[p &gt; 0.5] &lt;- 1 - p[p &gt; 0.5]\n  p[p &gt; 0.005] &lt;- round(p[p &gt; 0.005],2)\n  p[p &gt; 0.0005] &lt;- round(p[p &gt; 0.0005],3)\n  p[p &gt; 0.00005] &lt;- round(p[p &gt; 0.00005],4)\n  p[p &lt; 0.00005] &lt;- \"&lt; 0.0001\"\n  \n  if(is.data.frame(x)==T){\n    print(\"Correlation:\")\n    print(round(r, 2))\n    if(ncol(x) == 2){\n      print(paste0(\"Degrees of freedom: \", n - 2))\n      print(paste0(\"t value: \", round(t_val, 2)))\n      print(paste0(\"P value: \", p))\n    }\n    if(ncol(x) &gt; 2){\n      print(paste0(\"Degrees of freedom: \", n - 2))\n      print(\"\")\n      print(\"t value: \")\n      print(round(t_val, 2))\n      print(\"\")\n      print(\"P value: \")\n      print(p)\n    }\n  }else{\n    print(paste0(\"Correlation: \", round(r, 2)))\n    print(paste0(\"Degrees of freedom: \", n - 2))\n    print(paste0(\"t value: \", round(t_val, 2)))\n    print(paste0(\"P value: \", p))\n  }\n}\n\nLet’s test our function.\n\nbiol305_cor(x, y, method = \"pearson\")\n\n[1] \"Correlation: 0.86\"\n[1] \"Degrees of freedom: 28\"\n[1] \"t value: 8.93\"\n[1] \"P value: &lt; 0.0001\"\n\n\nThere we go! Our function printed out everything that we need.\n\n\n14.2.2 Spearman’s\nSpearman’s correlation is one of the non-parametric methods for our correlation tests. We can use this for ranked data or for non-parametric datasets. We do this the exact same way, except we change method = \"spearman\".\n\n\n14.2.3 Other non-parametric methods\nTo be expanded upon, but not necessary for the class at present.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#regression",
    "href": "cor_reg.html#regression",
    "title": "14  Correlation & regression",
    "section": "14.3 Regression",
    "text": "14.3 Regression\nRegression is used when we want to know what the relationship is between two variables. Regression operates similar to ANOVA and correlation, providing us with the nature of the relationship, the strength of the relationship, and gives us values for calculating the relationship. For this class, we are only focusing on linear regression for relationships between linear variables.\nThe equation for a regression line is often written as \\(y_i = \\alpha + \\beta x_i + e_i\\), where \\(\\alpha\\) is the \\(y\\) intercept, \\(\\beta\\) is the slope, and \\(e\\) is the error around each point. We will not perform regression calculations by hand in this class.\n\n14.3.1 Parametric\nWe will use out previous established x and y datasets that are strongly positively correlated for this example. The equation for calculating a linear relationship is lm, which stands for “linear model”. This uses equations like ANOVA, but can also use two vectors of data.\n\nxy_linear &lt;- lm(y ~ x)\n\nsummary(xy_linear)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1189 -1.4838 -0.5423  1.9757  5.7460 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1370     1.2825   1.666     0.12    \nx             0.7117     0.1169   6.086 3.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.964 on 13 degrees of freedom\nMultiple R-squared:  0.7402,    Adjusted R-squared:  0.7202 \nF-statistic: 37.04 on 1 and 13 DF,  p-value: 3.867e-05\n\n\nAs we can see, this returned an ANOVA table to use that tells us the value and significance of our intercept as well as the value and significance of the the slope (here, shown as x; it will always show the explanatory variable in this slot for the name).\nLooking at the above, we can see that the slope is not significantly non-zero with a \\(p = 0.12\\), but that the slope is significantly non-zero with \\(p &lt; 0.0001\\). We also have our \\(R^2\\) values returned, which is similar to the \\(r\\) we got for correlation. Indeed, our correlation was \\(r = 0.86\\), with \\(r^2 = 0.74\\), which is very similar to the Multiple R-squared shown in the above ANOVA table.\nR has a built in function within ggplot that will add a linear model to our plot and will show error regions as well. First, we need to make sure our data are in a data.frame.\n\ndata &lt;- data.frame(x, y)\n\ndata\n\n    x  y\n1   1  2\n2   2  5\n3   5  4\n4   3  3\n5   4  8\n6   5  6\n7   8  4\n8   7  2\n9   9  8\n10  6  9\n11 10 15\n12 12 13\n13 15 10\n14 20 18\n15 25 19\n\n\nNext, we can plot the data in ggplot.\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") + \n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nJust like that, we have created a plot of our linear regression. Note that you should always plot the lines only within the extent of the data; this is harder to do in other programs, but R does it for us!\nR can also allow us to predict different values using our linear model:\n\n# must be in data frame format\ntest_data &lt;- data.frame(1:25)\n\ncolnames(test_data) &lt;- \"x\" # must be same as explanatory in data\n\npredict(xy_linear, test_data)\n\n        1         2         3         4         5         6         7         8 \n 2.848692  3.560399  4.272105  4.983811  5.695517  6.407223  7.118929  7.830635 \n        9        10        11        12        13        14        15        16 \n 8.542341  9.254047  9.965753 10.677460 11.389166 12.100872 12.812578 13.524284 \n       17        18        19        20        21        22        23        24 \n14.235990 14.947696 15.659402 16.371108 17.082814 17.794521 18.506227 19.217933 \n       25 \n19.929639 \n\n\nLet’s visualize these points for illustration’s sake.\n\nplot(x, y, pch = 19)\npoints(1:25, as.numeric(predict(xy_linear, test_data)),\n       pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\nAs we can see above, our points follow the line from the plot. By using this format, however, we can make predictions of value for any point we want.\n\n\n14.3.2 Non-parametric\nWe are not doing non-parametric linear regression in this class.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "cor_reg.html#homework",
    "href": "cor_reg.html#homework",
    "title": "14  Correlation & regression",
    "section": "14.4 Homework",
    "text": "14.4 Homework\n\n14.4.1 Chapter 13\n\n\n14.4.2 Chapter 14",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Correlation & regression</span>"
    ]
  },
  {
    "objectID": "multivariate.html",
    "href": "multivariate.html",
    "title": "15  Multivariate methods",
    "section": "",
    "text": "15.1 Multivariate analyses\nMultivariate analyses are commonly used to help understand the variation that exists in large, multidimensional datasets. These analyses can help us understand relationships among individuals and help us determine how many groups (and what groups) exist in a dataset.\nFor this particular walkthrough, we will be using data on the Northern Double-collared Sunbird Cinnyris reichenowi sensu lato complex from equatorial Afromontane regions (Cooper et al. 2021a, b).\nlibrary(tidyverse)\n\ncinnyris &lt;- read_csv(\"https://raw.github.com/jacobccooper/biol305_unk/main/datasets/cinnyris.csv\")\n\nmale_cinnyris &lt;- cinnyris |&gt; \n  mutate(Genus = as.factor(Genus)) |&gt; \n  mutate(Species = as.factor(Species)) |&gt; \n  mutate(Subspecies = as.factor(Subspecies)) |&gt; \n  filter(Age == \"Adult\") |&gt; \n  filter(Sex == \"Male\") |&gt; \n  select(-`Collection Name`, -Locality, -Country, -Age, \n         -`Notes / Existing Damage Precluding Measurements`) |&gt; \n  na.omit()",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate methods</span>"
    ]
  },
  {
    "objectID": "multivariate.html#principal-components",
    "href": "multivariate.html#principal-components",
    "title": "15  Multivariate methods",
    "section": "15.2 Principal components",
    "text": "15.2 Principal components\n\n\nClick here for an overview of PCA\n\nA very common method of visualizing variation in population is using principal components analyses (PCA). Principal components help collapse multidimensional data into a few dimensions that explain the most variation possible across the dataset.\nFor principal components, we want to use the library vegan.\n\nlibrary(vegan)\n\nLoading required package: permute\n\n\nThe function for principal components in vegan is rda, which will only work on a whole dataframe with no non-numeric data. Thus, we must create a new object of only numerical data and then re-merge it with the original dataframe. The row order is maintained, so this will not be an issue.\n\nmale.cinnyris.data &lt;- male_cinnyris |&gt; \n  select(-Genus, -Species, -Subspecies, \n         -Collection, -Catalog, -Locality2, -Sex)\n\ncinnyris.pca &lt;- rda(male.cinnyris.data)\n\ncinnyris.pca\n\n\nCall: rda(X = male.cinnyris.data)\n\n              Inertia Rank\nTotal              23     \nUnconstrained      23    6\n\nInertia is variance\n\nEigenvalues for unconstrained axes:\n   PC1    PC2    PC3    PC4    PC5    PC6 \n15.620  4.443  1.940  0.877  0.076  0.040 \n\n\nAbove, we can see that we now have a complex object that contains our PCA. We can use the function biplot to see how our data look and how our variables contributed to the principal components.\n\nbiplot(cinnyris.pca)\nordihull(cinnyris.pca,\n         group = male_cinnyris$Subspecies)\n\n\n\n\n\n\n\n\nWe can also do a ggplot to color our data and see how it works. Let’s combine our datasets so we can do this.\n\ncinnyris.male.pca &lt;- cbind(male_cinnyris, \n                           # extract PC values\n                           cinnyris.pca$CA$u)\n\nggplot(cinnyris.male.pca, aes(x = PC1, y = PC2)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\nTo the naked eye, these data seem to fall into two major clusters. This can be examined again, this time assigning colors to one of our explanatory variables: subspecies.\n\nggplot(cinnyris.male.pca, aes(x = PC1, y = PC2, colour = Subspecies)) +\n  geom_point() +\n  stat_ellipse() +\n  theme_classic()\n\nToo few points to calculate an ellipse\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_path()`).\n\n\n\n\n\n\n\n\n\nAs we can see, many of these birds fall into one of the two groups. Only one population - genderuensis - seems to really bridge both groups.\nThe following shows how each variable contributed to each principal component:\n\ncinnyris.pca$CA$v\n\n                                                  PC1          PC2         PC3\nRight.wing.chord                          -0.61936877  0.498711666  0.60193981\nTail.length                               -0.69190828 -0.709947486 -0.12739287\nCulmen.length                             -0.33598591  0.419621089 -0.62835637\nBill.depth..base.of.feathers.on.mandible. -0.01633872  0.004353418 -0.01231269\nBill.width..base.of.feathers.on.maxilla.  -0.05865394  0.061714372 -0.09962101\nLeft.Tarsus                               -0.14509132  0.259522190 -0.46532983\n                                                  PC4          PC5\nRight.wing.chord                           0.07304690 -0.001241684\nTail.length                                0.02988534 -0.008716903\nCulmen.length                             -0.53855805 -0.161236784\nBill.depth..base.of.feathers.on.mandible. -0.05951994  0.216257889\nBill.width..base.of.feathers.on.maxilla.  -0.08219923  0.962867346\nLeft.Tarsus                                0.83272259  0.006646209\n                                                    PC6\nRight.wing.chord                          -0.0002699604\nTail.length                               -0.0062803042\nCulmen.length                             -0.0125622441\nBill.depth..base.of.feathers.on.mandible.  0.9742958451\nBill.width..base.of.feathers.on.maxilla.  -0.2212610835\nLeft.Tarsus                                0.0399226008\n\n\nWe can also see the percent variance explained by each axis, which is contained within the eigenvectors of the PCA:\n\ncinnyris.pca$CA$eig/sum(cinnyris.pca$CA$eig)\n\n        PC1         PC2         PC3         PC4         PC5         PC6 \n0.679234583 0.193214694 0.084367157 0.038143954 0.003291025 0.001748588 \n\n\nThe first principal component explains 68% of the variation, and the second 19% of the variation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate methods</span>"
    ]
  },
  {
    "objectID": "multivariate.html#clustering",
    "href": "multivariate.html#clustering",
    "title": "15  Multivariate methods",
    "section": "15.3 Clustering",
    "text": "15.3 Clustering\n\n15.3.1 K-means clustering\n\n\nClick here for K-means clustering\n\n\n\n\n15.3.2 Hierarchical clustering\n\n\nClick here for hierarchical clustering",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate methods</span>"
    ]
  },
  {
    "objectID": "multivariate.html#homework",
    "href": "multivariate.html#homework",
    "title": "15  Multivariate methods",
    "section": "15.4 Homework",
    "text": "15.4 Homework\n\n15.4.1 Disentangling microbiomes\nThis is an example problem. These data are from (Couper et al. 2019).\n\n# Run this whole chunk to load data\n\nmicrobiome &lt;- read_csv(\"https://zenodo.org/records/4940316/files/MicrobialCommunityAssembly_Dryad.csv\") |&gt; \n  t() |&gt; \n  as.data.frame()\n\nNew names:\nRows: 36 Columns: 69\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(69): ...1, PW001, PW005, PW012, PW013, PW014, PW015, PW016, PW017, PW01...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\ncolnames.x &lt;- microbiome[1,]\n\ncolnames(microbiome) &lt;- colnames.x\n\nmicrobiome &lt;- microbiome[-1,]\n\n\n\n\n\nCooper, J. C., J. D. Maddox, K. McKague, and J. M. Bates (2021b). Data from: Multiple lines of evidence indicate ongoing allopatric and parapatric diversification in an Afromontane sunbird (Cinnyris reichenowi). https://doi.org/10.5061/dryad.34tmpg4j0\n\n\nCooper, J. C., J. D. Maddox, K. McKague, and J. M. Bates (2021a). Multiple lines of evidence indicate ongoing allopatric and parapatric diversification in an Afromontane sunbird (Cinnyris reichenowi). Ornithology 138:ukaa081.\n\n\nCouper, L. I., J. Y. Kwan, J. Ma, and A. Swei (2019). Data from: Drivers and patterns of microbial community assembly in a lyme disease vector. [Online.] Available at https://doi.org/10.5061/dryad.2nv32qh.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multivariate methods</span>"
    ]
  },
  {
    "objectID": "picktest.html",
    "href": "picktest.html",
    "title": "16  Pick the test",
    "section": "",
    "text": "16.1 Picking the test\nA major component of the final will be picking the correct test to run on some data. Here, I cover the specific ways in which you can determine what test to use.\nPlease use this web page as an interactive way to pick the test.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "picktest.html#exceptions",
    "href": "picktest.html#exceptions",
    "title": "16  Pick the test",
    "section": "16.2 Exceptions",
    "text": "16.2 Exceptions\nBelow is a walk through for the most common statistical analyses, but keep in mind there are a few “less common” ones that we are using as well:\n\nBinomial test - if we are looking at something with discrete outcomes - like coin tosses, die rolls, etc. - we are doing a binomial test to determine the probability of a specific outcome. You can do this with binom.test.\nPoisson test - if we are looking at the probability of obtaining certain counts over events - specifically, looking at the probability of rare events - we will use a Poisson. You can do this with poisson.test.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "picktest.html#overview-of-picking-the-test",
    "href": "picktest.html#overview-of-picking-the-test",
    "title": "16  Pick the test",
    "section": "16.3 Overview of picking the test",
    "text": "16.3 Overview of picking the test\nBelow is an overview of picking basic statistical tests. There are more complex tests, but these are the main ones to consider for exams in this course.\n\nWhich test to pick for which combinations of variables.\n\n\n\n\n\n\n\n\nCategorical Explanatory\nContinuous Explanatory\n\n\n\n\nCategorical Response\n\n\\(\\chi^2\\) tests, especially if count data for category\n\nYate’s correction auto-applied for 2x2 tables\n\nFisher test is a 2x2 table\nNull is that counts match a known proportion (goodness-of-fit) or counts match each other (independence)\n\n\nLogistic regression, modeling categorical responses across continuous variables.\n\nNot covered in BIOL 305 at present\n\n\n\n\nContinuous Response\n\nSingle mean to population\n\nThis is a \\(Z\\)-score comparison, when the population parameters are known\n\n\\(H_0: \\bar{x} = \\mu\\)\n\nAlmost always better to resort to \\(t\\)-test comparison if unsure is a population - \\(t\\) approaches a \\(Z\\) as \\(df \\rightarrow \\infty\\)\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\nTwo measurements\n\nRemember - all \\(t\\)-tests default to Welch’s in R, assuming unequal variance\n\nWhen \\(\\sigma^2_1 = \\sigma^2_2\\), Welch’s \\(t\\)-test is the same as a standard \\(t\\)-test\nUsually better to use Welch’s, but justify reasoning\n\nOne-sample \\(t\\)-test, comparing the two groups\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\nTwo samples\n\nTwo-sample \\(t\\)-test, comparing the two samples\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\nPaired samples\n\nSpecial case - used when there are repeated measurements of the same sampling units\n\nNeed to set paired = TRUE\n\\(H_0: \\mu_d = 0\\)\n\n\n\\(\\ge\\) 3 samples\n\nANOVA - analysis of variance across multiple samples, computing variance within samples and between samples\n\nFor all ANOVA, \\(H_0: \\mu_1 = \\mu_2 = ... \\mu_i\\)\nSingle type of measurement is a one-way ANOVA\nWhen things are “blocked” into categories - like litters - that are unique per row, use a randomized block ANOVA (remember to put factor() around anything you are worried won’t be read as categorical!)\n\naov(response ~ explanatory + block, data)\n\nWhen things are individuals being repeatedly measured, use a repeated measures ANOVA (mathematically like randomized block ANOVA)\n\naov(response ~ explanatory + individual, data)\n\nWhen we are looking at more than one variable at once, you are doing an interactive ANOVA\n\naov(response ~ explanatory1 + explanatory2 + explanatory1*explanatory2)\n\n\n\n\n\nWhen looking at if there is a relationship, use correlation\n\n\\(H_0: \\rho = 0\\)\nEvaluated using a \\(t\\) statistic\n\nWhen looking at what the relationship is, use linear regression\n\nLine formula is \\(y = \\beta x + \\alpha + \\epsilon_i\\)\nVery similar to your high-school \\(y = mx +b\\)\n\\(H_0: \\beta = 0\\)\n\nRemember - linear regression works like an ANOVA, and has similar outputs\n\nCheck the ANOVA page for more information!",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "picktest.html#another-method---checklist",
    "href": "picktest.html#another-method---checklist",
    "title": "16  Pick the test",
    "section": "16.4 Another method - checklist",
    "text": "16.4 Another method - checklist\nFeel free to go through the following headings to also help you pick a test.\n\n16.4.1 Explanatory Variable\nThe explanatory variable is continuous and numeric. Pick this one for a continuous measurement variable, for example, such as Longitude, concentration, or other ratio or integer data.\nThe explanatory variable is discontinuous and categorical. In these cases, the explanatory variable is a condition, like a control and a treatment.\n\n\n16.4.2 Continuous explanatory variable\n\n16.4.2.1 Continuous response variable\nIf you have a continuous response variable, then you need to see what the question is asking:\n\nIs there a relationship?\n\nIf you are looking at a problem and it is simply asking if there is a relationship, you are looking at a correlation analysis.\n\nWhat is the relationship?\n\nIf you are asking what the relationship is or looking to be able to predict a value based on what you know, you are doing a linear regression. Note there are other kinds of regression, but in this class, we focus on linear regression.\nIf we have multiple response variables, we can do multiple regression, which we do not cover here.\n\n\n16.4.2.2 Discrete response variable\nIf you are looking at a discrete response variable, such as a state of 1 or 0 in response to a certain amount of stimulus, then you are doing a logistic regression. We did not cover this analysis in this class.\n\n\n\n16.4.3 Discrete explanatory variable\nFor discrete explanatory variables, we are often looking at categorical treatments or distinct groups, like species or geographic locations.\n\n16.4.3.1 Continuous response variable\nFor a continuous response variable, we need to ask ourselves how many categories we are dealing with.\n\nIf we are dealing with two categories or two measurements from the same individual, we are using a \\(t\\)-test.\n\nMake sure you check the t-test page to understand what kind of t-test is being performed. For repeated measurements from the same individuals or populations under different conditions, we have the paired t-test. Otherwise, we have the Welch’s t-test as the default in R. NOTE that the default assumes unequal variance; you must set var.equal = TRUE to perform a “true” t-test. Make sure you familiarize yourself with why our code is assuming variances are unequal.\nIf we know what the population is, we will use a \\(Z\\)-score, but bear in mind that we will almost always use a \\(t\\)-test to account for error. A \\(t\\)-test with infinite degrees of freedom is the same as a \\(Z\\)-score, so it is better to default to a \\(t\\)-test.\n\nIf we have three or more categories or treatments then we need to perform an ANOVA.\n\nIf we are simply comparing multiple groups, we are performing a one-way ANOVA. We also need to make sure we aren’t doing some sort of factorial ANOVA, repeated-measures ANOVA, or interactive ANOVA. Please read the ANOVA page to ensure you are using the correct format.\nRemember to label ANOVA plots with letters to indicate the separate groups.\nIf we have multiple response variables, we can use a MANOVA; we do not cover that in this class.\n\n\n16.4.3.2 Discrete response variable\nIf our response variable and our explanatory variable are discrete (i.e., categorical or nominal), then we are doing a \\(\\chi^2\\) test. This tests looks at counts in different categories. For example, looking at proportions of men and women who do and do not smoke would be a classic \\(\\chi^2\\) test.\n\nOther Online Resources\nhttps://statkat.com/statistical-technique-selection/tool-for-selecting-a-statistical-technique-step-one.php",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Pick the test</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "17  Conclusions",
    "section": "",
    "text": "17.1 Parting thoughts\nIn this class, we have covered two major things: (1) the basics of statistics for biological research and (2) the basics of using R to solve different computational problems. It is my hope that this class helps you both become a better researcher and also a more efficient researcher and student by using code to help you with your future projects.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "conclusion.html#ꮩꮣꮣꭺꮂꭲ",
    "href": "conclusion.html#ꮩꮣꮣꭺꮂꭲ",
    "title": "17  Conclusions",
    "section": "17.2 ᏙᏓᏓᎪᎲᎢ",
    "text": "17.2 ᏙᏓᏓᎪᎲᎢ\nᏙᏓᏓᎪᎲᎢ (pronounced doh-dah-dah-go-huh-ee) is a traditional Cherokee farewell. It does not mean goodbye, but rather reflects a parting of ways until a group of folks meet again.\nI enjoyed getting to know all of you in class, and please feel free to reach out or stop by and say hi if you are ever passing through Kearney in the future or if you need help with something biology related.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "18  Functions & Glossary",
    "section": "",
    "text": "18.1 Rounding & Formatting",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#rounding-formatting",
    "href": "glossary.html#rounding-formatting",
    "title": "18  Functions & Glossary",
    "section": "",
    "text": "Click for information on rounding\n\nAs a reminder, we are following the rounding rules of the Auk and Condor (now Ornithology and Ornithological Applications).\nSpecifically, these rules are as follows:\n\nNo naked decimals (except for caliber):\n\nPreface all decimals with a leading zero; \\(0.002\\) is correct, \\(.002\\) is incorrect.\nFor caliber, such as with weapons, never have a leading zero; ex., .45 bullet.\n\nUse the same number of decimals as the original data or the precision of the instrument:\n\nEx., if precision is \\(0.01\\), then all values should be rounded to hundredths.\n\nUse the same number of decimals for values and uncertainties:\n\nEx., \\(45 \\pm 4\\), \\(45.2 \\pm 3.8\\).\n\nRound percentages to whole numbers* (or to one decimal):\n\nFor this class, can round to one decimal, but in the referenced publications always round to whole number. You will never miss points for rounding percentage to a whole number.\n\nAll test statistics should be rounded to two (2) decimal places:\n\n\\(F\\), \\(t\\), \\(p\\), \\(\\tau\\), \\(\\chi^2\\), etc. should all be rounded to two decimal places.\n\nAll test statistics should show degrees of freedom:\n\nFor values with one degree of freedom, list like \\(t_{(9)}=1.56\\).\nFor values with multiple, list them; for ANOVA, put the degrees of freedom for Sum of Squares Between before degrees of freedom for Degrees of Error Within, like \\(F_{(5,43)}=5.23\\).\n\nRound P-values as follows:\n\n\\(P \\ge 0.01\\): round to two decimal places like \\(P = 0.24\\).\n\\(0.01 &gt; P \\ge 0.001\\): round to three decimal places like \\(P = 0.005\\).\n\\(0.001 &gt; P \\ge 0.0001\\): round to four decimal places like \\(P = 0.0007\\).\n\\(P &lt; 0.0001\\): list exactly as \\(P &lt; 0.0001\\).\n\nDO NOT list without spaces, or as \\(P&lt;.0001\\)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#common-commands",
    "href": "glossary.html#common-commands",
    "title": "18  Functions & Glossary",
    "section": "18.2 Common Commands",
    "text": "18.2 Common Commands\nThe following are common useful commands used in R, with examples of their use.\nCommands are given as Windows & Linux / Mac.\n\n18.2.1 The basics\n\nSave your file with CTRL + S / ⌘ + S\nUndo your last action with CTRL + Z / ⌘ + Z\nCut text with CTRL + X / ⌘ + X\nCopy text with CTRL + C / ⌘ + C\nPaste text with CTRL + V / ⌘ + V\nSelect all text in a document with CTRL + A / ⌘ + A\n\n\n\n18.2.2 Code basics\n\nA code chunk can be inserted using CTRL + SHIFT + I / ⌘ + shift + I\nRun a line of code with CTRL + ENTER / ⌘ + return\nRun a code chunk with CTRL + SHIFT + ENTER / ⌘ + shift + return\nRun all code in the document above where you were CTRL + SHIFT + ALT +P / ⌘ + shift + ⌥ + P\nRun all code with CTRL + ALT + R / ⌘ + ⌥ + R\n\n\n\n18.2.3 Code formatting\n\n&lt;- - save a value as an object. On Mac, the keyboard shortcut is ⌥ + -. Windows can be formatted so that the shortcut is ALT + -.\n\n\nx &lt;- 10\nx\n\n[1] 10\n\n\n\n|&gt; - “pipe” a command or output into another command. You can use the shortcut CTRL+SHIFT+M / ^ + SHIFT + M.\n\n\n# make repeatable\nset.seed(930)\n\n# random string\nx &lt;- rnorm(20)\n\nx |&gt; \n  # pass to summary\n  summary() |&gt; \n  # pass summary through round\n  round(2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -1.94   -0.48   -0.06   -0.05    0.36    1.80 \n\n\n\n-&gt; - save a value as an object at the end of a pipeline\n\n\n# same pipeline as previous, but more sequential\n\n# get random values\nrnorm(20) |&gt; \n  # pass to summary\n  summary() |&gt; \n  # pass summary through round\n  round(2) -&gt;\n  # save as x\n  x\n\nx\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -2.60   -0.77   -0.24   -0.18    0.57    1.73 \n\n\n\nc - concatenate, place two values together\n\n\nx &lt;- c(10,11)\nx\n\n[1] 10 11\n\n\n\n\n18.2.4 Boolean logic\nIn order to create arguments in R, such as commands like which, we need to use Boolean logic commands.\nFor this example, I will create a string from 1 to 10 to perform the commands on. Note that I am using brackets [ ] to call out an index within the object using these commands.\nThese are as follows:\n\n# example string\nx &lt;- 1:10\n\n\n&gt;: greater than\n\n\nx[x &gt; 5]\n\n[1]  6  7  8  9 10\n\n\n\n&lt;: less that\n\n\nx[x &lt; 5]\n\n[1] 1 2 3 4\n\n\n\n&gt;=: greater than or equal to\n\n\nx[x &gt;= 5]\n\n[1]  5  6  7  8  9 10\n\n\n\n&lt;=: less than or equal to\n\n\nx[x &lt;= 5]\n\n[1] 1 2 3 4 5\n\n\n\n==: is equal to\n\n\nx[x == 5]\n\n[1] 5\n\n\n\n|: or\n\n\nx[x &lt; 3 | x &gt; 7]\n\n[1]  1  2  8  9 10\n\n\n\n&: and\n\n\nx[x &gt; 3 & x &lt; 7]\n\n[1] 4 5 6",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#basic-statistics",
    "href": "glossary.html#basic-statistics",
    "title": "18  Functions & Glossary",
    "section": "18.3 Basic statistics",
    "text": "18.3 Basic statistics\n\n\nClick here for basic statistics\n\nFor these examples, we will create a random vector of number to demonstrate how they work.\n\n# make repeatable\nset.seed(8675309)\n# get random normal values\nx &lt;- rnorm(1000)\n\n\nmean - get the mean / average of a set of data\n\n\nmean(x)\n\n[1] -0.02659863\n\n\n\nmedian(x)\n\n[1] -0.0006443188\n\n\n\nIQR(x)\n\n[1] 1.378508\n\n\n\nrange(x)\n\n[1] -3.449557  3.412794\n\n\n\n# calculate range\nabs(range(x)[1] - range(x)[2])\n\n[1] 6.862351\n\n\n\n# calculate range\nmax(x) - min(x)\n\n[1] 6.862351\n\n\n\nhist(x)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#null-hypotheses",
    "href": "glossary.html#null-hypotheses",
    "title": "18  Functions & Glossary",
    "section": "18.4 Null hypotheses",
    "text": "18.4 Null hypotheses\n\n\nClick here for null hypotheses\n\n\n18.4.1 \\(\\chi^2\\) tests\n\n18.4.1.1 \\(\\chi^2\\) goodness-of-fit\n\\(H_0\\): The frequency of observations fits the expected observation frequencies.\n\n\n18.4.1.2 \\(\\chi^2\\) test of independence (i.e., association)\n\\(H_0\\): Variable X and Variable Y are not related.\n\n\n\n18.4.2 One-sample tests\n\n18.4.2.1 One-sample \\(t\\)-test\n\\(H_0\\): \\(\\mu=X\\)\n\n\n18.4.2.2 Paired \\(t\\)-test\n\\(H_0\\): \\(\\mu_1-\\mu_2=0\\) or \\(\\mu_d=0\\)\n\n\n18.4.2.3 Two-sample \\(t\\)-test\n\\(H_0\\): \\(\\mu_1=\\mu_2\\)\n\n\n\n18.4.3 Multi-sample tests\nAll ANOVA tests have essentially the same null hypotheses, with the primary null hypothesis being that all means are the same. Secondary hypotheses may exist about which means are different; we have to do a post-hoc test to determine which means are different.\n\\(H_0\\): \\(\\mu_1=\\mu_2=\\mu_3=...=\\mu_n\\)\n\n\n18.4.4 Correlation\nFor all correlation tests, the null is that the correlation is 0 (i.e., not correlated positively or negatively). You can get the \\(p\\) value for a test from a \\(t\\) distribution.\nAs an example, Pearson’s correlation:\n\\(H_0\\): \\(\\rho = 0\\)\n\n\n18.4.5 Regression\n\\(H_0\\): \\(\\beta = 0\\)\nwhere \\(\\beta\\) is the slope.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#custom-functions-from-class-and-elsewhere",
    "href": "glossary.html#custom-functions-from-class-and-elsewhere",
    "title": "18  Functions & Glossary",
    "section": "18.5 Custom functions from class and elsewhere",
    "text": "18.5 Custom functions from class and elsewhere\nThe following functions are those developed for this class or adapted from code posted to sources like StackOverflow.\nFor these stats, we are using the following example data:\n\n### EXAMPLE DATA ###\nx &lt;- c(1,2,3,5,5,4,6,8,7,9,5)\n\n\n18.5.0.1 Mode calculations\n\n\nClick here for mode\n\n\n# Based on Statology function\n# define function to calculate mode\n# works on vectors of data\nfind_mode &lt;- function(x) {\n  # get unique values from vector\n  u &lt;- unique(x)\n  # count number of occurrences for each value\n  tab &lt;- tabulate(match(x, u))\n  \n  # if no mode, say so\n  if(length(x)==length(u[tab == max(tab)])){\n    print(\"No mode.\")\n  }else{\n    # return the value with the highest count\n    u[tab == max(tab)]\n  }\n}\n\nfind_mode(x)\n\n[1] 5\n\n\n\n\n18.5.0.2 Standard error\n\nse &lt;- function(x){\n  n &lt;- length(x) # calculate n\n  s &lt;- sd(x) # calculate standard deviation\n  se_val &lt;- s/sqrt(n)\n  return(se_val)\n}\n\nse(x)\n\n[1] 0.7385489\n\n\n\n\n\n18.5.0.3 Coefficient of variation\n\n\nClick here for coefficient of variation\n\n\ncv &lt;- function(x){\n  sigma &lt;- sd(x)\n  mu &lt;- mean(x)\n  val &lt;- sigma/mu*100\n  return(val)\n}\n\ncv(x)\n\n[1] 48.98979\n\n\n\n\n\n18.5.1 Normal distributions\n\n18.5.1.1 \\(Z\\) score\n\n\nClick here for \\(Z\\)-score\n\nRemember - in the \\(Z\\)-score code below, if no \\(n\\) is specified, then it will default to \\(n = 1\\).\n\nzscore &lt;- function(xbar, mu, sd.x, n = 1){\n  z &lt;- (xbar - mu)/(sd.x/sqrt(n))\n  return(z)\n}\n\nzscore(xbar = 62,\n       mu = 65,\n       sd.x = 3.5,\n       n = 5)\n\n[1] -1.91663\n\n\n\n\n\n\n18.5.2 ANOVA\n\n\nClick here for ANOVA functions\n\nThe following example data are going to be used to illustrate these functions.\n\n#### EXAMPLE DATA ####\nset.seed(8675309)\n\nfor(i in 1:4){\n  x &lt;- rnorm(10)\n  if(i == 1){\n    x &lt;- rnorm(10, mean = 2)\n    data &lt;- x |&gt; as.data.frame()\n    colnames(data) &lt;- \"Response\"\n    data$Explanatory &lt;- paste0(\"x\",i)\n  }else{\n    newdat &lt;- x |&gt; as.data.frame()\n    colnames(newdat) &lt;- \"Response\"\n    newdat$Explanatory &lt;- paste0(\"x\",i)\n    data &lt;- rbind(data,newdat)\n  }\n}\n\n# split into \"typical\" table\nexpanded_data &lt;- NULL\nexpanded_data$x1 &lt;- data$Response[which(data$Explanatory==\"x1\")]\nexpanded_data$x2 &lt;- data$Response[which(data$Explanatory==\"x2\")]\nexpanded_data$x3 &lt;- data$Response[which(data$Explanatory==\"x3\")]\nexpanded_data$x4 &lt;- data$Response[which(data$Explanatory==\"x4\")]\n\nexpanded_data &lt;- expanded_data |&gt;\n  as.data.frame()\n\nThe above is a one-way ANOVA. As a reminder, we would calculate the test as follows:\n\n# pivot longer does not work for one-way ANOVA, requires blocking factor\n# can rbind things with same colnames to make longer\n\nexample_aov &lt;- aov(Response ~ Explanatory, data)\n\nsummary(example_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nExplanatory  3  23.40   7.801   11.54 1.89e-05 ***\nResiduals   36  24.33   0.676                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAbove, we can see a significant result of the ANOVA. We can follow this up with a Tukey test. This requires the package agricolae! Check the ANOVA pages, however, as not all ANOVA can use this agricolae shortcut method.\n\nexample_tukey &lt;- HSD.test(example_aov,\n                          # what to group by?\n                          \"Explanatory\",\n                          # significance level?\n                          alpha = 0.05, \n                          # are data unbalanced\n                          unbalanced = FALSE,\n                          # show answer?\n                          console = TRUE)\n\n\nStudy: example_aov ~ \"Explanatory\"\n\nHSD Test for Response \n\nMean Square Error:  0.6758192 \n\nExplanatory,  means\n\n     Response       std  r        se        Min      Max        Q25         Q50\nx1  1.7620153 1.0505466 10 0.2599652  0.4504476 3.972459  1.1175485  1.44911720\nx2  0.2841495 0.7532422 10 0.2599652 -0.4729986 1.985826 -0.3497379  0.21347543\nx3  0.2197337 0.7019368 10 0.2599652 -0.6150452 1.574903 -0.2436023 -0.04493909\nx4 -0.2890524 0.7345336 10 0.2599652 -1.9769014 0.684072 -0.5394534 -0.07741642\n          Q75\nx1 2.07491533\nx2 0.64579865\nx3 0.53544151\nx4 0.04323417\n\nAlpha: 0.05 ; DF Error: 36 \nCritical Value of Studentized Range: 3.808798 \n\nMinimun Significant Difference: 0.9901551 \n\nTreatments with the same letter are not significantly different.\n\n     Response groups\nx1  1.7620153      a\nx2  0.2841495      b\nx3  0.2197337      b\nx4 -0.2890524      b\n\n\n\n18.5.2.1 Summarize data (for plotting)\nRemember - you need to change \"Explanatory\" to your explanatory variable (in quotes!) and you need to change Response to your response column (no quotes!). The following requires plyr to work, but the function itself should call up plyr if you do not yet have it loaded.\n\n# summarize by group\nsummary_data &lt;- function(data, explanatory){\n  require(plyr)\n  ddply(data, paste(explanatory), summarise,\n                 N = length(Response),\n                 mean = mean(Response),\n                 sd = sd(Response),\n                 se = sd / sqrt(N))\n}\n\nexample_summary &lt;- summary_data(data = data, explanatory = \"Explanatory\")\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nexample_summary\n\n  Explanatory  N       mean        sd        se\n1          x1 10  1.7620153 1.0505466 0.3322120\n2          x2 10  0.2841495 0.7532422 0.2381961\n3          x3 10  0.2197337 0.7019368 0.2219719\n4          x4 10 -0.2890524 0.7345336 0.2322799\n\n\n\n\n18.5.2.2 Significant label maker\nThis command requires a Tukey HSD object from agricolae. You can manually create a table like this for some other scenarios; see relevant pages for documentation.\n\n# note first group must be EXACT MATCH to your summary_data object\n# groups are saved in the Tukey object\n# this is true for Tukey later as well\n\n# the following is a function that will make the significant label table\nsig.label.maker &lt;- function(tukey_test, group_name){\n  sig.labels &lt;- tukey_test$groups |&gt; \n    # convert to a data.frame\n    as.data.frame() |&gt;\n    # create a new column - place rownames into the column\n    # converts to a format better for ggplot\n    mutate(Explanatorys = rownames(tukey_test$groups)) |&gt;\n    # rename column to prevent confusion\n    # specify dplyr; default function may be from plyr and not work\n    dplyr::rename(Significance = groups)\n  colnames(sig.labels)[which(colnames(sig.labels) == \"Explanatorys\")] &lt;- group_name\n  return(sig.labels)\n}\n\n# Function requires explanatory groups in quotes\nsig_labels &lt;- sig.label.maker(example_tukey, \"Explanatory\")\n\nsig_labels\n\n     Response Significance Explanatory\nx1  1.7620153            a          x1\nx2  0.2841495            b          x2\nx3  0.2197337            b          x3\nx4 -0.2890524            b          x4\n\n\n\n\n18.5.2.3 ANOVA plotter\nThe following function plots ANOVAS if you have a summary_data object and a sig_labels object, as shown above. This does not work on ANOVA with interactive components.\n\nanova_plotter &lt;- function(summary_data, explanatory, \n                          response, sig_labels,\n                          y_lim=NA, label_height=NA, \n                          y_lab=NA, x_lab=NA){\n  require(tidyverse)\n  \n  plot_data_1 &lt;- summary_data[,c(explanatory, response, \"se\")]\n  plot_data_2 &lt;- sig_labels[,c(explanatory,\"Significance\")]\n  \n  colnames(plot_data_1) &lt;- c(\"explanatory\", \"response\", \"se\")\n  colnames(plot_data_2) &lt;- c(\"explanatory\", \"Significance\")\n  \n  plot_data &lt;- plot_data_1 |&gt; \n    full_join(plot_data_2, by = \"explanatory\")\n  \n  if(is.na(y_lim)){\n    if(min(plot_data$response) &lt; 0){\n      y_lim &lt;- c(min(plot_data$response) - \n          4*max(plot_data$se),\n        max(plot_data$response) + \n          4*max(plot_data$se))\n    }else{\n      y_lim &lt;- c(0,max(plot_data$response) + \n                                4*max(plot_data$se))\n    }\n  }\n  if(is.na(label_height)){label_height &lt;- 0.25*max(y_lim)}\n  if(is.na(y_lab)){y_lab &lt;- \"Response\"}\n  if(is.na(x_lab)){x_lab &lt;- \"Treatment\"}\n  \n  plot_1 &lt;- ggplot(plot_data,\n         aes(x = explanatory, y = response)) +\n    geom_point() +\n    geom_errorbar(data = plot_data,\n                  aes(ymin = response - 2*se,\n                      ymax = response + 2*se,\n                      width = 0.1)) +\n    ylim(y_lim) +\n    theme_classic() + \n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5)) +\n  geom_text(data = plot_data,\n            # make bold\n            fontface = \"bold\",\n            # define where labels should go\n            aes(x = explanatory, \n                # define height of label\n                y = label_height, \n                # what are the labels?\n                label = paste0(Significance))) +\n  xlab(x_lab) +\n  ylab(y_lab)\n  \n  print(plot_1)\n}\n\nanova_plotter(summary_data = example_summary,\n              explanatory = \"Explanatory\",\n              response = \"mean\", # from summary_data table! What is to be plotted\n              sig_labels = sig_labels)\n\n\n\n\n\n\n\n\nNote that in the above, the default label height is not working for us. We can adjust this with label_height.\n\nanova_plotter(summary_data = example_summary,\n              explanatory = \"Explanatory\",\n              response = \"mean\", # from summary_data table! What is to be plotted\n              sig_labels = sig_labels,\n              label_height = -1)\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions & Glossary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boyle, W. A., and B. J. Sigel (2016). Data from: Ongoing changes in the\navifauna of la selva biological station, costa rica: Twenty-three years\nof christmas bird counts. [Online.] Available at https://doi.org/10.5061/dryad.65v10.\n\n\nBrambilla, A., A. von Hardenberg, B. Bassano, L. Ranghetti, L. Keller,\nand M. Festa-Bianchet (2024). Data from: Climate drives body mass\nchanges in a mountain ungulate: Shorter winters lead to heavier alpine\nibex. [Online.] Available at https://doi.org/10.5061/dryad.w9ghx3fz6.\n\n\nComont, R. (2020). BeeWalk dataset 2008-23.\nhttps://doi.org/10.6084/m9.figshare.12280547.v4\n\n\nCooper, J. C. (2021). Biogeographic and Ecologic\nDrivers of Avian Diversity.\n[Online.] Available at https://doi.org/10.6082/uchicago.3379.\n\n\nCooper, J. C., J. D. Maddox, K. McKague, and J. M. Bates (2021a). Data\nfrom: Multiple lines of evidence indicate ongoing\nallopatric and parapatric diversification in an Afromontane\nsunbird (Cinnyris reichenowi). https://doi.org/10.5061/dryad.34tmpg4j0\n\n\nCooper, J. C., J. D. Maddox, K. McKague, and J. M. Bates (2021b). Multiple lines of\nevidence indicate ongoing allopatric and parapatric diversification in\nan Afromontane sunbird (Cinnyris\nreichenowi). Ornithology 138:ukaa081.\n\n\nCouper, L. I., J. Y. Kwan, J. Ma, and A. Swei (2019). Data from: Drivers\nand patterns of microbial community assembly in a lyme disease vector.\n[Online.] Available at https://doi.org/10.5061/dryad.2nv32qh.\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth\nScores. https://doi.org/10.6084/m9.figshare.8081108.v1\n\n\nLele, A., H. Garrod, E. Ferguson, C. Azahara Prieto Gil, and M. Ellis\n(2022). Morphological measurements in a coastal Ecuadorian\navifauna. [Online.] Available at https://zenodo.org/records/6511860.\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020a). Patient\nages at presentation. https://doi.org/10.1371/journal.pcbi.1007838.s006\n\n\nLydeamore, M. J., P. T. Campbell, D. J. Price, Y. Wu, A. J. Marcato, W.\nCuningham, J. R. Carapetis, R. M. Andrews, M. I. McDonald, J. McVernon,\nS. Y. C. Tong, and J. M. McCaw (2020b). Estimation of the\nforce of infection and infectious period of skin sores in remote\nAustralian communities using interval-censored data.\nPLOS Computational Biology 16:e1007838.\n\n\nMoura, R., N. P. Santos, and A. Rocha (2023). Processed csv file of the piracy dataset.\nhttps://doi.org/10.6084/m9.figshare.24119643.v1\n\n\nRuxton, G. D. (2006). The unequal variance\nt-test is an underused alternative to Student’s\nt-test and the Mann–Whitney\nu test. Behavioral Ecology 17:688–690.\n\n\nUrban Big Data Centre (2025). Counts hourly pay Adzuna\njobs. [Online.] Available at https://zenodo.org/records/14771706.\n\n\nVerstraete, H., W. Courtens, R. Daelemans, M. Van de walle, N. Vanermen,\nP. Desmet, and E. W. M. Stienen (2020). Photos and measurements of\notoliths from fish caught in the southern part of the North\nSea. [Online.] Available at https://zenodo.org/records/4066594.\n\n\nWeber, G. (2019). Fremont Bridge Hourly\nBicycle Counts by Month\nOctober 2012 to present. [Online.] Available at https://zenodo.org/records/2648564.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ttest.html#homework-one-sample",
    "href": "ttest.html#homework-one-sample",
    "title": "11  Means testing",
    "section": "11.6 Homework: One Sample",
    "text": "11.6 Homework: One Sample\n\n11.6.1 Answer each question. Perform all necessary tests. Perform transformations on the data if required.\n\n# install.packages(\"lme4\")   # required for \"sleepstudy\" dataset\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\ndata(\"sleepstudy\")\nhead(sleepstudy)\n\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n5 356.8519    4     308\n6 414.6901    5     308\n\n# install.packages(\"MASS\")   # required for \"galaxies\" dataset\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndata(\"galaxies\")\nhead(galaxies)\n\n[1]  9172  9350  9483  9558  9775 10227\n\n\n\n\n11.6.2 Question 1:\nThese are the test scores of 12 students on a quiz. The quiz was out of 100 points. Suppose we want to test whether the mean score differs significantly from a predicted average of 70.\n\nquiz_scores &lt;- c(75, 82, 68, 90, 73, 85, 77, 79, 88, 91, 83, 80)  \n\n\nState the null and alternative hypotheses. (2 pts)\nAre these data normal? (2 pts)\nIs the mean score significantly different from the expected result? (2 pts)\nDid the students do better or worse than expected, if there is a difference? (2 pts)\n\n\n\n11.6.3 Question 2:\nThe following is a list of reported study hours for Biostats per week. We expect the class average to be about three hours a week. Using this dataset, answer the following questions:\n\nstudy_hours &lt;- c(0.5, 3.0, 2.5, 4.5, 3.0, 1.5, 2.0, 3.5, 6, 1.0)  \n\n\nState the null and alternative hypotheses. (2 pts)\nAre these data normal? (2 pts)\nDo students spend the expected amount of time studying per week? (2 pts)\nDo students spend more or less time studying per week, if there is a difference? (2 pts)\n\n\n\n11.6.4 Question 3:\nThe following dataset records the reaction times of people who have had less than three hours of sleep on the night before this test. Using the reaction time column, perform a \\(t\\)-test to determine if these people have a statistically different reaction time than the human average (250 ms).\n\nreaction_times &lt;- sleepstudy$Reaction\n\n\nState the null and alternative hypotheses. (2 pts)\nAre these data normal? (2 pts)\nIs the mean score significantly different from the expected result? (2 pts)\nAre the people in the dataset slower or faster than average, if there is a difference? What might be the reason for this? (2 pts)\n\n\n\n11.6.5 Question 4:\nWhole milk is expected to be around 3.25% fat. Researchers from Florida wanted to determine if this was the case and used two methods to measure the fat percentage in the milk they tested. Using the enzymatic method ($triglyceride), determine if the fat percentage of this milk was significantly different from the 3.25% expected.\n\nmilk &lt;- read.csv(\"https://users.stat.ufl.edu/~winner/data/milkfat_measure.csv\")\nmilk_fats &lt;- milk$triglyceride\n\nmilk_fats\n\n [1] 0.96 1.16 0.97 1.01 1.25 1.22 1.46 1.66 1.75 1.72 1.67 1.67 1.93 1.99 2.01\n[16] 2.28 2.15 2.29 2.45 2.40 2.79 2.77 2.64 2.73 2.67 2.61 3.01 2.93 3.18 3.18\n[31] 3.19 3.12 3.33 3.51 3.66 3.95 4.20 4.05 4.30 4.74 4.71 4.71 4.74 5.23 6.21\n\n\n\nState the null and alternative hypotheses.(2 pts)\nIs the mean score significantly different from the expected result? (4 pts)\nIs the milk fattier or leaner than expected, if there is a difference? (2 pts)\n\n\n\n11.6.6 Question 5:\nGalaxies are rapidly moving away from us at various speeds. Previous studies had offered an average recession rate of 20,000 km/s. Data collected using redshift allows us to calculate the actual speed of recession of a galaxy. Using the data from R. J. Roeder (1990), saved as “galaxies”, determine if the average galaxy is actually receding at the previously estimated rate.\n\nhead(galaxies)\n\n[1]  9172  9350  9483  9558  9775 10227\n\n\n\nState the null and alternative hypotheses. (2 pts)\nIs the mean score significantly different from the expected result? (4 pts)\nAre the galaxies moving away faster or slower, if there is a difference? (2 pts)\n11.7 Homework: Chapter 10\n\n\n11.7.0.1 Two-sample means are practiced in Chapter 10. Please see Canvas for more information.\n\n\n\n\nCourtenay, L. (2019). Measurements on Canid Tooth Scores. https://doi.org/10.6084/m9.figshare.8081108.v1\n\n\nRuxton, G. D. (2006). The unequal variance t-test is an underused alternative to Student’s t-test and the Mann–Whitney u test. Behavioral Ecology 17:688–690.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Means testing</span>"
    ]
  },
  {
    "objectID": "visual.html#acknowledgments",
    "href": "visual.html#acknowledgments",
    "title": "5  Diagnosing data visually",
    "section": "5.9 Acknowledgments",
    "text": "5.9 Acknowledgments\nThanks to Hernan Vargas & Riley Grieser for help in formatting this page. Additional comments provided by BIOL 305 classes.\n\n\n\n\nVerstraete, H., W. Courtens, R. Daelemans, M. Van de walle, N. Vanermen, P. Desmet, and E. W. M. Stienen (2020). Photos and measurements of otoliths from fish caught in the southern part of the North Sea. [Online.] Available at https://zenodo.org/records/4066594.\n\n\nWeber, G. (2019). Fremont Bridge Hourly Bicycle Counts by Month October 2012 to present. [Online.] Available at https://zenodo.org/records/2648564.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Diagnosing data visually</span>"
    ]
  }
]