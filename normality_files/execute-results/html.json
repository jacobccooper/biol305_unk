{
  "hash": "e7e0763ac543fab8589a0014c80a65fc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Normality & hypothesis testing\"\nauthor: \"Dr. Jacob C. Cooper\"\nformat: html\neditor: visual\n---\n\n\n\n\n\n\n## Normal distributions\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nA *standard normal distribution* is a mathematical model that describes a commonly observed phenomenon in nature. When measuring many different kinds of datasets, the data being measured often becomes something that resembles a standard normal distribution. This distribution is described by the following equation:\n\n$$f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^\\frac{(x-\\mu)^2}{2\\sigma^2}$$\n\nThis equation is fairly well defined by the *variance* ($\\sigma^2$), the overall spread of the data, and by the *standard deviation* ($\\sigma$), which is defined by the square root of the variance.\n\n![A standard normal distribution, illustrating the percentage of area found within each standard deviation away from the mean. By Ainali on Wikipedia; CC-BY-SA 3.0.](images/Standard_deviation_diagram_micro.svg.png)\n\nStandard normal distributions have a mean, median, and mode that are *equal*. The standard normal distribution is a *density function*, and we are interested in the \"area under the curve\" (AUC) to understand the relative probability of an event occurring. At the mean/median/mode, the probability on either side of the distribution is $50$%. When looking at a normal distribution distribution, it is impossible to say the probability of a specific event occurring, but it is possible to state the probability of an event *as extreme or more extreme than the event observed* occurring. This is known as the $p$ value.\n\n### Example in nature\n\nIn order to see an example of the normal distribution in nature, we are going to examine the BeeWalk survey database from the island of Great Britain [@Comont2020]. We are not interested in the bee data at present, however, but in the climatic data from when the surveys were performed.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeewalk <- curl(\"https://figshare.com/ndownloader/files/44726902\") |>\n  read_csv()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 306550 Columns: 49\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): Website.ID, Website.RecordKey, SiteName, Site.section, ViceCounty,...\ndbl (19): RecordKey, established, Precision, Transect.lat, Transect.long, tr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNote that this is another massive dataset - $306,550$ rows of data!\n\nThe dataset has the following columns:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(beewalk)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"RecordKey\"            \"Website.ID\"           \"Website.RecordKey\"   \n [4] \"SiteName\"             \"Site.section\"         \"ViceCounty\"          \n [7] \"established\"          \"GridReference\"        \"Projection\"          \n[10] \"Precision\"            \"Transect.lat\"         \"Transect.long\"       \n[13] \"transect.OS1936.lat\"  \"Transect.OS1936.long\" \"transect_length\"     \n[16] \"section_length\"       \"section_grid_ref\"     \"H1\"                  \n[19] \"H2\"                   \"H3\"                   \"H4\"                  \n[22] \"habitat_description\"  \"L1\"                   \"L2\"                  \n[25] \"land_use_description\" \"start_time\"           \"end_time\"            \n[28] \"sunshine\"             \"wind_speed\"           \"temperature\"         \n[31] \"TaxonVersionKey\"      \"species\"              \"latin\"               \n[34] \"queens\"               \"workers\"              \"males\"               \n[37] \"unknown\"              \"Comment\"              \"transect_comment\"    \n[40] \"flower_visited\"       \"StartDate\"            \"EndDate\"             \n[43] \"DateType\"             \"Year\"                 \"Month\"               \n[46] \"Day\"                  \"Sensitive\"            \"Week\"                \n[49] \"TotalCount\"          \n```\n\n\n:::\n:::\n\n\n\n\n\n\nWe are specifically interested in `temperature` to determine weather conditions. Let's see what the mean of this variable is.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(beewalk$temperature)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n```\n\n\n:::\n:::\n\n\n\n\n\n\nHmmm... we are getting an `NA` value, indicating that not every cell has data recorded. Let's view `summary`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(beewalk$temperature)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   16.00   19.00   18.65   21.00   35.00   16151 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nAs we can see, $16,151$ rows do not have temperature recorded! We want to remove these `NA` rows, which we can do by using using `na.omit`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeewalk$temperature |>\n  na.omit() |>\n  mean() |>\n  round(2) # don't forget to round!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.65\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNow we can record the mean.\n\nLet's visualize these data using a histogram. *Note* I do not use `na.omit` as the `hist` function automatically performs this data-cleaning step!\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(beewalk$temperature,breaks = 5)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nEven with only five breaks, we can see an interesting, normal-esque distribution in the data. Let's refine the bin number.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(beewalk$temperature,breaks = 40)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nWith forty breaks, the pattern becomes even more clear. Let's see what a *standard normal distribution* around these data would look like.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save temperature vector without NA values\ntemps <- beewalk$temperature |> na.omit()\n\nmu <- mean(temps)\nt.sd <- sd(temps)\n\n# sample random values\nnormal.temps <- rnorm(length(temps), # sample same size vector\n                      mean = mu,\n                      sd = t.sd)\n\nhist(normal.temps, breaks = 40)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAs we can see, our normal approximation of temperatures is not too dissimilar from the distribution of temperatures we actually see!\n\nLet's see what kind of data we have for temperatures:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load moments library\nlibrary(moments)\n\nskewness(temps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02393257\n```\n\n\n:::\n:::\n\n\n\n\n\n\nData do not have any significant skew.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkurtosis(temps)-3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3179243\n```\n\n\n:::\n:::\n\n\n\n\n\n\nData do not show any significant kurtosis.\n\n### Effect of sampling\n\nOftentimes, we will see things approach the normal distribution as we collect more samples. We can model this by subsampling our temperature vector.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make reproducible\nset.seed(1839)\n\nsub.temps <- sample(temps,\n                    size = 10,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"10 samples\")\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nWith only ten values sampled, we do not have much of a normal distribution. Let's up this to $100$ samples.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsub.temps <- sample(temps,\n                    size = 100,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"100 samples\",breaks = 10)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow we are starting to see more of a normal distribution! Let's increase this to $1000$ temperatures.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsub.temps <- sample(temps,\n                    size = 1000,\n                    replace = FALSE)\n\nhist(sub.temps, main = \"1000 samples\", breaks = 40)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow the normal distribution is even more clear. As we can also see, the more we sample, the more we approach the true means and distribution of the actual dataset. Because of this, we can perform experiments and observations of small groups and subsamples and make inferences about the whole, given that most systems naturally approach statistical distributions like the normal!\n\n### Testing if data are normal\n\nThere are two major methods we can use to see if data are normally distributed.\n\n#### QQ Plots\n\nAnother way to see if data are normal is to use a [QQ plot](https://www.statology.org/q-q-plot-r/). These plots data quantiles to theoretical quantiles to see how well they align, with a perfectly normal distribution having a completely linear QQ plot. Let's look at these with our `beewalk` data.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(temps)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAs we can see above, the data are roughly linear, which means are data are very normal. The \"stairsteps\" are from the accuracy in measuring temperature, which was likely rounded and thus created a distribution that is not completely continuous.\n\n#### Shapiro-Wilk test\n\nAnother way to test for normality is to use a Shapiro-Wilk test of normality. We will not get into the specific of this distribution, but this tests the null hypothesis that data originated in a normal distribution, with the alternative hypothesis that the data originated in a non-normal distribution. You will read next about the specifics of hypothesis testing, but this test uses an $\\alpha = 0.05$, and we *reject the null hypothesis* if our $p < \\alpha$, with $p$ representing the probability of observing something as extreme or more extreme than the result we observe. Our `temps` dataset is too large, as `shapiro.test` requires vectors of $< 5000$. Let's take a random sample and try again.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(temps, 200) |>\n  shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sample(temps, 200)\nW = 0.98922, p-value = 0.1371\n```\n\n\n:::\n:::\n\n\n\n\n\n\nFor our subsets of temperature, we find that the temperature is normally distributed. **Note** that having all $5000$ temperatures included makes this test find a \"non-normal\" result, likely from the same stairstepping phenomenon from rounding that we discussed before, which may result in [model overfitting](https://en.wikipedia.org/wiki/Overfitting) to the rounded data.\n\n## Hypothesis testing\n\nSince we can define specific areas under the curve within these distributions, we can look at the percentage of area within a certain bound to determine how likely a specific outcome would be. Thus, we can begin to test what the *probability of observing an event* is within a theoretical, probabilistic space. A couple of important conceptual ideas:\n\n1.  We may not be able to know the probability of a specific event, but we can figure out the probability of events more extreme or less extreme as that event.\n2.  If the most likely result is the mean, then the further we move away from the mean, the less likely an event becomes.\n3.  If we look *away* from the mean at a certain point, then the area represents the chances of getting a result *as extreme or more extreme than what we observe*. This probability is known as the $p$ value.\n\nOnce we have a $p$ value, we can make statements about the event that we've seen relative to the overall nature of the dataset, but we do not have sufficient information to declare if this result is *statistically significant*.\n\n### Critical Values - $\\alpha$\n\nIn order to determine if something is significant, we compare things to a *critical value*, known as $\\alpha$. This value is traditionally defined as $0.05$, essentially stating that we deem an event as significant if $5$% or fewer of observed or predicted events are as extreme or more extreme than what we observe.\n\n**Your value should always set your** $\\alpha$ **critical value before you do your experiments and analyses.**\n\nOur critical value of $\\alpha$ represents our criterion for *rejecting the null hypothesis*. We set our $\\alpha$ to try to minimize the chances of error.\n\n**Type I Error** is also known as a **false-positive**, and is when we **reject the null hypothesis when the null is true**.\n\n**Type II Error** is also known as a **false-negative**, and is when we **support the null hypothesis when the null is false**.\n\nBy setting an $\\alpha$, we are creating a threshold of probability at which point we can say, with confidence, that results are different.\n\n### Introduction to $p$ values\n\nLet's say that we are looking at a dataset defined by a standard normal distribution with $\\mu=0$ and $\\sigma=1$. We draw a random value, $x$, with $x=1.6$. What is the probability of drawing a number this extreme or more extreme from the dataset?\n\nFirst, let's visualize this distribution:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###THIS WILL TAKE A WHILE TO RUN###\n\n# create gigantic normal distribution dataset\n# will be essentially normal for plotting\n# rnorm gets random values\nx <- rnorm(100000000)\n\n# convert to data frame\nx <- as.data.frame(x)\n# rename column\ncolnames(x) <- c(\"values\")\n\n# thank you stack overflow for the following\n# Creating density plot\np = ggplot(x, \n           aes(x = values)\n          ) + \n  # generic density plot, no fill\n  geom_density(fill=\"lightblue\")\n\n# Building shaded area\n# create new plot object\np2  <-  p + # add previous step as a \"backbone\"\n  # rename axes\n  geom_vline(xintercept = 1.6) +\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAbove, the solid black line represents $x$, with the illustrated standard normal distribution being filled in blue.\n\nLet's see how much of the area represents values *as extreme or more extreme* as our value $x$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  <-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  <-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x < 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow we can see that it is only a portion of the distribution *as extreme or more extreme* than the value we placed on the graph. The area of this region is our $p$ value. This represents the *probability of an event as extreme or more extreme occurring* given the random variation observed in the dataset or in the distribution approximating the dataset. This is the value we compare to $\\alpha$ - our threshold for rejecting the null hypothesis - to determine whether or not we are going to reject the null hypothesis.\n\nLet's look at the above graph again, but let's visualize a two-tailed $\\alpha$ around the mean with a $95$% confidence interval. First, we need to get the $Z$ scores for our $\\alpha=0.05$, which we calculate by taking $\\frac{\\alpha}{2}$ to account for the two tails. (Two tails essentially meaning we reject the null mean if we see things *greater than* or *less than* our expected value to a significant extent). We can calculate $Z$ scores using `qnorm`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlow_alpha <- qnorm(0.025) # looks left\nhi_alpha <- qnorm(0.975) # looks left\n\nprint(paste0(\"Our Z scores are: \", \n             round(low_alpha,2), \n             \" & \", \n             round(hi_alpha,2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Our Z scores are: -1.96 & 1.96\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe above values make sense, given the distribution is symmetrical. Our above dashed line is as $Z = 1.6$, which means we should have a $p = 0.05$, so the dashed line should be *closer* to the mean than our cutoffs.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### THIS WILL TAKE A WHILE TO RUN ###\n\n# Getting the values of plot\n# something I wasn't familiar with before making this!\nd  <-  ggplot_build(p)$data[[1]]\n\n# Building shaded area\n# create new plot object\np2  <-  p + # add previous step as a \"backbone\"\n  # add new shaded area\n  geom_area(data = subset(d, x < 1.6), # select area\n            # define color, shading intensity, etc.\n            aes(x=x,y=y), fill = \"white\", alpha = 1) +\n  # add value line\n  geom_vline(xintercept = 1.6, colour = \"black\", \n             linetype = \"dashed\",linewidth = 1) +\n  geom_vline(xintercept = low_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  geom_vline(xintercept = hi_alpha, colour = \"red\", \n             linetype = \"solid\",linewidth = 1) +\n  # rename axes\n  xlab(\"Test Statistic\") +\n  ylab(\"Frequency (Probability)\") +\n  # make it neat and tidy\n  theme_classic()\n\n# plot it\n# can use ggsave function to save\nplot(p2)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nExactly as we calculated, we see that our $p < \\alpha$ and thus we do not see an area (in blue) less than the area that would be further than the mean as defined by the red lines.\n\n### Calculating a $Z$ score\n\nWhen we are trying to compare our data to a normal distribution, we need to calculate a $Z$ score for us to perform the comparison. A $Z$ score is essentially a measurement of the number of standard deviations we are away from the mean on a standard normal distribution. The equation for a $Z$ score is:\n\n$$\nZ = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n$$\n\nWhere $\\bar{x}$ is either a sample mean or a sample value, $\\mu$ is the population mean, $\\sigma$ is the population standard deviation and $n$ is the number of individuals in your sample ($1$ if comparing to a single value).\n\nWe can calculate this in *R* using the following function:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzscore <- function(xbar, mu, sd.x, n = 1){\n  z <- (xbar - mu)/(sd.x/sqrt(n))\n  return(z)\n}\n```\n:::\n\n\n\n\n\n\n**NOTE** that is the above isn't working for you, *you have a mistake somewhere in your code*. Try comparing - character by character - what is listed above to what you have.\n\nLet's work through an example, where we have a sample mean of $62$ with $5$ samples compared to a sample mean of $65$ with a standard deviation of $3.5$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nZ <- zscore(xbar = 62, # sample mean\n            mu = 65, # population mean\n            sd.x = 3.5, # population standard deviation\n            n = 5) # number in sample\n\nprint(Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.91663\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNow, we can calculate the $p$ value for this $Z$ score.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0276425\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAfter rounding, we get $p = 0.03$, a $p$ that is significant if for a one-tailed $\\alpha = 0.05$ but insignificant for a two-tailed $\\alpha = 0.05$.\n\n### Calculated the $p$ value\n\nWe have two different methods for calculating a $p$ value:\n\n#### Comparing to a $z$ table\n\nWe can compare the $z$ value we calculate to a $z$ table, such as the one at [ztable.net](https://www.ztable.net/). On this webpage, you can scroll and find tables for positive and negative $z$ scores. *Note* that normal distributions are symmetrical, so you can also transform from negative to positive to get an idea of the area as well. Given that a normal distribution is centered at $0$, a $z$ score of $0$ will have a $p$ value of $0.50$.\n\nOn the $z$ tables, you will find the tenths place for your decimal in the rows, and then go across to the columns for the hundredths place. For example, go to the website and find the $p$ value for a $z$ score of $-1.33$. You should find the cell marked $0.09176$. *Note* the website uses naked decimals, which we do not use in this class.\n\nFor values that aren't on the $z$ table, we can approximate its position between different points on the $z$ table or, if it is extremely unlikely, denote that $p < 0.0001$.\n\n#### Using *R*\n\nIn *R*, we can calculate a $p$ value using the function `pnorm`. This function uses the arguments of `p` for our $p$ value, `mean` for the mean of our distribution, `sd` for the standard deviation of our distribution, and also information on whether we want to log-transform $p$ or if we are testing a specific hypothesis (lower tail, upper tail, or two-tailed). The function `pnorm` defaults to a standard normal distribution, which would be a $z$ score, but it can also perform the $z$ transformations for us if we define the mean and standard deviation.\n\nFor example, if we have a $z$ score of $-1.33$:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(-1.33)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09175914\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAs we can see, we get the same result as our $z$ table, just with more precision!\n\nThere are other functions in this family as well in *R,* including `dnorm` for quantiles, `qnorm` for determining the $z$ score for a specific $p$ value, and `rnorm` for getting random values from a normal distribution with specific dimensions. For now, we will focus on `pnorm`.\n\n### Workthrough Example\n\nWhen this class was being designed, [Hurricane Milton](https://en.wikipedia.org/wiki/Hurricane_Milton) was about to make contact with Florida. Hurricane Milton is considered one of the strongest hurricanes of all time, so we can look at historical hurricane data to determine just how powerful this storm really was. We can get information on maximum wind speeds of all recorded Atlantic hurricanes as of 2024 from Wikipedia.\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhurricanes <- read_csv(\"https://raw.githubusercontent.com/jacobccooper/biol305_unk/main/assignments/hurricane_speeds.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 889 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): Hurricane_Windspeed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAbove, we have loaded a `.csv` of one column that has all the hurricane speeds up to 2024. Hurricane Milton is the last row - the most recent hurricane. Let's separate this one out. We will use `[ , ]`, which defines `[rows,columns]` to subset data.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmilton <- hurricanes$Hurricane_Windspeed[nrow(hurricanes)]\n\nother_hurricanes <- hurricanes[-nrow(hurricanes),]\n```\n:::\n\n\n\n\n\n\nWe want to compare the windspeed of Milton (180 mph) to the overall distribution of hurricane speeds. We can visualize this at first.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# all hurricanes\nhist(hurricanes$Hurricane_Windspeed)\n```\n\n::: {.cell-output-display}\n![](normality_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nWindspeeds are more towards the lower end of the distribution, with strong storms being rarer.\n\n*For the sake of this class, we will assume we can use a normal distribution for these data*, but if we were doing an official study we would likely need to use a non-parametric test (we will cover these later, but they cover non-normal data).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(other_hurricanes$Hurricane_Windspeed)\n\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n```\n\n\n:::\n:::\n\n\n\n\n\n\nHmmm... we need to use `na.omit` to be sure we do this properly.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nother_hurricanes_windspeed <- na.omit(other_hurricanes$Hurricane_Windspeed)\n\nmu <- mean(other_hurricanes_windspeed)\nmean(other_hurricanes_windspeed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 102.5254\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNext, we need the standard deviation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd.hurricane <- sd(other_hurricanes_windspeed)\n\nsd.hurricane\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23.08814\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNow, we can calculate our $Z$ value.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nZ <- (milton - mu)/sd.hurricane\n\nZ\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.355603\n```\n\n\n:::\n:::\n\n\n\n\n\n\nHow significant is this?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.999604\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThis is greater than $0.5$, so we need to do $1-p$ to figure things out.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.000395961\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThis rounds to $0.0004$, which means that this is an *extremely* strong hurricane.\n\n#### Non-normality, for those curious\n\nWe can do a Shapiro-Wilk test of normality to see if this dataset is normal.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(other_hurricanes_windspeed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  other_hurricanes_windspeed\nW = 0.88947, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nA $p < 0.05$ indicates that these data are *non-normal*.\n\nWe can do a Wilcoxon-Test since these data are extremely non-normal.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(other_hurricanes_windspeed,\n            milton)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  other_hurricanes_windspeed and milton\nW = 6.5, p-value = 0.08678\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\n\n\nUsing non-normal corrections, we find that this is *not* an extremely strong hurricane, but it is near the upper end of what we would consider \"normal\" under historical conditions. Still an extremely bad hurricane!\n\n## Confidence Intervals\n\nBecause we can figure out the probability of an event occurring, we can also calculate *confidence intervals*. A *confidence interval* provides a range of numbers around a value of interest that indicates where we believe the mean of a population lies and our confidence that it lies within that range. **Note** that nothing is ever 100% certain, but this helps us determine where a mean is and demonstrates our confidence in our results.\n\nSpecifically, if the tails of the distribution, our $\\alpha$, are $0.05$, then we have an area of $0.95$ around the mean where we do not reject results. Another perspective on this area is that we can say with $95$% certainty that a mean is within a certain area, and that if values fall within that confidence area then we do not reject the null hypothesis that the means are equal.\n\nWe will cover several different ways to calculate confidence intervals, but for normal distributions, we use the following equation, with the $0.95$ confidence interval shown as an example:\n\n$$\nCI=\\bar{x} \\pm Z_{1-\\frac{\\alpha}{2}}\\cdot \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nThis interval gives us an idea of where the mean should lie. For example, if we are looking at the aforemention `beewalk` temperature data, we can calulate a $0.95$ confidence interval around the mean.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemps <- na.omit(beewalk$temperature)\n\nxbar <- mean(temps)\nn <- length(temps)\nsdtemp <- sd(temps)\n# Z for 0.95 as P, so for 0.975, 0.025\n# get value from P\nZ <- qnorm(0.975)\n\nCI <- Z*(sdtemp/sqrt(n))\n\nCI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01458932\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWe have a very narrow confidence zone, because we have so many measurements. Let's round everything and present it in a good way.\n\nIf I want numbers to show up *in text* in RMarkdown, I can add code to a line of plain text using the following syntax:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DO NOT RUN\n# Format in plaintext\n`r xbar`\n```\n:::\n\n\n\n\n\n\n![This is the \"coded\" version of the text below. Compare the above window to the text below this image.](images/inline_code.png)\n\nTyping that into the plain text should render as the following: 18.645963. Then I can also type my answer as follows:\n\nThe $95$% Confidence Interval for the mean for this temperature dataset is:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(round(xbar, 2),\n      \"+/-\",\n      round(CI, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"18.65 +/- 0.01\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n*Note* that the above is rounded to two decimal places to illustrative purposes ONLY, and should be rounded to one decimal place if it was a homework assignment because the original data has only one decimal place.\n\n## Homework: Chapter 8\n\nPlease complete problems 8.1, 8.2, 8.3 & 8.6. Follow the directions as written in the book. Submit one `html` file, as derived from *RStudio*. For maximum clarity, create headings to separate your problems. (Remember, a header can be invoked by placing '`#`' in front of a line of text. For example: the header here is written as `# Homework: Chapter 8`).\n",
    "supporting": [
      "normality_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}